experiment_name: MountainCarContinuous-PPO

env_kwargs:
  id: MountainCarContinuous-v0
  vectorization_mode: async
  num_envs: 8

policy:
  type: ppo
  kwargs:
    gamma: 0.995
    lambda_: 0.95
    value_loss_coef: 0.5
    entropy_coef: 0.001
    entropy_decay: 0.99
    lr: 0.0003
    num_epochs: 10
    clip_epsilon: 0.2
    exploration_method: distribution
    advantage_normalize: global

worker_kwargs:
  device: auto
  record_step: 50000

train_kwargs:
  num_steps: 200000
  batch_size: 2048
  minibatch_size: 256

network:
  kwargs:
    initial_log_std: 0.
    num_features: 64
    backbone_name: mlp
    feature_extractor_name: shared
    head_name: actor_critic
    policy_name: actor_critic
    distribution: normal
