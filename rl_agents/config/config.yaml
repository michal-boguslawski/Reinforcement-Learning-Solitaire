env_kwargs:
  general_wrappers: {}
  id: BipedalWalker-v3
  training_wrappers:
    scale_reward:
      scale_factor: 0.2
      loc_factor: 0.
  # normalize_rewards: true
  num_envs: 8
  vectorization_mode: async
env_name: BipedalWalker
experiment_name: PPO/v7
network:
  kwargs:
    backbone_kwargs: {}
    backbone_name: mlp
    core_kwargs: {}
    core_name: gru
    distribution: normal
    head_kwargs: 
      activation_fn: relu
      hidden_dim: 64
    head_name: actor_critic
    initial_deviation: 0.6
    num_features: 64
policy:
  kwargs:
    advantage_normalize: batch
    clip_epsilon: 0.2
    entropy_kwargs:
      max_entropy: 0.03
      min_entropy: 0.000001
      scheduler_type: linear_entropy
      total_steps: 300
    exploration_method:
      kwargs: {}
      name: distribution
    gamma: 0.99
    lambda_: 0.95
    num_epochs: 6
    optimizer_kwargs:
      actor_lr: 0.0003
      critic_lr: 0.0005
    scheduler_kwargs:
      end_factor: 0.00001
      scheduler_type: linear_lr
      start_factor: 1.0
      total_iters: 500
    use_value_clipping: absolute
    value_loss_coef: 0.5
  type: ppo
train_kwargs:
  batch_size: 2048
  minibatch_size: 128
  num_steps: 1000000
worker_kwargs:
  device: auto
  record_step: 100000
  temperature_config: {}
  verbose: 1
