env_kwargs:
  general_wrappers: {}
  id: Acrobot-v1
  normalize_rewards: false
  num_envs: 8
  training_wrappers:
    power_obs_reward:
      pow_factors:
      - 0
      - 0
      - 0
      - 0
      - 0.01
      - 0.01
    scale_reward:
      loc_factor: 0.0
      scale_factor: 0.1
    terminal_bonus:
      terminated_bonus: 100
  vectorization_mode: async
env_name: Acrobot
experiment_name: A2C/v1
network:
  kwargs:
    backbone_kwargs: {}
    backbone_name: mlp
    core_kwargs: {}
    core_name: identity
    distribution: categorical
    head_kwargs: {}
    head_name: actor_critic
    initial_deviation: 1.0
    num_features: 16
policy:
  kwargs:
    entropy_coef: 0.01
    exploration_method:
      kwargs: {}
      name: distribution
    gamma: 0.99
    lambda_: 0.95
    optimizer_kwargs:
      actor_lr: 0.0003
      critic_lr: 0.001
    returns_normalize: false
    value_loss_coef: 0.5
  type: a2c
train_kwargs:
  batch_size: 128
  minibatch_size: 128
  num_steps: 50000
worker_kwargs:
  device: auto
  record_step: 10000
  temperature_config: {}
  verbose: 1
