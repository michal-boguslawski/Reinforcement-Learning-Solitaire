2026-02-10 15:17:39,076 : worker.worker : INFO : Current device cuda
2026-02-10 15:17:39,142 : envs.wrappers : INFO : TerminalBonusWrapper attached with params 0.0 -1.0
2026-02-10 15:17:39,142 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params None [-0.01, -0.005, 0.0, -0.002, -0.001, 0.001, 0, 0, 0, -0.001, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0, 0, 0.001, 0.0, 0.0, 0, 0, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1
2026-02-10 15:17:39,142 : envs.wrappers : INFO : ActionPowerRewardWrapper attached with params [-0.001, -0.002, -0.01, -0.002] [0.002, 0.001, 0.002, 0.001] 1
2026-02-10 15:17:39,144 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params [[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]
2026-02-10 15:17:39,160 : envs.wrappers : INFO : TerminalBonusWrapper attached with params 0.0 -1.0
2026-02-10 15:17:39,160 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params None [-0.01, -0.005, 0.0, -0.002, -0.001, 0.001, 0, 0, 0, -0.001, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0, 0, 0.001, 0.0, 0.0, 0, 0, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1
2026-02-10 15:17:39,160 : envs.wrappers : INFO : ActionPowerRewardWrapper attached with params [-0.001, -0.002, -0.01, -0.002] [0.002, 0.001, 0.002, 0.001] 1
2026-02-10 15:17:39,163 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params [[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]
2026-02-10 15:17:39,167 : envs.wrappers : INFO : TerminalBonusWrapper attached with params 0.0 -1.0
2026-02-10 15:17:39,167 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params None [-0.01, -0.005, 0.0, -0.002, -0.001, 0.001, 0, 0, 0, -0.001, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0, 0, 0.001, 0.0, 0.0, 0, 0, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1
2026-02-10 15:17:39,168 : envs.wrappers : INFO : ActionPowerRewardWrapper attached with params [-0.001, -0.002, -0.01, -0.002] [0.002, 0.001, 0.002, 0.001] 1
2026-02-10 15:17:39,170 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params [[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]
2026-02-10 15:17:39,172 : envs.wrappers : INFO : TerminalBonusWrapper attached with params 0.0 -1.0
2026-02-10 15:17:39,173 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params None [-0.01, -0.005, 0.0, -0.002, -0.001, 0.001, 0, 0, 0, -0.001, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0, 0, 0.001, 0.0, 0.0, 0, 0, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1
2026-02-10 15:17:39,173 : envs.wrappers : INFO : ActionPowerRewardWrapper attached with params [-0.001, -0.002, -0.01, -0.002] [0.002, 0.001, 0.002, 0.001] 1
2026-02-10 15:17:39,175 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params [[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]
2026-02-10 15:17:39,177 : envs.wrappers : INFO : TerminalBonusWrapper attached with params 0.0 -1.0
2026-02-10 15:17:39,177 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params None [-0.01, -0.005, 0.0, -0.002, -0.001, 0.001, 0, 0, 0, -0.001, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0, 0, 0.001, 0.0, 0.0, 0, 0, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1
2026-02-10 15:17:39,177 : envs.wrappers : INFO : ActionPowerRewardWrapper attached with params [-0.001, -0.002, -0.01, -0.002] [0.002, 0.001, 0.002, 0.001] 1
2026-02-10 15:17:39,180 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params [[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]
2026-02-10 15:17:39,180 : envs.wrappers : INFO : TerminalBonusWrapper attached with params 0.0 -1.0
2026-02-10 15:17:39,180 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params None [-0.01, -0.005, 0.0, -0.002, -0.001, 0.001, 0, 0, 0, -0.001, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0, 0, 0.001, 0.0, 0.0, 0, 0, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1
2026-02-10 15:17:39,180 : envs.wrappers : INFO : ActionPowerRewardWrapper attached with params [-0.001, -0.002, -0.01, -0.002] [0.002, 0.001, 0.002, 0.001] 1
2026-02-10 15:17:39,182 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params [[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]
2026-02-10 15:17:39,183 : envs.wrappers : INFO : TerminalBonusWrapper attached with params 0.0 -1.0
2026-02-10 15:17:39,183 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params None [-0.01, -0.005, 0.0, -0.002, -0.001, 0.001, 0, 0, 0, -0.001, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0, 0, 0.001, 0.0, 0.0, 0, 0, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1
2026-02-10 15:17:39,183 : envs.wrappers : INFO : ActionPowerRewardWrapper attached with params [-0.001, -0.002, -0.01, -0.002] [0.002, 0.001, 0.002, 0.001] 1
2026-02-10 15:17:39,186 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params [[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]
2026-02-10 15:17:39,187 : envs.wrappers : INFO : TerminalBonusWrapper attached with params 0.0 -1.0
2026-02-10 15:17:39,187 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params None [-0.01, -0.005, 0.0, -0.002, -0.001, 0.001, 0, 0, 0, -0.001, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0, 0, 0.001, 0.0, 0.0, 0, 0, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1
2026-02-10 15:17:39,187 : envs.wrappers : INFO : ActionPowerRewardWrapper attached with params [-0.001, -0.002, -0.01, -0.002] [0.002, 0.001, 0.002, 0.001] 1
2026-02-10 15:17:39,189 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params [[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]
2026-02-10 15:17:39,189 : envs.wrappers : INFO : TerminalBonusWrapper attached with params 0.0 -1.0
2026-02-10 15:17:39,190 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params None [-0.01, -0.005, 0.0, -0.002, -0.001, 0.001, 0, 0, 0, -0.001, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0, 0, 0.001, 0.0, 0.0, 0, 0, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1
2026-02-10 15:17:39,190 : envs.wrappers : INFO : ActionPowerRewardWrapper attached with params [-0.001, -0.002, -0.01, -0.002] [0.002, 0.001, 0.002, 0.001] 1
2026-02-10 15:17:39,192 : envs.wrappers : INFO : PowerObsRewardWrapper attached with params [[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]
2026-02-10 15:17:40,200 : worker.worker : INFO : ==================== Start training ====================
2026-02-10 15:17:40,515 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 15:17:41,864 : evaluate.evaluate : INFO : Evaluation results: mean = -124.53, std = 0.00, min = -124.53, max = -124.53, count = 1
2026-02-10 15:17:41,864 : evaluate.evaluate : INFO : Evaluation lengths: mean = 150.00, std = 0.00, min = 150.00, max = 150.00, count = 1
2026-02-10 15:17:41,865 : evaluate.evaluate : INFO : Covariance matrix:
[0.31622776 0.31622776 0.31622776 0.31622776]
2026-02-10 15:37:01,514 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 15:37:03,654 : evaluate.evaluate : INFO : Evaluation results: mean = -85.43, std = 0.00, min = -85.43, max = -85.43, count = 1
2026-02-10 15:37:03,654 : evaluate.evaluate : INFO : Evaluation lengths: mean = 262.00, std = 0.00, min = 262.00, max = 262.00, count = 1
2026-02-10 15:37:03,655 : evaluate.evaluate : INFO : Covariance matrix:
[0.18067275 0.2792422  0.16701856 0.3326084 ]
2026-02-10 15:41:29,216 : envs.wrappers : INFO : Env truncated due to lack of movement
2026-02-10 15:56:46,053 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 15:56:58,820 : evaluate.evaluate : INFO : Evaluation results: mean = -83.20, std = 0.00, min = -83.20, max = -83.20, count = 1
2026-02-10 15:56:58,821 : evaluate.evaluate : INFO : Evaluation lengths: mean = 1600.00, std = 0.00, min = 1600.00, max = 1600.00, count = 1
2026-02-10 15:56:58,822 : evaluate.evaluate : INFO : Covariance matrix:
[0.20674323 0.32758242 0.22464816 0.40211925]
2026-02-10 16:12:55,991 : envs.wrappers : INFO : Env truncated due to lack of movement
2026-02-10 16:16:22,956 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 16:16:35,687 : evaluate.evaluate : INFO : Evaluation results: mean = -115.09, std = 0.00, min = -115.09, max = -115.09, count = 1
2026-02-10 16:16:35,687 : evaluate.evaluate : INFO : Evaluation lengths: mean = 1482.00, std = 0.00, min = 1482.00, max = 1482.00, count = 1
2026-02-10 16:16:35,688 : evaluate.evaluate : INFO : Covariance matrix:
[0.2262781  0.32226673 0.28972942 0.45383918]
2026-02-10 16:35:51,236 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 16:35:53,095 : evaluate.evaluate : INFO : Evaluation results: mean = -44.45, std = 0.00, min = -44.45, max = -44.45, count = 1
2026-02-10 16:35:53,095 : evaluate.evaluate : INFO : Evaluation lengths: mean = 228.00, std = 0.00, min = 228.00, max = 228.00, count = 1
2026-02-10 16:35:53,096 : evaluate.evaluate : INFO : Covariance matrix:
[0.244124   0.3000876  0.31982762 0.5511858 ]
2026-02-10 16:55:13,150 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 16:55:31,306 : evaluate.evaluate : INFO : Evaluation results: mean = 239.78, std = 0.00, min = 239.78, max = 239.78, count = 1
2026-02-10 16:55:31,306 : evaluate.evaluate : INFO : Evaluation lengths: mean = 1436.00, std = 0.00, min = 1436.00, max = 1436.00, count = 1
2026-02-10 16:55:31,307 : evaluate.evaluate : INFO : Covariance matrix:
[0.24747369 0.2870925  0.31897634 0.62325066]
2026-02-10 17:14:56,408 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 17:15:08,360 : evaluate.evaluate : INFO : Evaluation results: mean = 51.53, std = 0.00, min = 51.53, max = 51.53, count = 1
2026-02-10 17:15:08,361 : evaluate.evaluate : INFO : Evaluation lengths: mean = 1042.00, std = 0.00, min = 1042.00, max = 1042.00, count = 1
2026-02-10 17:15:08,361 : evaluate.evaluate : INFO : Covariance matrix:
[0.25370345 0.27605027 0.32735586 0.5981361 ]
2026-02-10 17:34:38,524 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 17:34:51,067 : evaluate.evaluate : INFO : Evaluation results: mean = 267.50, std = 0.00, min = 267.50, max = 267.50, count = 1
2026-02-10 17:34:51,068 : evaluate.evaluate : INFO : Evaluation lengths: mean = 1057.00, std = 0.00, min = 1057.00, max = 1057.00, count = 1
2026-02-10 17:34:51,069 : evaluate.evaluate : INFO : Covariance matrix:
[0.2729357  0.27892917 0.32297754 0.55932236]
2026-02-10 17:52:15,404 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 17:52:18,502 : evaluate.evaluate : INFO : Evaluation results: mean = -11.46, std = 0.00, min = -11.46, max = -11.46, count = 1
2026-02-10 17:52:18,503 : evaluate.evaluate : INFO : Evaluation lengths: mean = 353.00, std = 0.00, min = 353.00, max = 353.00, count = 1
2026-02-10 17:52:18,503 : evaluate.evaluate : INFO : Covariance matrix:
[0.2921693  0.25713685 0.31094918 0.5401529 ]
2026-02-10 18:08:43,966 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 18:08:47,179 : evaluate.evaluate : INFO : Evaluation results: mean = -5.44, std = 0.00, min = -5.44, max = -5.44, count = 1
2026-02-10 18:08:47,179 : evaluate.evaluate : INFO : Evaluation lengths: mean = 371.00, std = 0.00, min = 371.00, max = 371.00, count = 1
2026-02-10 18:08:47,180 : evaluate.evaluate : INFO : Covariance matrix:
[0.2794343  0.24472088 0.26059842 0.47617483]
2026-02-10 18:25:22,342 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 18:25:28,105 : evaluate.evaluate : INFO : Evaluation results: mean = 41.29, std = 0.00, min = 41.29, max = 41.29, count = 1
2026-02-10 18:25:28,106 : evaluate.evaluate : INFO : Evaluation lengths: mean = 592.00, std = 0.00, min = 592.00, max = 592.00, count = 1
2026-02-10 18:25:28,106 : evaluate.evaluate : INFO : Covariance matrix:
[0.21424627 0.22533955 0.17604609 0.4009047 ]
2026-02-10 18:25:41,777 : envs.wrappers : INFO : Env truncated due to lack of movement
2026-02-10 18:35:06,858 : envs.wrappers : INFO : Env truncated due to lack of movement
2026-02-10 18:39:50,005 : envs.wrappers : INFO : Env truncated due to lack of movement
2026-02-10 18:41:55,752 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 18:42:06,897 : evaluate.evaluate : INFO : Evaluation results: mean = 277.32, std = 0.00, min = 277.32, max = 277.32, count = 1
2026-02-10 18:42:06,897 : evaluate.evaluate : INFO : Evaluation lengths: mean = 950.00, std = 0.00, min = 950.00, max = 950.00, count = 1
2026-02-10 18:42:06,898 : evaluate.evaluate : INFO : Covariance matrix:
[0.20293698 0.17012829 0.09504773 0.37195456]
2026-02-10 18:44:14,998 : envs.wrappers : INFO : Env truncated due to lack of movement
2026-02-10 18:45:26,054 : envs.wrappers : INFO : Env truncated due to lack of movement
2026-02-10 18:48:43,629 : envs.wrappers : INFO : Env truncated due to lack of movement
2026-02-10 18:58:33,185 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 18:58:35,211 : evaluate.evaluate : INFO : Evaluation results: mean = -50.42, std = 0.00, min = -50.42, max = -50.42, count = 1
2026-02-10 18:58:35,211 : evaluate.evaluate : INFO : Evaluation lengths: mean = 253.00, std = 0.00, min = 253.00, max = 253.00, count = 1
2026-02-10 18:58:35,212 : evaluate.evaluate : INFO : Covariance matrix:
[0.2521601  0.2140645  0.10257444 0.36299032]
2026-02-10 18:59:25,246 : envs.wrappers : INFO : Env truncated due to lack of movement
2026-02-10 19:15:14,554 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 19:15:26,055 : evaluate.evaluate : INFO : Evaluation results: mean = 276.54, std = 0.00, min = 276.54, max = 276.54, count = 1
2026-02-10 19:15:26,057 : evaluate.evaluate : INFO : Evaluation lengths: mean = 984.00, std = 0.00, min = 984.00, max = 984.00, count = 1
2026-02-10 19:15:26,058 : evaluate.evaluate : INFO : Covariance matrix:
[0.22502771 0.2690496  0.1382704  0.330769  ]
2026-02-10 19:31:57,032 : network.model : INFO : Saved model to logs/BipedalWalkerHardcore/PPO/v1/model.pt
2026-02-10 19:32:02,408 : evaluate.evaluate : INFO : Evaluation results: mean = 47.10, std = 0.00, min = 47.10, max = 47.10, count = 1
2026-02-10 19:32:02,408 : evaluate.evaluate : INFO : Evaluation lengths: mean = 541.00, std = 0.00, min = 541.00, max = 541.00, count = 1
2026-02-10 19:32:02,409 : evaluate.evaluate : INFO : Covariance matrix:
[0.1548822  0.23540285 0.12730396 0.22779502]
2026-02-10 19:36:18,510 : envs.wrappers : INFO : Env truncated due to lack of movement
2026-02-10 19:43:28,649 : envs.wrappers : INFO : Env truncated due to lack of movement
