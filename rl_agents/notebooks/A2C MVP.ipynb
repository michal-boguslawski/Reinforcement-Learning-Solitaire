{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d360fb9",
   "metadata": {},
   "source": [
    "# 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4638645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import make\n",
    "import torch as T\n",
    "from torch import nn\n",
    "from torch.distributions import Normal, TransformedDistribution, TanhTransform, AffineTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f08949",
   "metadata": {},
   "source": [
    "# 2. Creating network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cfcf93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(T.zeros(1, requires_grad=True))\n",
    "        # self.log_std = nn.Parameter(-1 * T.ones(1, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        actor_mean = self.actor(x)\n",
    "        critic_value = self.critic(x)\n",
    "        std = T.exp(self.log_std)\n",
    "        base_dist = Normal(loc=actor_mean, scale=std)\n",
    "        dist = TransformedDistribution(\n",
    "            base_dist,\n",
    "            [TanhTransform(), AffineTransform(loc=0, scale=1)]\n",
    "        )\n",
    "        return actor_mean, critic_value, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2350631",
   "metadata": {},
   "source": [
    "# 3. Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2b528",
   "metadata": {},
   "source": [
    "## 3.1. Creating objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a022fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"MountainCarContinuous-v0\")\n",
    "\n",
    "network = ActorCriticNetwork()\n",
    "optimizer = T.optim.Adam(network.parameters(), lr=2e-4)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "train_step = 2048\n",
    "\n",
    "# parameters\n",
    "gamma_ = 0.99\n",
    "lambda_ = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396c4b4",
   "metadata": {},
   "source": [
    "## 3.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f95bea",
   "metadata": {},
   "source": [
    "<li>states: torch.Size([32, 2]) <br>\n",
    "<li>actons: torch.Size([32, 1]) <br>\n",
    "<li>rewards: torch.Size([32]) <br>\n",
    "<li>next_states: torch.Size([32, 2]) <br>\n",
    "<li>dones: torch.Size([32]) <br>\n",
    "<li>critic_values: torch.Size([32, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2b65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 20479, last rewards: -39.30, max reward: -38.53\n",
      "Loss: 0.0339, policy loss: 0.0245, critic_loss: 0.0472, entropy: 1.4111\n",
      "Returns: 0.6478, advantages: 0.3513, actions: 0.0292, actions std: 0.6247\n",
      "Step: 40959, last rewards: -39.08, max reward: -36.53\n",
      "Loss: 1.1020, policy loss: 0.0116, critic_loss: 2.2089, entropy: 1.3992\n",
      "Returns: 1.7788, advantages: 1.0982, actions: -0.0177, actions std: 0.6297\n",
      "Step: 61439, last rewards: -38.80, max reward: -37.30\n",
      "Loss: 0.0515, policy loss: -0.0143, critic_loss: 0.1594, entropy: 1.3889\n",
      "Returns: 0.8924, advantages: -0.2257, actions: -0.0589, actions std: 0.6216\n",
      "Step: 81919, last rewards: -38.09, max reward: -36.77\n",
      "Loss: 0.0865, policy loss: -0.0025, critic_loss: 0.2057, entropy: 1.3802\n",
      "Returns: 1.7815, advantages: 0.2351, actions: 0.0397, actions std: 0.6173\n",
      "Step: 102399, last rewards: -37.91, max reward: -36.49\n",
      "Loss: 0.0333, policy loss: 0.0220, critic_loss: 0.0500, entropy: 1.3689\n",
      "Returns: 1.8006, advantages: 0.1893, actions: -0.0306, actions std: 0.6197\n",
      "Step: 122879, last rewards: -37.49, max reward: -36.44\n",
      "Loss: 0.0450, policy loss: 0.0093, critic_loss: 0.0985, entropy: 1.3595\n",
      "Returns: 1.7296, advantages: -0.0367, actions: -0.0151, actions std: 0.6142\n",
      "Step: 143359, last rewards: -36.98, max reward: -35.36\n",
      "Loss: 0.0154, policy loss: 0.0056, critic_loss: 0.0467, entropy: 1.3530\n",
      "Returns: 1.9830, advantages: -0.0873, actions: -0.0113, actions std: 0.6134\n",
      "Step: 163839, last rewards: -36.54, max reward: -34.72\n",
      "Loss: 0.4048, policy loss: 0.0040, critic_loss: 0.8286, entropy: 1.3463\n",
      "Returns: 2.7611, advantages: 0.5571, actions: -0.0270, actions std: 0.6069\n",
      "Step: 184319, last rewards: -36.15, max reward: -34.21\n",
      "Loss: -0.0048, policy loss: -0.0058, critic_loss: 0.0287, entropy: 1.3375\n",
      "Returns: 2.2957, advantages: 0.0700, actions: -0.0673, actions std: 0.6021\n",
      "Step: 204799, last rewards: -36.30, max reward: -35.30\n",
      "Loss: 0.6121, policy loss: 0.0034, critic_loss: 1.2439, entropy: 1.3272\n",
      "Returns: 2.5769, advantages: 0.3600, actions: 0.0111, actions std: 0.5979\n",
      "Step: 225279, last rewards: -35.37, max reward: -34.39\n",
      "Loss: 0.2826, policy loss: 0.0000, critic_loss: 0.5916, entropy: 1.3207\n",
      "Returns: 2.8328, advantages: 0.4627, actions: -0.0501, actions std: 0.5907\n",
      "Step: 245759, last rewards: -35.72, max reward: -33.95\n",
      "Loss: 0.0502, policy loss: -0.0053, critic_loss: 0.1374, entropy: 1.3132\n",
      "Returns: 2.3696, advantages: -0.1585, actions: -0.0435, actions std: 0.6045\n",
      "Step: 266239, last rewards: -35.75, max reward: -34.67\n",
      "Loss: 0.0569, policy loss: 0.0150, critic_loss: 0.1099, entropy: 1.3028\n",
      "Returns: 2.5135, advantages: 0.1366, actions: -0.0057, actions std: 0.5952\n",
      "Step: 286719, last rewards: -35.63, max reward: -34.04\n",
      "Loss: 0.0834, policy loss: 0.0078, critic_loss: 0.1771, entropy: 1.2954\n",
      "Returns: 2.0397, advantages: -0.4089, actions: -0.1068, actions std: 0.5867\n",
      "Step: 307199, last rewards: -34.59, max reward: -31.60\n",
      "Loss: 0.2034, policy loss: -0.0129, critic_loss: 0.4584, entropy: 1.2878\n",
      "Returns: 2.6256, advantages: 0.1887, actions: -0.0491, actions std: 0.5817\n",
      "Step: 327679, last rewards: -34.71, max reward: -33.78\n",
      "Loss: 0.2830, policy loss: 0.0038, critic_loss: 0.5840, entropy: 1.2800\n",
      "Returns: 3.0615, advantages: 0.6684, actions: -0.0305, actions std: 0.5852\n",
      "Step: 348159, last rewards: -34.62, max reward: -33.38\n",
      "Loss: 0.0176, policy loss: -0.0029, critic_loss: 0.0664, entropy: 1.2713\n",
      "Returns: 2.7093, advantages: 0.3458, actions: -0.0860, actions std: 0.5763\n",
      "Step: 368639, last rewards: -33.87, max reward: -32.42\n",
      "Loss: 0.0732, policy loss: 0.0078, critic_loss: 0.1562, entropy: 1.2616\n",
      "Returns: 2.5973, advantages: 0.0087, actions: 0.0444, actions std: 0.5760\n",
      "Step: 389119, last rewards: -33.51, max reward: -32.43\n",
      "Loss: 0.0198, policy loss: 0.0084, critic_loss: 0.0477, entropy: 1.2507\n",
      "Returns: 2.4980, advantages: -0.0214, actions: -0.0136, actions std: 0.5734\n",
      "Step: 409599, last rewards: -23.04, max reward: 68.91\n",
      "Loss: 0.0950, policy loss: -0.0093, critic_loss: 0.2335, entropy: 1.2413\n",
      "Returns: 2.4345, advantages: -0.2026, actions: 0.1039, actions std: 0.5667\n",
      "Step: 430079, last rewards: -22.40, max reward: 69.98\n",
      "Loss: 0.0189, policy loss: 0.0016, critic_loss: 0.0592, entropy: 1.2337\n",
      "Returns: 2.8553, advantages: 0.1991, actions: 0.0182, actions std: 0.5763\n",
      "Step: 450559, last rewards: -32.23, max reward: -31.38\n",
      "Loss: 0.1049, policy loss: -0.0020, critic_loss: 0.2383, entropy: 1.2249\n",
      "Returns: 2.7704, advantages: 0.2497, actions: 0.0123, actions std: 0.5716\n",
      "Step: 471039, last rewards: -32.70, max reward: -31.81\n",
      "Loss: 0.0646, policy loss: -0.0036, critic_loss: 0.1608, entropy: 1.2148\n",
      "Returns: 1.9536, advantages: -0.5438, actions: 0.0046, actions std: 0.5674\n",
      "Step: 491519, last rewards: -32.06, max reward: -29.85\n",
      "Loss: 0.1301, policy loss: -0.0019, critic_loss: 0.2881, entropy: 1.2066\n",
      "Returns: 2.9834, advantages: 0.3032, actions: 0.0173, actions std: 0.5687\n",
      "Step: 511999, last rewards: -32.02, max reward: -30.17\n",
      "Loss: 0.1313, policy loss: 0.0045, critic_loss: 0.2776, entropy: 1.1959\n",
      "Returns: 2.6437, advantages: -0.1237, actions: -0.0238, actions std: 0.5627\n",
      "Step: 532479, last rewards: -31.64, max reward: -29.00\n",
      "Loss: 0.0764, policy loss: -0.0125, critic_loss: 0.2015, entropy: 1.1902\n",
      "Returns: 2.9144, advantages: -0.1443, actions: -0.0386, actions std: 0.5709\n",
      "Step: 552959, last rewards: -31.24, max reward: -29.26\n",
      "Loss: 0.1549, policy loss: -0.0020, critic_loss: 0.3373, entropy: 1.1823\n",
      "Returns: 3.1266, advantages: -0.0455, actions: 0.0433, actions std: 0.5488\n",
      "Step: 573439, last rewards: -30.78, max reward: -28.75\n",
      "Loss: 0.1340, policy loss: 0.0012, critic_loss: 0.2890, entropy: 1.1746\n",
      "Returns: 2.7932, advantages: -0.2658, actions: -0.0013, actions std: 0.5628\n",
      "Step: 593919, last rewards: -30.62, max reward: -29.47\n",
      "Loss: 0.7065, policy loss: 0.0153, critic_loss: 1.4056, entropy: 1.1659\n",
      "Returns: 3.1867, advantages: 0.3337, actions: -0.0455, actions std: 0.5546\n",
      "Step: 614399, last rewards: -30.89, max reward: -28.64\n",
      "Loss: 0.3500, policy loss: -0.0100, critic_loss: 0.7432, entropy: 1.1569\n",
      "Returns: 4.5152, advantages: 1.6474, actions: -0.0026, actions std: 0.5557\n",
      "Step: 634879, last rewards: -30.66, max reward: -29.79\n",
      "Loss: 0.0222, policy loss: -0.0175, critic_loss: 0.1024, entropy: 1.1497\n",
      "Returns: 2.6463, advantages: -0.3701, actions: -0.1126, actions std: 0.5488\n",
      "Step: 655359, last rewards: -30.00, max reward: -28.21\n",
      "Loss: 0.1869, policy loss: 0.0234, critic_loss: 0.3498, entropy: 1.1402\n",
      "Returns: 3.3263, advantages: 0.4531, actions: 0.0739, actions std: 0.5477\n",
      "Step: 675839, last rewards: -29.56, max reward: -28.28\n",
      "Loss: 0.3803, policy loss: -0.0345, critic_loss: 0.8522, entropy: 1.1334\n",
      "Returns: 2.9284, advantages: 0.0759, actions: -0.0641, actions std: 0.5483\n",
      "Step: 696319, last rewards: -29.50, max reward: -28.60\n",
      "Loss: 0.0647, policy loss: -0.0251, critic_loss: 0.2021, entropy: 1.1248\n",
      "Returns: 2.8010, advantages: -0.0107, actions: 0.0097, actions std: 0.5421\n",
      "Step: 716799, last rewards: -29.47, max reward: -28.29\n",
      "Loss: 0.0227, policy loss: -0.0046, critic_loss: 0.0770, entropy: 1.1151\n",
      "Returns: 2.4685, advantages: -0.3724, actions: 0.0230, actions std: 0.5469\n",
      "Step: 737279, last rewards: -29.08, max reward: -27.63\n",
      "Loss: 0.2967, policy loss: 0.0407, critic_loss: 0.5341, entropy: 1.1060\n",
      "Returns: 3.6941, advantages: 0.8789, actions: -0.1253, actions std: 0.5376\n",
      "Step: 757759, last rewards: -28.68, max reward: -27.84\n",
      "Loss: -0.0120, policy loss: -0.0254, critic_loss: 0.0486, entropy: 1.0970\n",
      "Returns: 2.7503, advantages: -0.0017, actions: -0.0020, actions std: 0.5353\n",
      "Step: 778239, last rewards: -28.25, max reward: -27.33\n",
      "Loss: 0.3045, policy loss: -0.0584, critic_loss: 0.7475, entropy: 1.0869\n",
      "Returns: 3.0650, advantages: 0.4441, actions: 0.0310, actions std: 0.5339\n",
      "Step: 798719, last rewards: -27.97, max reward: -27.06\n",
      "Loss: 0.0450, policy loss: -0.0218, critic_loss: 0.1551, entropy: 1.0777\n",
      "Returns: 3.1182, advantages: 0.4943, actions: 0.0451, actions std: 0.5272\n",
      "Step: 819199, last rewards: -28.27, max reward: -26.65\n",
      "Loss: 0.1594, policy loss: -0.0025, critic_loss: 0.3451, entropy: 1.0703\n",
      "Returns: 3.4363, advantages: 0.6376, actions: 0.0873, actions std: 0.5258\n",
      "Step: 839679, last rewards: -27.71, max reward: -26.47\n",
      "Loss: 0.0170, policy loss: -0.0055, critic_loss: 0.0661, entropy: 1.0609\n",
      "Returns: 2.7406, advantages: -0.0999, actions: 0.0446, actions std: 0.5152\n",
      "Step: 860159, last rewards: -27.11, max reward: -25.36\n",
      "Loss: 0.0832, policy loss: -0.0314, critic_loss: 0.2502, entropy: 1.0548\n",
      "Returns: 4.5479, advantages: 1.4247, actions: -0.0835, actions std: 0.5240\n",
      "Step: 880639, last rewards: -26.90, max reward: -25.01\n",
      "Loss: 0.3996, policy loss: -0.0050, critic_loss: 0.8299, entropy: 1.0454\n",
      "Returns: 3.3093, advantages: 0.1646, actions: 0.0091, actions std: 0.5208\n",
      "Step: 901119, last rewards: -27.61, max reward: -26.34\n",
      "Loss: 0.2174, policy loss: 0.0124, critic_loss: 0.4307, entropy: 1.0384\n",
      "Returns: 3.4680, advantages: 0.1053, actions: -0.1241, actions std: 0.5110\n",
      "Step: 921599, last rewards: -26.40, max reward: -24.89\n",
      "Loss: 0.0364, policy loss: 0.0136, critic_loss: 0.0662, entropy: 1.0295\n",
      "Returns: 3.6435, advantages: 0.3510, actions: -0.0085, actions std: 0.5197\n",
      "Step: 942079, last rewards: -26.55, max reward: -25.27\n",
      "Loss: 0.1081, policy loss: -0.0197, critic_loss: 0.2762, entropy: 1.0208\n",
      "Returns: 3.3085, advantages: -0.0080, actions: 0.0657, actions std: 0.5142\n",
      "Step: 962559, last rewards: -26.50, max reward: -25.03\n",
      "Loss: 0.0786, policy loss: -0.0196, critic_loss: 0.2167, entropy: 1.0123\n",
      "Returns: 3.5572, advantages: 0.3332, actions: 0.0396, actions std: 0.5173\n",
      "Step: 983039, last rewards: -25.63, max reward: -24.25\n",
      "Loss: 0.1047, policy loss: 0.0005, critic_loss: 0.2286, entropy: 1.0048\n",
      "Returns: 3.2231, advantages: -0.2194, actions: -0.0166, actions std: 0.5018\n",
      "Step: 1003519, last rewards: -26.00, max reward: -25.10\n",
      "Loss: 0.0426, policy loss: -0.0146, critic_loss: 0.1344, entropy: 0.9988\n",
      "Returns: 4.3898, advantages: 0.7048, actions: 0.0022, actions std: 0.5074\n",
      "Step: 1023999, last rewards: -25.91, max reward: -24.00\n",
      "Loss: 0.0864, policy loss: -0.0398, critic_loss: 0.2723, entropy: 0.9908\n",
      "Returns: 3.2017, advantages: -0.5519, actions: 0.0522, actions std: 0.4936\n",
      "Step: 1044479, last rewards: -25.68, max reward: -24.80\n",
      "Loss: 0.0076, policy loss: -0.0446, critic_loss: 0.1241, entropy: 0.9811\n",
      "Returns: 3.7437, advantages: 0.1116, actions: -0.0174, actions std: 0.4997\n",
      "Step: 1064959, last rewards: -25.72, max reward: -24.20\n",
      "Loss: 0.1138, policy loss: -0.0234, critic_loss: 0.2939, entropy: 0.9729\n",
      "Returns: 4.1034, advantages: 0.3436, actions: 0.0807, actions std: 0.5037\n",
      "Step: 1085439, last rewards: -25.22, max reward: -23.46\n",
      "Loss: 0.0872, policy loss: -0.0169, critic_loss: 0.2276, entropy: 0.9648\n",
      "Returns: 3.7132, advantages: -0.1936, actions: 0.0530, actions std: 0.4956\n",
      "Step: 1105919, last rewards: -25.11, max reward: -23.83\n",
      "Loss: 0.2657, policy loss: -0.0208, critic_loss: 0.5921, entropy: 0.9582\n",
      "Returns: 4.3038, advantages: 0.4037, actions: -0.0251, actions std: 0.4994\n",
      "Step: 1126399, last rewards: -25.05, max reward: -23.96\n",
      "Loss: 0.2380, policy loss: -0.0072, critic_loss: 0.5095, entropy: 0.9525\n",
      "Returns: 4.4356, advantages: 0.4178, actions: 0.1050, actions std: 0.4939\n",
      "Step: 1146879, last rewards: -25.08, max reward: -23.57\n",
      "Loss: 0.0781, policy loss: -0.0002, critic_loss: 0.1755, entropy: 0.9460\n",
      "Returns: 4.4737, advantages: 0.3876, actions: -0.0644, actions std: 0.4922\n",
      "Step: 1167359, last rewards: -25.33, max reward: -23.59\n",
      "Loss: 0.0115, policy loss: -0.0322, critic_loss: 0.1064, entropy: 0.9412\n",
      "Returns: 4.1762, advantages: -0.1065, actions: 0.0954, actions std: 0.4823\n",
      "Step: 1187839, last rewards: -25.10, max reward: -23.07\n",
      "Loss: 0.0863, policy loss: -0.0494, critic_loss: 0.2902, entropy: 0.9357\n",
      "Returns: 4.8555, advantages: 0.6695, actions: -0.0721, actions std: 0.4969\n",
      "Step: 1208319, last rewards: -25.54, max reward: -23.39\n",
      "Loss: 0.0257, policy loss: -0.0408, critic_loss: 0.1515, entropy: 0.9283\n",
      "Returns: 4.8112, advantages: 0.5518, actions: -0.1046, actions std: 0.4990\n",
      "Step: 1228799, last rewards: -25.09, max reward: -22.91\n",
      "Loss: 0.2341, policy loss: -0.0617, critic_loss: 0.6100, entropy: 0.9218\n",
      "Returns: 5.6659, advantages: 1.5125, actions: -0.0460, actions std: 0.5127\n",
      "Step: 1249279, last rewards: -24.01, max reward: -22.78\n",
      "Loss: -0.0134, policy loss: -0.0382, critic_loss: 0.0678, entropy: 0.9155\n",
      "Returns: 5.5480, advantages: 1.2676, actions: -0.0061, actions std: 0.4963\n",
      "Step: 1269759, last rewards: -24.65, max reward: -22.70\n",
      "Loss: 0.1696, policy loss: -0.0380, critic_loss: 0.4334, entropy: 0.9086\n",
      "Returns: 5.4495, advantages: 1.1201, actions: -0.0012, actions std: 0.5127\n",
      "Step: 1290239, last rewards: -24.31, max reward: -22.14\n",
      "Loss: 0.7996, policy loss: -0.0107, critic_loss: 1.6387, entropy: 0.9028\n",
      "Returns: 5.6155, advantages: 1.1740, actions: -0.0876, actions std: 0.5014\n",
      "Step: 1310719, last rewards: -24.61, max reward: -21.98\n",
      "Loss: 0.1388, policy loss: -0.0159, critic_loss: 0.3274, entropy: 0.8971\n",
      "Returns: 4.4657, advantages: -0.0643, actions: 0.1352, actions std: 0.4783\n",
      "Step: 1331199, last rewards: -24.43, max reward: -21.90\n",
      "Loss: 0.9461, policy loss: -0.0165, critic_loss: 1.9432, entropy: 0.8911\n",
      "Returns: 5.2192, advantages: 0.6628, actions: 0.0565, actions std: 0.4856\n",
      "Step: 1351679, last rewards: -24.12, max reward: -22.14\n",
      "Loss: 0.0928, policy loss: -0.0018, critic_loss: 0.2070, entropy: 0.8861\n",
      "Returns: 4.8728, advantages: 0.2887, actions: 0.0809, actions std: 0.4797\n",
      "Step: 1372159, last rewards: -23.37, max reward: -21.88\n",
      "Loss: -0.0051, policy loss: -0.0597, critic_loss: 0.1268, entropy: 0.8795\n",
      "Returns: 4.6766, advantages: -0.0067, actions: -0.1129, actions std: 0.4758\n",
      "Step: 1392639, last rewards: -15.87, max reward: 73.37\n",
      "Loss: -0.0004, policy loss: -0.0213, critic_loss: 0.0593, entropy: 0.8730\n",
      "Returns: 5.0476, advantages: 0.4859, actions: 0.1410, actions std: 0.4724\n",
      "Step: 1413119, last rewards: -22.55, max reward: -20.85\n",
      "Loss: 0.2540, policy loss: -0.0434, critic_loss: 0.6121, entropy: 0.8667\n",
      "Returns: 4.5267, advantages: -0.1570, actions: 0.0347, actions std: 0.4594\n",
      "Step: 1433599, last rewards: -23.50, max reward: -20.88\n",
      "Loss: -0.0083, policy loss: -0.0278, critic_loss: 0.0561, entropy: 0.8619\n",
      "Returns: 5.3603, advantages: 0.7661, actions: 0.0079, actions std: 0.4938\n",
      "Step: 1454079, last rewards: -24.74, max reward: -21.65\n",
      "Loss: 0.0284, policy loss: -0.0065, critic_loss: 0.0869, entropy: 0.8578\n",
      "Returns: 4.6910, advantages: -0.0506, actions: 0.1209, actions std: 0.4588\n",
      "Step: 1474559, last rewards: -12.77, max reward: 73.64\n",
      "Loss: 0.0123, policy loss: -0.0345, critic_loss: 0.1107, entropy: 0.8527\n",
      "Returns: 6.7004, advantages: 1.9812, actions: 0.0169, actions std: 0.4959\n",
      "Step: 1495039, last rewards: -12.68, max reward: 77.15\n",
      "Loss: 0.1285, policy loss: -0.0446, critic_loss: 0.3631, entropy: 0.8469\n",
      "Returns: 5.4223, advantages: 0.7698, actions: 0.1200, actions std: 0.4696\n",
      "Step: 1515519, last rewards: -22.20, max reward: -20.25\n",
      "Loss: 0.0946, policy loss: -0.0289, critic_loss: 0.2637, entropy: 0.8392\n",
      "Returns: 5.2614, advantages: 0.7461, actions: 0.0174, actions std: 0.4830\n",
      "Step: 1535999, last rewards: -23.39, max reward: -22.72\n",
      "Loss: 0.9156, policy loss: -0.0340, critic_loss: 1.9159, entropy: 0.8361\n",
      "Returns: 5.7897, advantages: 1.0518, actions: -0.0072, actions std: 0.4833\n",
      "Step: 1556479, last rewards: -22.08, max reward: -20.07\n",
      "Loss: -0.0496, policy loss: -0.0589, critic_loss: 0.0351, entropy: 0.8299\n",
      "Returns: 5.8480, advantages: 0.9201, actions: 0.0220, actions std: 0.4741\n",
      "Step: 1576959, last rewards: -22.28, max reward: -19.74\n",
      "Loss: 0.1236, policy loss: -0.0635, critic_loss: 0.3907, entropy: 0.8220\n",
      "Returns: 4.6193, advantages: -0.3397, actions: 0.0184, actions std: 0.4555\n",
      "Step: 1597439, last rewards: -22.64, max reward: -19.80\n",
      "Loss: 0.1296, policy loss: -0.0217, critic_loss: 0.3191, entropy: 0.8158\n",
      "Returns: 4.9955, advantages: -0.0929, actions: 0.0113, actions std: 0.4586\n",
      "Step: 1617919, last rewards: -21.42, max reward: -19.48\n",
      "Loss: -0.0344, policy loss: -0.0490, critic_loss: 0.0455, entropy: 0.8112\n",
      "Returns: 4.7839, advantages: -0.3737, actions: -0.0125, actions std: 0.4539\n",
      "Step: 1638399, last rewards: -22.26, max reward: -20.60\n",
      "Loss: 0.0017, policy loss: -0.0414, critic_loss: 0.1022, entropy: 0.8050\n",
      "Returns: 5.2510, advantages: 0.0650, actions: -0.1041, actions std: 0.4479\n",
      "Step: 1658879, last rewards: -22.74, max reward: -18.70\n",
      "Loss: 1.5529, policy loss: -0.0007, critic_loss: 3.1231, entropy: 0.7989\n",
      "Returns: 6.9900, advantages: 1.7911, actions: -0.0848, actions std: 0.5011\n",
      "Step: 1679359, last rewards: -21.22, max reward: -18.97\n",
      "Loss: 0.2018, policy loss: 0.0236, critic_loss: 0.3722, entropy: 0.7924\n",
      "Returns: 5.2154, advantages: -0.0375, actions: -0.0679, actions std: 0.4528\n",
      "Step: 1699839, last rewards: -21.02, max reward: -19.27\n",
      "Loss: 0.1993, policy loss: -0.0422, critic_loss: 0.4986, entropy: 0.7856\n",
      "Returns: 4.8896, advantages: -0.2791, actions: -0.0628, actions std: 0.4429\n",
      "Step: 1720319, last rewards: -21.40, max reward: -18.96\n",
      "Loss: -0.0414, policy loss: -0.0784, critic_loss: 0.0895, entropy: 0.7780\n",
      "Returns: 6.0323, advantages: 0.9261, actions: 0.0056, actions std: 0.4812\n",
      "Step: 1740799, last rewards: -22.64, max reward: -18.92\n",
      "Loss: 0.4881, policy loss: -0.0269, critic_loss: 1.0456, entropy: 0.7729\n",
      "Returns: 6.1608, advantages: 0.9495, actions: -0.0563, actions std: 0.4653\n",
      "Step: 1761279, last rewards: -21.63, max reward: -18.54\n",
      "Loss: 0.0880, policy loss: -0.0130, critic_loss: 0.2173, entropy: 0.7678\n",
      "Returns: 5.6250, advantages: 0.3302, actions: 0.0838, actions std: 0.4530\n",
      "Step: 1781759, last rewards: -22.24, max reward: -18.19\n",
      "Loss: 1.7383, policy loss: 0.0520, critic_loss: 3.3879, entropy: 0.7608\n",
      "Returns: 6.7227, advantages: 1.4897, actions: -0.0351, actions std: 0.5034\n",
      "Step: 1802239, last rewards: -21.76, max reward: -19.04\n",
      "Loss: 0.0331, policy loss: -0.0243, critic_loss: 0.1301, entropy: 0.7572\n",
      "Returns: 6.1891, advantages: 0.7296, actions: -0.0413, actions std: 0.4639\n",
      "Step: 1822719, last rewards: -22.63, max reward: -18.10\n",
      "Loss: 1.3815, policy loss: -0.0428, critic_loss: 2.8636, entropy: 0.7539\n",
      "Returns: 6.4222, advantages: 0.8406, actions: -0.0101, actions std: 0.4682\n",
      "Step: 1843199, last rewards: -22.26, max reward: -17.30\n",
      "Loss: 0.1806, policy loss: 0.0087, critic_loss: 0.3588, entropy: 0.7506\n",
      "Returns: 5.5716, advantages: -0.1667, actions: -0.0143, actions std: 0.4432\n",
      "Step: 1863679, last rewards: -20.61, max reward: -18.50\n",
      "Loss: 0.3256, policy loss: 0.0480, critic_loss: 0.5703, entropy: 0.7459\n",
      "Returns: 6.5345, advantages: 0.8167, actions: -0.0015, actions std: 0.4626\n",
      "Step: 1884159, last rewards: -21.01, max reward: -17.91\n",
      "Loss: 0.0892, policy loss: -0.0168, critic_loss: 0.2268, entropy: 0.7426\n",
      "Returns: 6.6408, advantages: 0.8661, actions: -0.0135, actions std: 0.4745\n",
      "Step: 1904639, last rewards: -20.01, max reward: -17.61\n",
      "Loss: 0.2317, policy loss: -0.0500, critic_loss: 0.5782, entropy: 0.7366\n",
      "Returns: 5.5646, advantages: -0.2577, actions: -0.0026, actions std: 0.4309\n",
      "Step: 1925119, last rewards: -21.13, max reward: -18.72\n",
      "Loss: -0.0194, policy loss: -0.0802, critic_loss: 0.1362, entropy: 0.7302\n",
      "Returns: 5.3423, advantages: -0.3606, actions: 0.0383, actions std: 0.4333\n",
      "Step: 1945599, last rewards: -22.37, max reward: -18.51\n",
      "Loss: 0.2486, policy loss: -0.0534, critic_loss: 0.6185, entropy: 0.7255\n",
      "Returns: 5.7671, advantages: -0.0229, actions: -0.0228, actions std: 0.4367\n",
      "Step: 1966079, last rewards: -20.68, max reward: -17.72\n",
      "Loss: 0.3608, policy loss: -0.0220, critic_loss: 0.7800, entropy: 0.7182\n",
      "Returns: 7.2542, advantages: 1.4750, actions: 0.0082, actions std: 0.4798\n",
      "Step: 1986559, last rewards: -20.27, max reward: -18.59\n",
      "Loss: 1.3878, policy loss: -0.0354, critic_loss: 2.8607, entropy: 0.7119\n",
      "Returns: 6.7016, advantages: 0.9129, actions: 0.0304, actions std: 0.4730\n",
      "Step: 2007039, last rewards: -12.64, max reward: 77.51\n",
      "Loss: 0.0523, policy loss: -0.0166, critic_loss: 0.1521, entropy: 0.7079\n",
      "Returns: 6.9859, advantages: 1.1593, actions: 0.0597, actions std: 0.4720\n",
      "Step: 2027519, last rewards: -21.01, max reward: -18.35\n",
      "Loss: 0.8370, policy loss: 0.0579, critic_loss: 1.5723, entropy: 0.7025\n",
      "Returns: 6.0540, advantages: 0.1967, actions: -0.0052, actions std: 0.4488\n",
      "Step: 2047999, last rewards: -22.48, max reward: -18.92\n",
      "Loss: 0.0639, policy loss: 0.0159, critic_loss: 0.1100, entropy: 0.6985\n",
      "Returns: 6.9076, advantages: 0.8290, actions: 0.0183, actions std: 0.4641\n",
      "Step: 2068479, last rewards: -20.62, max reward: -16.85\n",
      "Loss: 1.0528, policy loss: 0.0457, critic_loss: 2.0280, entropy: 0.6948\n",
      "Returns: 6.3567, advantages: 0.2405, actions: 0.0013, actions std: 0.4464\n",
      "Step: 2088959, last rewards: -20.45, max reward: -16.49\n",
      "Loss: 0.1542, policy loss: -0.0264, critic_loss: 0.3750, entropy: 0.6913\n",
      "Returns: 5.7097, advantages: -0.5717, actions: 0.0063, actions std: 0.4084\n",
      "Step: 2109439, last rewards: -20.30, max reward: -17.74\n",
      "Loss: 0.8787, policy loss: 0.0505, critic_loss: 1.6701, entropy: 0.6855\n",
      "Returns: 6.5880, advantages: 0.4090, actions: -0.0122, actions std: 0.4554\n",
      "Step: 2129919, last rewards: -20.11, max reward: -16.67\n",
      "Loss: 0.7048, policy loss: 0.0151, critic_loss: 1.3931, entropy: 0.6788\n",
      "Returns: 7.3247, advantages: 1.1030, actions: -0.0657, actions std: 0.4567\n",
      "Step: 2150399, last rewards: -20.09, max reward: -16.89\n",
      "Loss: 0.5960, policy loss: 0.0016, critic_loss: 1.2025, entropy: 0.6746\n",
      "Returns: 7.1575, advantages: 0.9459, actions: -0.2112, actions std: 0.4348\n",
      "Step: 2170879, last rewards: -20.62, max reward: -18.37\n",
      "Loss: 0.0724, policy loss: 0.0077, critic_loss: 0.1428, entropy: 0.6712\n",
      "Returns: 7.6863, advantages: 1.3907, actions: -0.0438, actions std: 0.4717\n",
      "Step: 2191359, last rewards: -20.98, max reward: -15.94\n",
      "Loss: 0.1632, policy loss: 0.0073, critic_loss: 0.3253, entropy: 0.6680\n",
      "Returns: 6.7059, advantages: 0.2689, actions: -0.0201, actions std: 0.4387\n",
      "Step: 2211839, last rewards: -19.97, max reward: -16.49\n",
      "Loss: 0.3724, policy loss: -0.0238, critic_loss: 0.8057, entropy: 0.6655\n",
      "Returns: 7.3096, advantages: 0.8580, actions: 0.0045, actions std: 0.4555\n",
      "Step: 2232319, last rewards: -22.89, max reward: -18.19\n",
      "Loss: 1.5754, policy loss: 0.0425, critic_loss: 3.0790, entropy: 0.6626\n",
      "Returns: 7.4953, advantages: 0.8282, actions: -0.1148, actions std: 0.4544\n",
      "Step: 2252799, last rewards: -19.64, max reward: -17.22\n",
      "Loss: 0.0450, policy loss: -0.0165, critic_loss: 0.1361, entropy: 0.6559\n",
      "Returns: 7.7281, advantages: 1.0370, actions: -0.0697, actions std: 0.4640\n",
      "Step: 2273279, last rewards: -20.44, max reward: -17.16\n",
      "Loss: 0.8702, policy loss: 0.0231, critic_loss: 1.7072, entropy: 0.6515\n",
      "Returns: 7.1546, advantages: 0.2861, actions: -0.0301, actions std: 0.4380\n",
      "Step: 2293759, last rewards: -20.33, max reward: -16.15\n",
      "Loss: 0.3750, policy loss: 0.0086, critic_loss: 0.7457, entropy: 0.6474\n",
      "Returns: 7.5933, advantages: 0.6505, actions: -0.0426, actions std: 0.4546\n",
      "Step: 2314239, last rewards: -22.12, max reward: -17.20\n",
      "Loss: 0.8851, policy loss: -0.0168, critic_loss: 1.8167, entropy: 0.6449\n",
      "Returns: 7.2232, advantages: 0.1006, actions: -0.1170, actions std: 0.4276\n",
      "Step: 2334719, last rewards: -20.73, max reward: -16.14\n",
      "Loss: 0.5314, policy loss: -0.0195, critic_loss: 1.1147, entropy: 0.6434\n",
      "Returns: 7.5490, advantages: 0.3968, actions: -0.0542, actions std: 0.4418\n",
      "Step: 2355199, last rewards: -20.83, max reward: -16.66\n",
      "Loss: 0.8131, policy loss: -0.1244, critic_loss: 1.8877, entropy: 0.6399\n",
      "Returns: 8.7160, advantages: 1.5909, actions: -0.0674, actions std: 0.4697\n",
      "Step: 2375679, last rewards: -21.01, max reward: -18.62\n",
      "Loss: 0.0145, policy loss: -0.0416, critic_loss: 0.1249, entropy: 0.6353\n",
      "Returns: 7.2926, advantages: 0.1034, actions: -0.1319, actions std: 0.4291\n",
      "Step: 2396159, last rewards: -19.89, max reward: -15.96\n",
      "Loss: 0.4849, policy loss: -0.0489, critic_loss: 1.0803, entropy: 0.6316\n",
      "Returns: 7.5287, advantages: 0.2998, actions: 0.0914, actions std: 0.4390\n",
      "Step: 2416639, last rewards: -20.66, max reward: -18.14\n",
      "Loss: 0.4979, policy loss: -0.0396, critic_loss: 1.0876, entropy: 0.6304\n",
      "Returns: 9.1196, advantages: 1.8329, actions: -0.0995, actions std: 0.4671\n",
      "Step: 2437119, last rewards: -21.06, max reward: -17.07\n",
      "Loss: 0.7411, policy loss: -0.1215, critic_loss: 1.7378, entropy: 0.6262\n",
      "Returns: 8.3161, advantages: 0.8631, actions: -0.1173, actions std: 0.4341\n",
      "Step: 2457599, last rewards: -19.80, max reward: -14.68\n",
      "Loss: 0.0537, policy loss: 0.0205, critic_loss: 0.0788, entropy: 0.6209\n",
      "Returns: 8.0877, advantages: 0.5440, actions: -0.0370, actions std: 0.4340\n",
      "Step: 2478079, last rewards: -18.86, max reward: -15.01\n",
      "Loss: 0.3048, policy loss: -0.0261, critic_loss: 0.6741, entropy: 0.6171\n",
      "Returns: 7.5334, advantages: 0.0362, actions: 0.0167, actions std: 0.4313\n",
      "Step: 2498559, last rewards: -1.05, max reward: 80.04\n",
      "Loss: 0.7484, policy loss: 0.0053, critic_loss: 1.4983, entropy: 0.6143\n",
      "Returns: 9.1537, advantages: 1.6128, actions: 0.0516, actions std: 0.4835\n",
      "Step: 2519039, last rewards: -19.56, max reward: -16.02\n",
      "Loss: 0.1796, policy loss: -0.0240, critic_loss: 0.4194, entropy: 0.6130\n",
      "Returns: 8.0278, advantages: 0.4482, actions: -0.0985, actions std: 0.4294\n",
      "Step: 2539519, last rewards: -21.11, max reward: -17.39\n",
      "Loss: 0.3902, policy loss: 0.0585, critic_loss: 0.6757, entropy: 0.6130\n",
      "Returns: 8.2776, advantages: 0.5841, actions: -0.0979, actions std: 0.4248\n",
      "Step: 2559999, last rewards: -21.08, max reward: -17.95\n",
      "Loss: 0.3852, policy loss: 0.1185, critic_loss: 0.5455, entropy: 0.6102\n",
      "Returns: 7.8891, advantages: 0.1440, actions: -0.1345, actions std: 0.4221\n",
      "Step: 2580479, last rewards: -18.74, max reward: -14.21\n",
      "Loss: 0.5037, policy loss: -0.0549, critic_loss: 1.1293, entropy: 0.6087\n",
      "Returns: 7.8866, advantages: 0.1057, actions: 0.0219, actions std: 0.4342\n",
      "Step: 2600959, last rewards: -19.19, max reward: -15.37\n",
      "Loss: 0.2476, policy loss: -0.0512, critic_loss: 0.6097, entropy: 0.6079\n",
      "Returns: 6.8195, advantages: -0.9704, actions: 0.1151, actions std: 0.3851\n",
      "Step: 2621439, last rewards: -20.47, max reward: -16.40\n",
      "Loss: 0.0073, policy loss: -0.0097, critic_loss: 0.0462, entropy: 0.6057\n",
      "Returns: 7.7507, advantages: -0.1150, actions: 0.1097, actions std: 0.4251\n",
      "Step: 2641919, last rewards: -21.09, max reward: -15.16\n",
      "Loss: 0.2372, policy loss: -0.0949, critic_loss: 0.6764, entropy: 0.6047\n",
      "Returns: 7.8258, advantages: -0.1238, actions: -0.0390, actions std: 0.4262\n",
      "Step: 2662399, last rewards: -21.63, max reward: -17.00\n",
      "Loss: 0.6605, policy loss: -0.0234, critic_loss: 1.3797, entropy: 0.5991\n",
      "Returns: 8.9654, advantages: 0.9931, actions: 0.0404, actions std: 0.4748\n",
      "Step: 2682879, last rewards: -20.34, max reward: -16.16\n",
      "Loss: 0.0804, policy loss: -0.0980, critic_loss: 0.3688, entropy: 0.5957\n",
      "Returns: 8.6915, advantages: 0.7009, actions: -0.0933, actions std: 0.4572\n",
      "Step: 2703359, last rewards: -10.61, max reward: 84.73\n",
      "Loss: 1.0774, policy loss: -0.0839, critic_loss: 2.3342, entropy: 0.5912\n",
      "Returns: 9.8082, advantages: 1.7468, actions: 0.1006, actions std: 0.4720\n",
      "Step: 2723839, last rewards: 9.85, max reward: 83.44\n",
      "Loss: 0.2308, policy loss: 0.0124, critic_loss: 0.4486, entropy: 0.5898\n",
      "Returns: 8.4064, advantages: 0.3359, actions: 0.0928, actions std: 0.4368\n",
      "Step: 2744319, last rewards: -21.26, max reward: -16.09\n",
      "Loss: 0.3928, policy loss: 0.0277, critic_loss: 0.7418, entropy: 0.5879\n",
      "Returns: 8.1802, advantages: 0.0432, actions: -0.0271, actions std: 0.4340\n",
      "Step: 2764799, last rewards: -22.15, max reward: -15.66\n",
      "Loss: 0.1414, policy loss: -0.0398, critic_loss: 0.3741, entropy: 0.5828\n",
      "Returns: 8.8541, advantages: 0.6761, actions: -0.1163, actions std: 0.4389\n",
      "Step: 2785279, last rewards: -20.74, max reward: -14.89\n",
      "Loss: 1.6634, policy loss: -0.0199, critic_loss: 3.3782, entropy: 0.5812\n",
      "Returns: 9.0857, advantages: 0.8696, actions: -0.0893, actions std: 0.4544\n",
      "Step: 2805759, last rewards: -12.32, max reward: 80.19\n",
      "Loss: 0.0600, policy loss: -0.0229, critic_loss: 0.1774, entropy: 0.5785\n",
      "Returns: 9.2032, advantages: 0.9083, actions: 0.0960, actions std: 0.4317\n",
      "Step: 2826239, last rewards: -10.11, max reward: 79.22\n",
      "Loss: 0.1117, policy loss: -0.0790, critic_loss: 0.3930, entropy: 0.5760\n",
      "Returns: 9.5632, advantages: 1.2186, actions: -0.0493, actions std: 0.4689\n",
      "Step: 2846719, last rewards: -20.21, max reward: -15.10\n",
      "Loss: 1.0044, policy loss: -0.0700, critic_loss: 2.1602, entropy: 0.5727\n",
      "Returns: 9.7562, advantages: 1.3864, actions: -0.0321, actions std: 0.4698\n",
      "Step: 2867199, last rewards: 1.87, max reward: 83.96\n",
      "Loss: 0.1263, policy loss: -0.0065, critic_loss: 0.2770, entropy: 0.5676\n",
      "Returns: 9.7422, advantages: 1.3713, actions: 0.0941, actions std: 0.4582\n",
      "Step: 2887679, last rewards: -2.60, max reward: 81.20\n",
      "Loss: 0.8542, policy loss: -0.1196, critic_loss: 1.9589, entropy: 0.5614\n",
      "Returns: 10.0680, advantages: 1.6309, actions: 0.0368, actions std: 0.5085\n",
      "Step: 2908159, last rewards: -24.93, max reward: -21.77\n",
      "Loss: 0.9699, policy loss: -0.0705, critic_loss: 2.0920, entropy: 0.5562\n",
      "Returns: 10.4454, advantages: 1.9368, actions: -0.2104, actions std: 0.4608\n",
      "Step: 2928639, last rewards: -20.32, max reward: -15.18\n",
      "Loss: 0.5685, policy loss: -0.0467, critic_loss: 1.2414, entropy: 0.5531\n",
      "Returns: 9.1872, advantages: 0.4904, actions: -0.1026, actions std: 0.4294\n",
      "Step: 2949119, last rewards: -20.04, max reward: -14.54\n",
      "Loss: 1.3847, policy loss: -0.0736, critic_loss: 2.9276, entropy: 0.5497\n",
      "Returns: 10.5057, advantages: 1.7721, actions: -0.0939, actions std: 0.4765\n",
      "Step: 2969599, last rewards: -22.43, max reward: -17.45\n",
      "Loss: 0.2871, policy loss: -0.1817, critic_loss: 0.9484, entropy: 0.5477\n",
      "Returns: 10.1812, advantages: 1.3010, actions: -0.2097, actions std: 0.4454\n",
      "Step: 2990079, last rewards: -20.04, max reward: -14.15\n",
      "Loss: 1.1457, policy loss: -0.0357, critic_loss: 2.3738, entropy: 0.5435\n",
      "Returns: 9.3811, advantages: 0.3505, actions: -0.0374, actions std: 0.4283\n",
      "Step: 3010559, last rewards: -21.25, max reward: -19.20\n",
      "Loss: 0.6605, policy loss: -0.0185, critic_loss: 1.3687, entropy: 0.5402\n",
      "Returns: 10.4447, advantages: 1.2577, actions: -0.0048, actions std: 0.4542\n",
      "Step: 3031039, last rewards: 2.00, max reward: 86.46\n",
      "Loss: 0.4722, policy loss: -0.0418, critic_loss: 1.0388, entropy: 0.5371\n",
      "Returns: 9.9081, advantages: 0.7092, actions: -0.0298, actions std: 0.4200\n",
      "Step: 3051519, last rewards: -17.61, max reward: -14.47\n",
      "Loss: 0.2363, policy loss: -0.0629, critic_loss: 0.6091, entropy: 0.5361\n",
      "Returns: 9.5967, advantages: 0.3288, actions: 0.0172, actions std: 0.4197\n",
      "Step: 3071999, last rewards: -7.22, max reward: 78.79\n",
      "Loss: 0.2356, policy loss: 0.0156, critic_loss: 0.4506, entropy: 0.5336\n",
      "Returns: 8.0743, advantages: -1.1739, actions: 0.0495, actions std: 0.3786\n",
      "Step: 3092479, last rewards: -18.87, max reward: -16.63\n",
      "Loss: 0.0337, policy loss: -0.0493, critic_loss: 0.1766, entropy: 0.5328\n",
      "Returns: 9.3070, advantages: 0.0362, actions: -0.1810, actions std: 0.3854\n",
      "Step: 3112959, last rewards: -18.01, max reward: -14.34\n",
      "Loss: 1.2522, policy loss: -0.0188, critic_loss: 2.5525, entropy: 0.5317\n",
      "Returns: 9.5483, advantages: 0.2476, actions: -0.1238, actions std: 0.4063\n",
      "Step: 3133439, last rewards: -17.47, max reward: -12.56\n",
      "Loss: 0.3476, policy loss: 0.0089, critic_loss: 0.6881, entropy: 0.5333\n",
      "Returns: 9.5738, advantages: 0.3124, actions: -0.0584, actions std: 0.4108\n",
      "Step: 3153919, last rewards: -17.66, max reward: -12.90\n",
      "Loss: 0.2988, policy loss: 0.0396, critic_loss: 0.5290, entropy: 0.5287\n",
      "Returns: 9.3090, advantages: 0.0480, actions: -0.1056, actions std: 0.4013\n",
      "Step: 3174399, last rewards: -18.62, max reward: -13.40\n",
      "Loss: 0.0826, policy loss: -0.0385, critic_loss: 0.2528, entropy: 0.5260\n",
      "Returns: 8.4739, advantages: -0.7796, actions: 0.0330, actions std: 0.3834\n",
      "Step: 3194879, last rewards: -18.49, max reward: -14.12\n",
      "Loss: 1.0564, policy loss: 0.0187, critic_loss: 2.0859, entropy: 0.5243\n",
      "Returns: 8.7952, advantages: -0.4293, actions: 0.0435, actions std: 0.3933\n",
      "Step: 3215359, last rewards: 22.25, max reward: 85.13\n",
      "Loss: 0.8724, policy loss: -0.0017, critic_loss: 1.7587, entropy: 0.5215\n",
      "Returns: 9.9131, advantages: 0.7930, actions: 0.2290, actions std: 0.4027\n",
      "Step: 3235839, last rewards: 62.17, max reward: 84.65\n",
      "Loss: 3.5921, policy loss: -0.0771, critic_loss: 7.3486, entropy: 0.5174\n",
      "Returns: 10.9911, advantages: 1.9660, actions: 0.1621, actions std: 0.4275\n",
      "Step: 3256319, last rewards: -19.13, max reward: -13.90\n",
      "Loss: 0.5457, policy loss: 0.0330, critic_loss: 1.0357, entropy: 0.5141\n",
      "Returns: 10.0991, advantages: 1.0607, actions: -0.0392, actions std: 0.4366\n",
      "Step: 3276799, last rewards: -18.50, max reward: -12.69\n",
      "Loss: 1.2322, policy loss: -0.0054, critic_loss: 2.4853, entropy: 0.5119\n",
      "Returns: 9.4116, advantages: 0.3611, actions: 0.0085, actions std: 0.4285\n",
      "Step: 3297279, last rewards: -18.01, max reward: -12.84\n",
      "Loss: 0.1577, policy loss: -0.0447, critic_loss: 0.4150, entropy: 0.5070\n",
      "Returns: 10.2269, advantages: 1.1810, actions: -0.0870, actions std: 0.4438\n",
      "Step: 3317759, last rewards: -20.11, max reward: -15.45\n",
      "Loss: 0.6277, policy loss: -0.0162, critic_loss: 1.2980, entropy: 0.5052\n",
      "Returns: 10.2781, advantages: 1.1871, actions: -0.2004, actions std: 0.4269\n",
      "Step: 3338239, last rewards: -17.66, max reward: -12.80\n",
      "Loss: 0.1381, policy loss: -0.0485, critic_loss: 0.3832, entropy: 0.5039\n",
      "Returns: 10.0478, advantages: 0.9399, actions: -0.0722, actions std: 0.4220\n",
      "Step: 3358719, last rewards: -17.59, max reward: -13.59\n",
      "Loss: 1.0924, policy loss: -0.0063, critic_loss: 2.2075, entropy: 0.5037\n",
      "Returns: 10.0954, advantages: 0.9582, actions: -0.0619, actions std: 0.4210\n",
      "Step: 3379199, last rewards: -17.34, max reward: -12.60\n",
      "Loss: -0.0031, policy loss: -0.0626, critic_loss: 0.1291, entropy: 0.4997\n",
      "Returns: 9.4401, advantages: 0.2298, actions: 0.0269, actions std: 0.4077\n",
      "Step: 3399679, last rewards: -19.06, max reward: -14.36\n",
      "Loss: 1.2407, policy loss: -0.0003, critic_loss: 2.4919, entropy: 0.4973\n",
      "Returns: 9.8734, advantages: 0.6814, actions: -0.0143, actions std: 0.4288\n",
      "Step: 3420159, last rewards: 52.54, max reward: 89.79\n",
      "Loss: 0.9912, policy loss: 0.0268, critic_loss: 1.9386, entropy: 0.4989\n",
      "Returns: 11.5568, advantages: 2.3529, actions: 0.0204, actions std: 0.4585\n",
      "Step: 3440639, last rewards: 54.59, max reward: 87.72\n",
      "Loss: 3.8303, policy loss: 0.0281, critic_loss: 7.6142, entropy: 0.4961\n",
      "Returns: 10.9031, advantages: 1.6307, actions: 0.1317, actions std: 0.4064\n",
      "Step: 3461119, last rewards: -9.53, max reward: 84.72\n",
      "Loss: 0.2122, policy loss: -0.0340, critic_loss: 0.5025, entropy: 0.4957\n",
      "Returns: 10.6425, advantages: 1.3339, actions: -0.0572, actions std: 0.4304\n",
      "Step: 3481599, last rewards: 66.19, max reward: 88.53\n",
      "Loss: 3.8822, policy loss: -0.0115, critic_loss: 7.7973, entropy: 0.4928\n",
      "Returns: 11.7096, advantages: 2.4476, actions: 0.0597, actions std: 0.4385\n",
      "Step: 3502079, last rewards: 65.55, max reward: 88.19\n",
      "Loss: 1.3229, policy loss: -0.1215, critic_loss: 2.8985, entropy: 0.4886\n",
      "Returns: 11.3592, advantages: 2.1203, actions: 0.0724, actions std: 0.4247\n",
      "Step: 3522559, last rewards: 65.34, max reward: 86.82\n",
      "Loss: 0.1498, policy loss: -0.0637, critic_loss: 0.4365, entropy: 0.4836\n",
      "Returns: 9.1847, advantages: -0.0793, actions: 0.2255, actions std: 0.3531\n",
      "Step: 3543039, last rewards: 23.99, max reward: 87.85\n",
      "Loss: 0.1464, policy loss: 0.0553, critic_loss: 0.1918, entropy: 0.4808\n",
      "Returns: 11.4417, advantages: 2.2598, actions: 0.0543, actions std: 0.4028\n",
      "Step: 3563519, last rewards: 35.41, max reward: 91.45\n",
      "Loss: 0.7056, policy loss: -0.0479, critic_loss: 1.5166, entropy: 0.4762\n",
      "Returns: 11.6656, advantages: 2.4604, actions: 0.0240, actions std: 0.4029\n",
      "Step: 3583999, last rewards: 45.81, max reward: 88.92\n",
      "Loss: 0.3702, policy loss: -0.0298, critic_loss: 0.8096, entropy: 0.4743\n",
      "Returns: 10.4507, advantages: 1.2388, actions: 0.1081, actions std: 0.3918\n",
      "Step: 3604479, last rewards: -14.13, max reward: -11.48\n",
      "Loss: 0.6085, policy loss: -0.0005, critic_loss: 1.2274, entropy: 0.4698\n",
      "Returns: 7.7421, advantages: -1.4072, actions: -0.0090, actions std: 0.3492\n",
      "Step: 3624959, last rewards: -4.57, max reward: 85.34\n",
      "Loss: 0.0656, policy loss: -0.0247, critic_loss: 0.1899, entropy: 0.4668\n",
      "Returns: 8.2634, advantages: -0.8332, actions: 0.0887, actions std: 0.3552\n",
      "Step: 3645439, last rewards: 26.75, max reward: 89.46\n",
      "Loss: 0.3156, policy loss: -0.0030, critic_loss: 0.6465, entropy: 0.4632\n",
      "Returns: 9.6253, advantages: 0.6675, actions: 0.0443, actions std: 0.3818\n",
      "Step: 3665919, last rewards: -15.06, max reward: -12.44\n",
      "Loss: 1.2789, policy loss: -0.1101, critic_loss: 2.7872, entropy: 0.4626\n",
      "Returns: 10.5781, advantages: 1.5498, actions: -0.1618, actions std: 0.3918\n",
      "Step: 3686399, last rewards: -17.39, max reward: -15.23\n",
      "Loss: 0.1607, policy loss: 0.0171, critic_loss: 0.2964, entropy: 0.4598\n",
      "Returns: 10.1265, advantages: 1.0368, actions: -0.2069, actions std: 0.3842\n",
      "Step: 3706879, last rewards: -16.62, max reward: -14.73\n",
      "Loss: 0.4111, policy loss: 0.0226, critic_loss: 0.7862, entropy: 0.4587\n",
      "Returns: 9.9673, advantages: 0.7815, actions: -0.1358, actions std: 0.3894\n",
      "Step: 3727359, last rewards: 57.01, max reward: 89.20\n",
      "Loss: 3.5929, policy loss: -0.0651, critic_loss: 7.3252, entropy: 0.4586\n",
      "Returns: 11.7421, advantages: 2.5145, actions: 0.1172, actions std: 0.3778\n",
      "Step: 3747839, last rewards: 63.76, max reward: 91.96\n",
      "Loss: 0.2247, policy loss: 0.0568, critic_loss: 0.3450, entropy: 0.4537\n",
      "Returns: 10.3787, advantages: 1.1810, actions: 0.3048, actions std: 0.3436\n",
      "Step: 3768319, last rewards: 79.16, max reward: 92.86\n",
      "Loss: 3.6010, policy loss: -0.0351, critic_loss: 7.2812, entropy: 0.4511\n",
      "Returns: 11.8935, advantages: 2.8030, actions: 0.0878, actions std: 0.3872\n",
      "Step: 3788799, last rewards: 76.80, max reward: 90.28\n",
      "Loss: 0.0496, policy loss: -0.0312, critic_loss: 0.1706, entropy: 0.4520\n",
      "Returns: 10.9814, advantages: 1.9542, actions: 0.1426, actions std: 0.3796\n",
      "Step: 3809279, last rewards: -17.20, max reward: -14.88\n",
      "Loss: 0.8330, policy loss: -0.0802, critic_loss: 1.8354, entropy: 0.4510\n",
      "Returns: 10.2082, advantages: 1.1584, actions: -0.1318, actions std: 0.3976\n",
      "Step: 3829759, last rewards: -18.13, max reward: -16.82\n",
      "Loss: 0.3534, policy loss: -0.0382, critic_loss: 0.7922, entropy: 0.4519\n",
      "Returns: 10.6278, advantages: 1.5094, actions: -0.1056, actions std: 0.4211\n",
      "Step: 3850239, last rewards: -18.71, max reward: -14.28\n",
      "Loss: 1.2245, policy loss: -0.0478, critic_loss: 2.5538, entropy: 0.4540\n",
      "Returns: 8.8951, advantages: -0.3524, actions: -0.1869, actions std: 0.3627\n",
      "Step: 3870719, last rewards: 67.27, max reward: 91.31\n",
      "Loss: 1.4237, policy loss: -0.0647, critic_loss: 2.9860, entropy: 0.4528\n",
      "Returns: 11.7444, advantages: 2.4567, actions: 0.0513, actions std: 0.4200\n",
      "Step: 3891199, last rewards: -5.83, max reward: 88.55\n",
      "Loss: 0.6276, policy loss: -0.1627, critic_loss: 1.5896, entropy: 0.4525\n",
      "Returns: 10.0720, advantages: 0.8090, actions: -0.0687, actions std: 0.4088\n",
      "Step: 3911679, last rewards: -9.18, max reward: 89.03\n",
      "Loss: 0.5271, policy loss: -0.0888, critic_loss: 1.2407, entropy: 0.4521\n",
      "Returns: 10.5268, advantages: 1.1238, actions: -0.0505, actions std: 0.4402\n",
      "Step: 3932159, last rewards: 85.03, max reward: 89.29\n",
      "Loss: 0.1556, policy loss: 0.0124, critic_loss: 0.2954, entropy: 0.4499\n",
      "Returns: 10.7093, advantages: 1.2854, actions: 0.3522, actions std: 0.3580\n",
      "Step: 3952639, last rewards: 84.51, max reward: 88.85\n",
      "Loss: 0.1589, policy loss: -0.0060, critic_loss: 0.3389, entropy: 0.4500\n",
      "Returns: 11.9437, advantages: 2.5855, actions: 0.2660, actions std: 0.3794\n",
      "Step: 3973119, last rewards: 78.13, max reward: 91.64\n",
      "Loss: 1.1420, policy loss: 0.0423, critic_loss: 2.2083, entropy: 0.4456\n",
      "Returns: 12.2850, advantages: 2.9416, actions: 0.0509, actions std: 0.3965\n",
      "Step: 3993599, last rewards: 14.60, max reward: 90.15\n",
      "Loss: 0.3096, policy loss: 0.0096, critic_loss: 0.6089, entropy: 0.4453\n",
      "Returns: 10.8970, advantages: 1.5351, actions: -0.0144, actions std: 0.4097\n",
      "Step: 4014079, last rewards: 79.71, max reward: 92.08\n",
      "Loss: 2.9826, policy loss: -0.0416, critic_loss: 6.0572, entropy: 0.4455\n",
      "Returns: 12.2781, advantages: 2.8387, actions: 0.0724, actions std: 0.3866\n",
      "Step: 4034559, last rewards: 78.39, max reward: 90.46\n",
      "Loss: 3.7525, policy loss: 0.0165, critic_loss: 7.4808, entropy: 0.4422\n",
      "Returns: 12.1187, advantages: 2.6573, actions: 0.1209, actions std: 0.3796\n",
      "Step: 4055039, last rewards: 59.26, max reward: 93.31\n",
      "Loss: 0.1624, policy loss: -0.0497, critic_loss: 0.4330, entropy: 0.4403\n",
      "Returns: 10.3821, advantages: 0.9151, actions: 0.0375, actions std: 0.3803\n",
      "Step: 4075519, last rewards: 78.29, max reward: 92.66\n",
      "Loss: 3.8269, policy loss: -0.0405, critic_loss: 7.7436, entropy: 0.4363\n",
      "Returns: 9.7377, advantages: 0.2618, actions: 0.1702, actions std: 0.3554\n",
      "Step: 4095999, last rewards: -15.10, max reward: -12.51\n",
      "Loss: 0.2021, policy loss: -0.0604, critic_loss: 0.5336, entropy: 0.4347\n",
      "Returns: 10.0161, advantages: 0.4720, actions: -0.0759, actions std: 0.3639\n",
      "Step: 4116479, last rewards: -15.57, max reward: -13.86\n",
      "Loss: 0.8969, policy loss: -0.0116, critic_loss: 1.8258, entropy: 0.4347\n",
      "Returns: 10.7563, advantages: 1.1200, actions: -0.1229, actions std: 0.3686\n",
      "Step: 4136959, last rewards: -13.50, max reward: -11.40\n",
      "Loss: 1.3986, policy loss: -0.0531, critic_loss: 2.9120, entropy: 0.4288\n",
      "Returns: 10.9469, advantages: 1.3099, actions: -0.0670, actions std: 0.3727\n",
      "Step: 4157439, last rewards: -16.03, max reward: -13.61\n",
      "Loss: 0.9152, policy loss: -0.0627, critic_loss: 1.9643, entropy: 0.4281\n",
      "Returns: 10.9996, advantages: 1.2940, actions: -0.0780, actions std: 0.3850\n",
      "Step: 4177919, last rewards: 17.83, max reward: 91.41\n",
      "Loss: 0.3469, policy loss: 0.0005, critic_loss: 0.7014, entropy: 0.4278\n",
      "Returns: 11.0188, advantages: 1.2697, actions: -0.0034, actions std: 0.3708\n",
      "Step: 4198399, last rewards: -15.76, max reward: -14.08\n",
      "Loss: 0.4010, policy loss: -0.0261, critic_loss: 0.8627, entropy: 0.4261\n",
      "Returns: 11.2513, advantages: 1.5022, actions: -0.1275, actions std: 0.3832\n",
      "Step: 4218879, last rewards: -16.10, max reward: -13.19\n",
      "Loss: 0.9366, policy loss: -0.0912, critic_loss: 2.0640, entropy: 0.4218\n",
      "Returns: 9.0726, advantages: -0.7221, actions: -0.1444, actions std: 0.3407\n",
      "Step: 4239359, last rewards: -15.78, max reward: -14.01\n",
      "Loss: 0.0558, policy loss: 0.0127, critic_loss: 0.0945, entropy: 0.4191\n",
      "Returns: 11.0898, advantages: 1.1642, actions: -0.1122, actions std: 0.3867\n",
      "Step: 4259839, last rewards: 27.28, max reward: 94.81\n",
      "Loss: 3.6938, policy loss: -0.0307, critic_loss: 7.4574, entropy: 0.4172\n",
      "Returns: 13.2167, advantages: 3.1815, actions: 0.0112, actions std: 0.3824\n",
      "Step: 4280319, last rewards: 89.37, max reward: 92.24\n",
      "Loss: 0.5061, policy loss: 0.0513, critic_loss: 0.9181, entropy: 0.4164\n",
      "Returns: 12.2957, advantages: 2.3173, actions: 0.2607, actions std: 0.3466\n",
      "Step: 4300799, last rewards: 90.69, max reward: 92.36\n",
      "Loss: 3.8590, policy loss: -0.0464, critic_loss: 7.8190, entropy: 0.4175\n",
      "Returns: 13.3832, advantages: 3.4722, actions: 0.1543, actions std: 0.3727\n",
      "Step: 4321279, last rewards: 80.14, max reward: 94.27\n",
      "Loss: 4.1724, policy loss: -0.0140, critic_loss: 8.3813, entropy: 0.4162\n",
      "Returns: 12.8721, advantages: 2.9781, actions: 0.0482, actions std: 0.3865\n",
      "Step: 4341759, last rewards: 90.01, max reward: 93.37\n",
      "Loss: 3.7361, policy loss: -0.0098, critic_loss: 7.5002, entropy: 0.4163\n",
      "Returns: 12.5415, advantages: 2.6496, actions: 0.0769, actions std: 0.3767\n",
      "Step: 4362239, last rewards: 47.08, max reward: 91.50\n",
      "Loss: 1.0179, policy loss: -0.0657, critic_loss: 2.1756, entropy: 0.4160\n",
      "Returns: 11.1655, advantages: 1.2503, actions: -0.0590, actions std: 0.3964\n",
      "Step: 4382719, last rewards: 90.25, max reward: 92.18\n",
      "Loss: 0.1614, policy loss: 0.0141, critic_loss: 0.3028, entropy: 0.4163\n",
      "Returns: 13.1133, advantages: 3.1365, actions: 0.1493, actions std: 0.3670\n",
      "Step: 4403199, last rewards: 80.12, max reward: 94.56\n",
      "Loss: 4.0233, policy loss: -0.0719, critic_loss: 8.1988, entropy: 0.4184\n",
      "Returns: 12.3252, advantages: 2.3559, actions: -0.0057, actions std: 0.3987\n",
      "Step: 4423679, last rewards: 90.97, max reward: 93.80\n",
      "Loss: 0.3651, policy loss: -0.0408, critic_loss: 0.8202, entropy: 0.4203\n",
      "Returns: 11.0271, advantages: 1.0555, actions: 0.2047, actions std: 0.3353\n",
      "Step: 4444159, last rewards: 88.78, max reward: 93.39\n",
      "Loss: 0.5071, policy loss: -0.1381, critic_loss: 1.2988, entropy: 0.4178\n",
      "Returns: 13.3770, advantages: 3.4740, actions: -0.0596, actions std: 0.4061\n",
      "Step: 4464639, last rewards: 91.69, max reward: 93.17\n",
      "Loss: 0.1738, policy loss: -0.0282, critic_loss: 0.4124, entropy: 0.4205\n",
      "Returns: 12.4383, advantages: 2.4916, actions: 0.1736, actions std: 0.3734\n",
      "Step: 4485119, last rewards: 2.32, max reward: 91.62\n",
      "Loss: 0.7569, policy loss: -0.1077, critic_loss: 1.7376, entropy: 0.4196\n",
      "Returns: 12.1366, advantages: 2.0669, actions: -0.0540, actions std: 0.3976\n",
      "Step: 4505599, last rewards: 91.43, max reward: 93.51\n",
      "Loss: 1.0549, policy loss: -0.0484, critic_loss: 2.2151, entropy: 0.4214\n",
      "Returns: 11.6319, advantages: 1.5351, actions: -0.0088, actions std: 0.3660\n",
      "Step: 4526079, last rewards: 92.02, max reward: 94.54\n",
      "Loss: 3.5664, policy loss: 0.0337, critic_loss: 7.0738, entropy: 0.4232\n",
      "Returns: 13.0409, advantages: 2.9436, actions: 0.1329, actions std: 0.3674\n",
      "Step: 4546559, last rewards: 92.85, max reward: 95.30\n",
      "Loss: 0.2971, policy loss: -0.0313, critic_loss: 0.6653, entropy: 0.4219\n",
      "Returns: 13.4678, advantages: 3.3719, actions: 0.0003, actions std: 0.3875\n",
      "Step: 4567039, last rewards: 88.99, max reward: 92.71\n",
      "Loss: 0.1382, policy loss: 0.0236, critic_loss: 0.2376, entropy: 0.4190\n",
      "Returns: 12.0261, advantages: 1.9447, actions: 0.2744, actions std: 0.3412\n",
      "Step: 4587519, last rewards: 90.40, max reward: 93.25\n",
      "Loss: 3.6975, policy loss: -0.0488, critic_loss: 7.5007, entropy: 0.4136\n",
      "Returns: 12.6707, advantages: 2.6192, actions: -0.0375, actions std: 0.3842\n",
      "Step: 4607999, last rewards: 80.94, max reward: 94.00\n",
      "Loss: 3.5117, policy loss: -0.0482, critic_loss: 7.1282, entropy: 0.4151\n",
      "Returns: 12.2032, advantages: 2.1897, actions: 0.1545, actions std: 0.3766\n",
      "Step: 4628479, last rewards: 91.99, max reward: 94.32\n",
      "Loss: 3.7945, policy loss: 0.0231, critic_loss: 7.5510, entropy: 0.4127\n",
      "Returns: 13.1559, advantages: 3.2157, actions: 0.0721, actions std: 0.3666\n",
      "Step: 4648959, last rewards: 92.73, max reward: 95.37\n",
      "Loss: 0.3270, policy loss: -0.0099, critic_loss: 0.6821, entropy: 0.4132\n",
      "Returns: 14.0483, advantages: 4.2029, actions: 0.0218, actions std: 0.3790\n",
      "Step: 4669439, last rewards: 92.68, max reward: 94.79\n",
      "Loss: 3.7041, policy loss: 0.0381, critic_loss: 7.3403, entropy: 0.4130\n",
      "Returns: 12.8170, advantages: 2.9753, actions: 0.0827, actions std: 0.3586\n",
      "Step: 4689919, last rewards: 91.05, max reward: 95.48\n",
      "Loss: 0.3894, policy loss: -0.0341, critic_loss: 0.8552, entropy: 0.4110\n",
      "Returns: 13.0117, advantages: 3.1840, actions: -0.0152, actions std: 0.3730\n",
      "Step: 4710399, last rewards: -18.35, max reward: -15.72\n",
      "Loss: 0.4385, policy loss: -0.0087, critic_loss: 0.9026, entropy: 0.4094\n",
      "Returns: 11.1970, advantages: 1.2570, actions: -0.1998, actions std: 0.3845\n",
      "Step: 4730879, last rewards: 93.20, max reward: 94.72\n",
      "Loss: 3.5280, policy loss: 0.0166, critic_loss: 7.0309, entropy: 0.4071\n",
      "Returns: 13.0859, advantages: 3.0051, actions: 0.0757, actions std: 0.3676\n",
      "Step: 4751359, last rewards: 93.32, max reward: 95.42\n",
      "Loss: 2.8702, policy loss: -0.0449, critic_loss: 5.8384, entropy: 0.4058\n",
      "Returns: 13.3318, advantages: 3.2737, actions: 0.0714, actions std: 0.3687\n",
      "Step: 4771839, last rewards: 93.12, max reward: 94.93\n",
      "Loss: 0.3622, policy loss: -0.0538, critic_loss: 0.8400, entropy: 0.4034\n",
      "Returns: 13.3047, advantages: 3.2531, actions: 0.1376, actions std: 0.3593\n",
      "Step: 4792319, last rewards: 93.51, max reward: 95.00\n",
      "Loss: 3.5829, policy loss: -0.0238, critic_loss: 7.2214, entropy: 0.4036\n",
      "Returns: 13.0048, advantages: 3.0330, actions: 0.0542, actions std: 0.3679\n",
      "Step: 4812799, last rewards: 92.36, max reward: 95.44\n",
      "Loss: 1.9114, policy loss: 0.0047, critic_loss: 3.8216, entropy: 0.4051\n",
      "Returns: 12.2485, advantages: 2.2879, actions: 0.1199, actions std: 0.3666\n",
      "Step: 4833279, last rewards: 92.06, max reward: 93.43\n",
      "Loss: 3.6819, policy loss: -0.0936, critic_loss: 7.5593, entropy: 0.4066\n",
      "Returns: 14.5403, advantages: 4.5968, actions: 0.1538, actions std: 0.3957\n",
      "Step: 4853759, last rewards: 93.18, max reward: 95.10\n",
      "Loss: 0.2144, policy loss: -0.0189, critic_loss: 0.4746, entropy: 0.4041\n",
      "Returns: 13.1528, advantages: 3.1955, actions: -0.0001, actions std: 0.3666\n",
      "Step: 4874239, last rewards: 93.14, max reward: 95.02\n",
      "Loss: 3.5650, policy loss: -0.0343, critic_loss: 7.2066, entropy: 0.4022\n",
      "Returns: 14.1720, advantages: 4.1456, actions: 0.0829, actions std: 0.3764\n",
      "Step: 4894719, last rewards: 93.45, max reward: 94.65\n",
      "Loss: 0.1607, policy loss: -0.0432, critic_loss: 0.4159, entropy: 0.4037\n",
      "Returns: 12.9744, advantages: 2.9809, actions: 0.1102, actions std: 0.3598\n",
      "Step: 4915199, last rewards: 91.62, max reward: 94.09\n",
      "Loss: 0.5159, policy loss: 0.0283, critic_loss: 0.9834, entropy: 0.4024\n",
      "Returns: 12.9244, advantages: 2.9876, actions: 0.2230, actions std: 0.3603\n",
      "Step: 4935679, last rewards: 93.66, max reward: 95.14\n",
      "Loss: 3.5189, policy loss: -0.0742, critic_loss: 7.1943, entropy: 0.4014\n",
      "Returns: 13.9875, advantages: 4.0954, actions: 0.0081, actions std: 0.3736\n",
      "Step: 4956159, last rewards: 93.82, max reward: 94.77\n",
      "Loss: 3.5137, policy loss: 0.0310, critic_loss: 6.9734, entropy: 0.4007\n",
      "Returns: 14.3061, advantages: 4.3576, actions: 0.0306, actions std: 0.3793\n",
      "Step: 4976639, last rewards: 93.17, max reward: 95.66\n",
      "Loss: 3.4589, policy loss: -0.0567, critic_loss: 7.0392, entropy: 0.4032\n",
      "Returns: 15.0565, advantages: 5.0921, actions: 0.1519, actions std: 0.3852\n",
      "Step: 4997119, last rewards: 88.21, max reward: 92.21\n",
      "Loss: 0.4227, policy loss: -0.0318, critic_loss: 0.9170, entropy: 0.4040\n",
      "Returns: 13.6898, advantages: 3.7154, actions: 0.3769, actions std: 0.3372\n"
     ]
    }
   ],
   "source": [
    "buffer = []\n",
    "state, _ = env.reset()\n",
    "network.train()\n",
    "total_reward = 0\n",
    "total_reward_list = []\n",
    "\n",
    "for i in range(int(5e6)):\n",
    "    tensor_state = T.tensor(state).float()\n",
    "    with T.no_grad():\n",
    "        network_output = network(tensor_state)\n",
    "    actor_mean, critic_value, dist = network_output\n",
    "    action = dist.sample()\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action.numpy())\n",
    "    total_reward += float(reward)\n",
    "    done = T.tensor(terminated or truncated).float()\n",
    "    \n",
    "    velocity = next_state[-1]\n",
    "    buffer.append(\n",
    "        (\n",
    "            tensor_state,\n",
    "            action,\n",
    "            T.tensor(reward + 100 * (velocity ** 2) + 5 * abs(velocity)),\n",
    "            T.tensor(next_state),\n",
    "            done,\n",
    "            critic_value.detach()\n",
    "        )\n",
    "    )\n",
    "    if terminated or truncated:\n",
    "        state, _ = env.reset()\n",
    "        total_reward_list.append(total_reward)\n",
    "        total_reward = 0\n",
    "    else:\n",
    "        state = next_state\n",
    "    if len(buffer) == train_step:\n",
    "        states, actions, rewards, next_states, dones, critic_values = (\n",
    "            [T.stack(column, dim=0) for column in zip(*buffer)]\n",
    "        )\n",
    "        buffer.clear()\n",
    "        \n",
    "        # calculate returns and advantages\n",
    "        returns, advantages = [], []\n",
    "        g = T.tensor(0)\n",
    "        for k in reversed(range(train_step-1)):\n",
    "            td_error = rewards[k] + gamma_ * critic_values[k + 1, 0] * (1 - dones[k]) - critic_values[k, 0]\n",
    "            g = td_error + gamma_ * lambda_ * g * (1 - dones[k])\n",
    "            \n",
    "            returns.insert(0, g + critic_values[k, 0])\n",
    "            advantages.insert(0, g)\n",
    "        \n",
    "        returns = T.stack(returns, dim=0)\n",
    "        advantages = T.stack(advantages, dim=0)\n",
    "        \n",
    "        for batch_idx in range(0, train_step, 256):\n",
    "            batch_range = slice(batch_idx,min(batch_idx + 256, train_step-1))\n",
    "\n",
    "            batch_advantages = advantages[batch_range]\n",
    "            batch_states = states[batch_range]\n",
    "            batch_actions = actions[batch_range]\n",
    "            batch_returns = returns[batch_range]\n",
    "            \n",
    "            batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-8)\n",
    "            \n",
    "            # calculate losses\n",
    "            optimizer.zero_grad()\n",
    "            _, batch_critic_value, batch_dist = network(batch_states)\n",
    "            \n",
    "            log_prob = batch_dist.log_prob(batch_actions)\n",
    "            policy_loss = -(log_prob.sum(-1) * batch_advantages.detach()).mean()\n",
    "                \n",
    "            critic_loss = loss_fn(batch_critic_value.squeeze(-1), batch_returns)\n",
    "            \n",
    "            entropy = batch_dist.base_dist.entropy().mean()\n",
    "            \n",
    "            loss = policy_loss + 0.5 * critic_loss - 0.01 * entropy\n",
    "            loss.backward()\n",
    "            T.nn.utils.clip_grad_norm_(network.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "        \n",
    "        if (i + 1) % (2048 * 10) == 0:\n",
    "            print(f\"Step: {i}, last rewards: {sum(total_reward_list[-10:])/len(total_reward_list[-10:]):.2f}, max reward: {max(total_reward_list[-10:]):.2f}\")\n",
    "            print(f\"Loss: {loss.item():.4f}, policy loss: {policy_loss.item():.4f}, critic_loss: {critic_loss.item():.4f}, entropy: {entropy.item():.4f}\")\n",
    "            print(f\"Returns: {returns.mean().item():.4f}, advantages: {advantages.mean().item():.4f}, actions: {actions.mean().item():.4f}, actions std: {actions.std().item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
