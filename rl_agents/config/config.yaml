experiment_name: BipedalWalker/PPO/final

env_kwargs:
  id: BipedalWalker-v3
  num_envs: 16
  # hardcore: true
  training_wrappers:
    clip_reward:
      min_reward: -40.
    terminal_bonus:
      truncated_bonus: -1.
    power_obs_reward:
      abs_factors: [-0.02, -0.005, 0., -0.002, -0.001, 0.001, 0, 0, 0, -0.001, 0.001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
      # penalize too much of hull movements, hip extension, but reward hip speed
      nominal_factors: [0., 0., 0.05, 0., 0., 0., 0, 0, 0, 0., 0., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
      # encourage to move forward
    no_movement_truncate:
      index: 2
      eps: 0.01
      penalty: 2.
    action_reward:
      pow_factors: [-0.001, -0.002, -0.01, -0.002]
      abs_factors: [0.002, 0.001, 0.002, 0.001]
    # actions_interactions:
    #   factors:
    #     "(0, 2)": -0.2
    observations_interaction:
      factors:
        "(5, 10)": -1.
        # reward hip movements in opposite directions
    scale_reward:
      scale_factor: 0.1
      loc_factor: -0.002
    # time_limit:
    #   max_episode_steps: 500
  vectorization_mode: async

policy:
  type: ppo
  kwargs:
    advantage_normalize: batch
    returns_normalize: true
    gamma: 0.995
    lambda_: 0.97
    entropy_coef: 0.02
    value_loss_coef: 0.5
    clip_epsilon: 0.2
    num_epochs: 6
    optimizer_kwargs:
      actor_lr: 3e-4
      critic_lr: 3e-4
    exploration_method:
      name: distribution

worker_kwargs:
  device: auto
  record_step: 100000
  verbose: 1
  # temperature_config:
  #   temperature_start: 2.
  #   temperature_end: 1.
  #   temperature_steps: 10_000_000

train_kwargs:
  batch_size: 2048
  minibatch_size: 128
  num_steps: 2_000_000

network:
  kwargs:
    backbone_kwargs: {"hidden_dims": 64}
    backbone_name: mlp
    distribution: normal
    # core_name: gru
    head_kwargs: {}
    head_name: actor_critic
    num_features: 64
    initial_deviation: 0.6
