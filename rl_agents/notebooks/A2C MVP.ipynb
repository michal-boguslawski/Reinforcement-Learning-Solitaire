{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d360fb9",
   "metadata": {},
   "source": [
    "# 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4638645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import make\n",
    "import torch as T\n",
    "from torch import nn\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f08949",
   "metadata": {},
   "source": [
    "# 2. Creating network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cfcf93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(T.zeros(1, requires_grad=True))\n",
    "        # self.log_std = nn.Parameter(-1 * T.ones(1, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        actor_mean = self.actor(x)\n",
    "        critic_value = self.critic(x)\n",
    "        std = T.exp(self.log_std)\n",
    "        dist = Normal(loc=actor_mean, scale=std)\n",
    "        return actor_mean, critic_value, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2350631",
   "metadata": {},
   "source": [
    "# 3. Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2b528",
   "metadata": {},
   "source": [
    "## 3.1. Creating objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a022fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"MountainCarContinuous-v0\")\n",
    "\n",
    "network = ActorCriticNetwork()\n",
    "optimizer = T.optim.Adam(network.parameters(), lr=2e-4)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "train_step = 2048\n",
    "\n",
    "# parameters\n",
    "gamma_ = 0.99\n",
    "lambda_ = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396c4b4",
   "metadata": {},
   "source": [
    "## 3.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f95bea",
   "metadata": {},
   "source": [
    "<li>states: torch.Size([32, 2]) <br>\n",
    "<li>actons: torch.Size([32, 1]) <br>\n",
    "<li>rewards: torch.Size([32]) <br>\n",
    "<li>next_states: torch.Size([32, 2]) <br>\n",
    "<li>dones: torch.Size([32]) <br>\n",
    "<li>critic_values: torch.Size([32, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2b65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 20479, last rewards: -102.37, max reward: -96.03\n",
      "Loss: 0.1704, policy loss: -0.1153, critic_loss: 0.5996, entropy: 1.4039\n",
      "Returns: -0.5629, advantages: -0.0981, actions: 0.0526, actions std: 0.9947\n",
      "Step: 40959, last rewards: -93.98, max reward: -85.43\n",
      "Loss: 0.0688, policy loss: -0.1295, critic_loss: 0.4242, entropy: 1.3903\n",
      "Returns: -0.8652, advantages: -0.2951, actions: -0.0021, actions std: 0.9866\n",
      "Step: 61439, last rewards: -92.83, max reward: -88.47\n",
      "Loss: 0.1013, policy loss: -0.1336, critic_loss: 0.4973, entropy: 1.3752\n",
      "Returns: -0.4021, advantages: 0.3312, actions: -0.0006, actions std: 0.9648\n",
      "Step: 81919, last rewards: -90.00, max reward: -83.00\n",
      "Loss: 0.1869, policy loss: 0.0240, critic_loss: 0.3531, entropy: 1.3615\n",
      "Returns: -0.6653, advantages: 0.0491, actions: -0.0285, actions std: 0.9488\n",
      "Step: 102399, last rewards: -87.60, max reward: -82.89\n",
      "Loss: -0.0036, policy loss: -0.1029, critic_loss: 0.2255, entropy: 1.3488\n",
      "Returns: -0.6760, advantages: -0.0623, actions: 0.0090, actions std: 0.9435\n",
      "Step: 122879, last rewards: -87.31, max reward: -82.31\n",
      "Loss: -0.0105, policy loss: -0.1937, critic_loss: 0.3931, entropy: 1.3351\n",
      "Returns: -0.4869, advantages: 0.0114, actions: 0.0009, actions std: 0.9316\n",
      "Step: 143359, last rewards: -82.95, max reward: -77.81\n",
      "Loss: -0.1400, policy loss: -0.1710, critic_loss: 0.0882, entropy: 1.3187\n",
      "Returns: -0.7889, advantages: -0.1377, actions: -0.0316, actions std: 0.9132\n",
      "Step: 163839, last rewards: -80.23, max reward: -71.59\n",
      "Loss: 0.0056, policy loss: -0.0788, critic_loss: 0.1948, entropy: 1.3033\n",
      "Returns: -0.3967, advantages: 0.4147, actions: 0.0549, actions std: 0.8905\n",
      "Step: 184319, last rewards: -77.36, max reward: -70.08\n",
      "Loss: 0.2790, policy loss: -0.0825, critic_loss: 0.7488, entropy: 1.2909\n",
      "Returns: -0.5891, advantages: 0.0443, actions: 0.0012, actions std: 0.8718\n",
      "Step: 204799, last rewards: -75.47, max reward: -66.71\n",
      "Loss: -0.0728, policy loss: -0.1114, critic_loss: 0.1028, entropy: 1.2778\n",
      "Returns: -0.3194, advantages: 0.1454, actions: -0.0586, actions std: 0.8461\n",
      "Step: 225279, last rewards: -74.14, max reward: -69.32\n",
      "Loss: 0.0313, policy loss: -0.1353, critic_loss: 0.3584, entropy: 1.2623\n",
      "Returns: -0.1563, advantages: 0.2657, actions: -0.0166, actions std: 0.8729\n",
      "Step: 245759, last rewards: -72.35, max reward: -65.81\n",
      "Loss: 0.0787, policy loss: -0.1614, critic_loss: 0.5053, entropy: 1.2483\n",
      "Returns: -0.2776, advantages: 0.0332, actions: -0.0550, actions std: 0.8614\n",
      "Step: 266239, last rewards: -72.70, max reward: -68.47\n",
      "Loss: -0.0240, policy loss: -0.2039, critic_loss: 0.3846, entropy: 1.2344\n",
      "Returns: 0.0428, advantages: 0.2706, actions: 0.0402, actions std: 0.8501\n",
      "Step: 286719, last rewards: -68.48, max reward: -63.36\n",
      "Loss: 0.1004, policy loss: -0.0883, critic_loss: 0.4018, entropy: 1.2225\n",
      "Returns: 0.7306, advantages: 0.7532, actions: 0.0169, actions std: 0.8328\n",
      "Step: 307199, last rewards: -68.04, max reward: -64.41\n",
      "Loss: 0.5846, policy loss: -0.0661, critic_loss: 1.3257, entropy: 1.2105\n",
      "Returns: 0.8277, advantages: 0.6008, actions: 0.0096, actions std: 0.8487\n",
      "Step: 327679, last rewards: -67.16, max reward: -61.42\n",
      "Loss: -0.0289, policy loss: -0.0724, critic_loss: 0.1110, entropy: 1.1986\n",
      "Returns: 1.3322, advantages: 0.9310, actions: 0.0213, actions std: 0.8170\n",
      "Step: 348159, last rewards: -63.72, max reward: -59.57\n",
      "Loss: -0.1205, policy loss: -0.1413, critic_loss: 0.0653, entropy: 1.1875\n",
      "Returns: 0.6148, advantages: -0.0444, actions: 0.0199, actions std: 0.8101\n",
      "Step: 368639, last rewards: -61.48, max reward: -56.81\n",
      "Loss: -0.1229, policy loss: -0.1854, critic_loss: 0.1486, entropy: 1.1742\n",
      "Returns: 1.7674, advantages: 1.1172, actions: 0.0178, actions std: 0.7980\n",
      "Step: 389119, last rewards: -60.61, max reward: -56.91\n",
      "Loss: 0.2539, policy loss: -0.1064, critic_loss: 0.7439, entropy: 1.1620\n",
      "Returns: 2.1285, advantages: 1.3084, actions: -0.0442, actions std: 0.7779\n",
      "Step: 409599, last rewards: -60.50, max reward: -56.34\n",
      "Loss: 0.3912, policy loss: -0.0809, critic_loss: 0.9672, entropy: 1.1498\n",
      "Returns: 2.1143, advantages: 1.1433, actions: -0.0099, actions std: 0.7815\n",
      "Step: 430079, last rewards: -58.42, max reward: -56.75\n",
      "Loss: 0.1002, policy loss: -0.0729, critic_loss: 0.3690, entropy: 1.1388\n",
      "Returns: 1.0730, advantages: -0.1640, actions: 0.0474, actions std: 0.7545\n",
      "Step: 450559, last rewards: -58.79, max reward: -54.11\n",
      "Loss: -0.1034, policy loss: -0.1677, critic_loss: 0.1510, entropy: 1.1281\n",
      "Returns: 1.3229, advantages: 0.1883, actions: 0.0185, actions std: 0.7517\n",
      "Step: 471039, last rewards: -56.60, max reward: -51.55\n",
      "Loss: -0.0164, policy loss: -0.1908, critic_loss: 0.3712, entropy: 1.1177\n",
      "Returns: 0.8556, advantages: -0.3639, actions: -0.1543, actions std: 0.7382\n",
      "Step: 491519, last rewards: -54.94, max reward: -51.44\n",
      "Loss: 0.7023, policy loss: -0.0672, critic_loss: 1.5612, entropy: 1.1086\n",
      "Returns: 2.2499, advantages: 0.7890, actions: -0.0387, actions std: 0.7484\n",
      "Step: 511999, last rewards: -54.78, max reward: -51.81\n",
      "Loss: 0.0852, policy loss: -0.0401, critic_loss: 0.2727, entropy: 1.0982\n",
      "Returns: 1.4840, advantages: -0.2047, actions: -0.0118, actions std: 0.7348\n",
      "Step: 532479, last rewards: -53.42, max reward: -52.44\n",
      "Loss: -0.1188, policy loss: -0.1504, critic_loss: 0.0849, entropy: 1.0891\n",
      "Returns: 2.2333, advantages: 0.2947, actions: -0.0542, actions std: 0.7156\n",
      "Step: 552959, last rewards: -52.47, max reward: -48.57\n",
      "Loss: 0.1011, policy loss: -0.1037, critic_loss: 0.4311, entropy: 1.0802\n",
      "Returns: 1.9864, advantages: -0.0235, actions: -0.0813, actions std: 0.7187\n",
      "Step: 573439, last rewards: -51.38, max reward: -46.15\n",
      "Loss: -0.0734, policy loss: -0.1103, critic_loss: 0.0952, entropy: 1.0700\n",
      "Returns: 2.2560, advantages: 0.3428, actions: -0.0056, actions std: 0.7217\n",
      "Step: 593919, last rewards: -50.64, max reward: -46.86\n",
      "Loss: -0.0241, policy loss: -0.1305, critic_loss: 0.2340, entropy: 1.0634\n",
      "Returns: 3.2909, advantages: 0.8950, actions: 0.0223, actions std: 0.7221\n",
      "Step: 614399, last rewards: -49.63, max reward: -45.97\n",
      "Loss: 0.0626, policy loss: -0.1303, critic_loss: 0.4070, entropy: 1.0554\n",
      "Returns: 2.7866, advantages: 0.2793, actions: -0.1164, actions std: 0.7090\n",
      "Step: 634879, last rewards: -49.22, max reward: -45.35\n",
      "Loss: -0.0793, policy loss: -0.1845, critic_loss: 0.2314, entropy: 1.0474\n",
      "Returns: 2.7801, advantages: 0.1692, actions: -0.0108, actions std: 0.7007\n",
      "Step: 655359, last rewards: -48.00, max reward: -45.03\n",
      "Loss: -0.0721, policy loss: -0.1580, critic_loss: 0.1925, entropy: 1.0377\n",
      "Returns: 2.7431, advantages: -0.0410, actions: 0.0181, actions std: 0.7115\n",
      "Step: 675839, last rewards: -46.98, max reward: -43.71\n",
      "Loss: -0.1160, policy loss: -0.1609, critic_loss: 0.1105, entropy: 1.0292\n",
      "Returns: 2.9906, advantages: 0.1998, actions: -0.0487, actions std: 0.6708\n",
      "Step: 696319, last rewards: -46.34, max reward: -40.55\n",
      "Loss: -0.0492, policy loss: -0.1323, critic_loss: 0.1867, entropy: 1.0194\n",
      "Returns: 2.4761, advantages: -0.2257, actions: 0.0092, actions std: 0.6934\n",
      "Step: 716799, last rewards: -45.46, max reward: -42.95\n",
      "Loss: -0.0010, policy loss: -0.0710, critic_loss: 0.1602, entropy: 1.0103\n",
      "Returns: 3.1928, advantages: 0.4499, actions: 0.0717, actions std: 0.6685\n",
      "Step: 737279, last rewards: -44.39, max reward: -40.22\n",
      "Loss: 0.0088, policy loss: -0.0420, critic_loss: 0.1216, entropy: 1.0023\n",
      "Returns: 3.1736, advantages: 0.2938, actions: 0.0463, actions std: 0.6525\n",
      "Step: 757759, last rewards: -43.32, max reward: -41.43\n",
      "Loss: -0.0865, policy loss: -0.1088, critic_loss: 0.0646, entropy: 0.9929\n",
      "Returns: 2.5094, advantages: -0.3621, actions: 0.0298, actions std: 0.6627\n",
      "Step: 778239, last rewards: -43.55, max reward: -38.98\n",
      "Loss: 0.6062, policy loss: -0.1234, critic_loss: 1.4789, entropy: 0.9837\n",
      "Returns: 3.2701, advantages: 0.5589, actions: -0.0742, actions std: 0.6835\n",
      "Step: 798719, last rewards: -42.98, max reward: -40.44\n",
      "Loss: -0.0013, policy loss: -0.1162, critic_loss: 0.2494, entropy: 0.9762\n",
      "Returns: 2.4703, advantages: -0.3773, actions: 0.0113, actions std: 0.6520\n",
      "Step: 819199, last rewards: -30.48, max reward: 73.65\n",
      "Loss: 0.0436, policy loss: -0.0761, critic_loss: 0.2588, entropy: 0.9666\n",
      "Returns: 4.0483, advantages: 1.1901, actions: 0.0076, actions std: 0.6486\n",
      "Step: 839679, last rewards: -42.03, max reward: -38.24\n",
      "Loss: 0.1131, policy loss: -0.0031, critic_loss: 0.2515, entropy: 0.9567\n",
      "Returns: 2.9910, advantages: 0.1292, actions: -0.0717, actions std: 0.6269\n",
      "Step: 860159, last rewards: -39.41, max reward: -35.96\n",
      "Loss: 0.0387, policy loss: -0.0809, critic_loss: 0.2581, entropy: 0.9472\n",
      "Returns: 2.4605, advantages: -0.4289, actions: -0.0467, actions std: 0.6247\n",
      "Step: 880639, last rewards: -40.13, max reward: -35.23\n",
      "Loss: 0.2432, policy loss: -0.1371, critic_loss: 0.7794, entropy: 0.9381\n",
      "Returns: 3.5971, advantages: 0.6149, actions: 0.0562, actions std: 0.6281\n",
      "Step: 901119, last rewards: -38.74, max reward: -35.76\n",
      "Loss: 0.1659, policy loss: -0.0085, critic_loss: 0.3674, entropy: 0.9308\n",
      "Returns: 3.3198, advantages: 0.0887, actions: -0.0293, actions std: 0.6058\n",
      "Step: 921599, last rewards: -38.97, max reward: -34.34\n",
      "Loss: 0.8550, policy loss: -0.0132, critic_loss: 1.7548, entropy: 0.9222\n",
      "Returns: 3.2310, advantages: 0.0295, actions: -0.0441, actions std: 0.6180\n",
      "Step: 942079, last rewards: -37.26, max reward: -32.36\n",
      "Loss: 0.0663, policy loss: -0.1322, critic_loss: 0.4152, entropy: 0.9134\n",
      "Returns: 3.3522, advantages: 0.0970, actions: -0.0716, actions std: 0.5993\n",
      "Step: 962559, last rewards: -36.90, max reward: -34.66\n",
      "Loss: -0.0194, policy loss: -0.0682, critic_loss: 0.1156, entropy: 0.9036\n",
      "Returns: 3.1664, advantages: -0.0157, actions: -0.0531, actions std: 0.6183\n",
      "Step: 983039, last rewards: -36.09, max reward: -34.55\n",
      "Loss: -0.0059, policy loss: -0.1383, critic_loss: 0.2828, entropy: 0.8948\n",
      "Returns: 4.3138, advantages: 0.8728, actions: 0.0151, actions std: 0.6042\n",
      "Step: 1003519, last rewards: -35.14, max reward: -31.80\n",
      "Loss: -0.0184, policy loss: -0.1305, critic_loss: 0.2418, entropy: 0.8858\n",
      "Returns: 3.6648, advantages: 0.0191, actions: 0.0178, actions std: 0.5860\n",
      "Step: 1023999, last rewards: -35.16, max reward: -31.00\n",
      "Loss: 0.1201, policy loss: 0.0296, critic_loss: 0.1984, entropy: 0.8777\n",
      "Returns: 3.5412, advantages: -0.1243, actions: -0.0571, actions std: 0.5814\n",
      "Step: 1044479, last rewards: -35.10, max reward: -33.04\n",
      "Loss: 0.5168, policy loss: -0.1118, critic_loss: 1.2746, entropy: 0.8700\n",
      "Returns: 4.4437, advantages: 0.6777, actions: -0.0134, actions std: 0.6019\n",
      "Step: 1064959, last rewards: -33.93, max reward: -31.79\n",
      "Loss: 0.1778, policy loss: -0.0982, critic_loss: 0.5692, entropy: 0.8617\n",
      "Returns: 2.9135, advantages: -0.7437, actions: -0.0190, actions std: 0.5918\n",
      "Step: 1085439, last rewards: -33.65, max reward: -31.72\n",
      "Loss: 0.0447, policy loss: -0.1025, critic_loss: 0.3115, entropy: 0.8534\n",
      "Returns: 4.4829, advantages: 0.9487, actions: 0.0129, actions std: 0.5746\n",
      "Step: 1105919, last rewards: -33.63, max reward: -31.14\n",
      "Loss: 0.1311, policy loss: -0.0452, critic_loss: 0.3695, entropy: 0.8441\n",
      "Returns: 5.5301, advantages: 2.0298, actions: -0.0522, actions std: 0.6027\n",
      "Step: 1126399, last rewards: -32.58, max reward: -30.21\n",
      "Loss: 0.2317, policy loss: -0.0708, critic_loss: 0.6217, entropy: 0.8373\n",
      "Returns: 4.1727, advantages: 0.2103, actions: -0.0650, actions std: 0.5783\n",
      "Step: 1146879, last rewards: -32.96, max reward: -31.45\n",
      "Loss: 0.2169, policy loss: -0.0276, critic_loss: 0.5056, entropy: 0.8310\n",
      "Returns: 3.7241, advantages: -0.4538, actions: 0.0652, actions std: 0.5632\n",
      "Step: 1167359, last rewards: -32.86, max reward: -30.84\n",
      "Loss: 0.0941, policy loss: -0.1301, critic_loss: 0.4649, entropy: 0.8246\n",
      "Returns: 4.4354, advantages: 0.1427, actions: -0.0229, actions std: 0.5643\n",
      "Step: 1187839, last rewards: -31.85, max reward: -29.73\n",
      "Loss: 0.1012, policy loss: -0.0656, critic_loss: 0.3500, entropy: 0.8171\n",
      "Returns: 5.0239, advantages: 0.7267, actions: -0.1331, actions std: 0.5744\n",
      "Step: 1208319, last rewards: -31.79, max reward: -30.03\n",
      "Loss: 0.7221, policy loss: -0.0141, critic_loss: 1.4887, entropy: 0.8113\n",
      "Returns: 5.0975, advantages: 0.6057, actions: -0.0554, actions std: 0.5558\n",
      "Step: 1228799, last rewards: -30.54, max reward: -27.23\n",
      "Loss: 0.1831, policy loss: -0.0248, critic_loss: 0.4319, entropy: 0.8033\n",
      "Returns: 4.4213, advantages: -0.1822, actions: -0.0428, actions std: 0.5329\n",
      "Step: 1249279, last rewards: -31.51, max reward: -29.50\n",
      "Loss: -0.0091, policy loss: -0.0833, critic_loss: 0.1644, entropy: 0.7950\n",
      "Returns: 4.4778, advantages: -0.1013, actions: -0.0280, actions std: 0.5332\n",
      "Step: 1269759, last rewards: -29.31, max reward: -26.02\n",
      "Loss: 0.2000, policy loss: -0.0325, critic_loss: 0.4806, entropy: 0.7885\n",
      "Returns: 4.8597, advantages: 0.2156, actions: 0.0728, actions std: 0.5421\n",
      "Step: 1290239, last rewards: -29.57, max reward: -27.23\n",
      "Loss: 0.1091, policy loss: -0.0520, critic_loss: 0.3377, entropy: 0.7822\n",
      "Returns: 3.9893, advantages: -0.7794, actions: 0.0686, actions std: 0.5295\n",
      "Step: 1310719, last rewards: -31.01, max reward: -28.63\n",
      "Loss: 0.3031, policy loss: -0.0133, critic_loss: 0.6484, entropy: 0.7771\n",
      "Returns: 5.9703, advantages: 1.3544, actions: -0.0126, actions std: 0.5524\n",
      "Step: 1331199, last rewards: -29.57, max reward: -25.86\n",
      "Loss: -0.0590, policy loss: -0.0870, critic_loss: 0.0714, entropy: 0.7700\n",
      "Returns: 4.7776, advantages: 0.0492, actions: -0.0722, actions std: 0.5227\n",
      "Step: 1351679, last rewards: -19.67, max reward: 72.81\n",
      "Loss: -0.0679, policy loss: -0.0892, critic_loss: 0.0580, entropy: 0.7652\n",
      "Returns: 5.6329, advantages: 0.9097, actions: 0.0656, actions std: 0.5399\n",
      "Step: 1372159, last rewards: -8.21, max reward: 74.76\n",
      "Loss: 0.0539, policy loss: -0.0897, critic_loss: 0.3023, entropy: 0.7579\n",
      "Returns: 4.2923, advantages: -0.4958, actions: 0.0627, actions std: 0.5281\n",
      "Step: 1392639, last rewards: -29.33, max reward: -26.23\n",
      "Loss: 0.1116, policy loss: -0.0647, critic_loss: 0.3675, entropy: 0.7499\n",
      "Returns: 5.5590, advantages: 0.8480, actions: 0.0331, actions std: 0.5289\n",
      "Step: 1413119, last rewards: -28.39, max reward: -26.51\n",
      "Loss: 0.1195, policy loss: -0.0280, critic_loss: 0.3100, entropy: 0.7429\n",
      "Returns: 4.9839, advantages: 0.2596, actions: -0.0833, actions std: 0.5390\n",
      "Step: 1433599, last rewards: -8.52, max reward: 72.17\n",
      "Loss: 0.0527, policy loss: -0.0241, critic_loss: 0.1682, entropy: 0.7355\n",
      "Returns: 6.0319, advantages: 1.1787, actions: 0.0136, actions std: 0.5430\n",
      "Step: 1454079, last rewards: -27.64, max reward: -24.23\n",
      "Loss: 0.2723, policy loss: -0.0218, critic_loss: 0.6028, entropy: 0.7291\n",
      "Returns: 5.1829, advantages: 0.1360, actions: 0.0323, actions std: 0.4981\n",
      "Step: 1474559, last rewards: -26.51, max reward: -23.39\n",
      "Loss: -0.0196, policy loss: -0.1080, critic_loss: 0.1913, entropy: 0.7237\n",
      "Returns: 5.7105, advantages: 0.6560, actions: -0.0196, actions std: 0.5357\n",
      "Step: 1495039, last rewards: -8.22, max reward: 71.28\n",
      "Loss: 0.4786, policy loss: 0.0194, critic_loss: 0.9328, entropy: 0.7178\n",
      "Returns: 5.4643, advantages: 0.2884, actions: 0.0068, actions std: 0.5021\n",
      "Step: 1515519, last rewards: -27.16, max reward: -24.58\n",
      "Loss: 0.1404, policy loss: -0.0318, critic_loss: 0.3585, entropy: 0.7116\n",
      "Returns: 5.3190, advantages: 0.0374, actions: -0.0526, actions std: 0.5040\n",
      "Step: 1535999, last rewards: -17.96, max reward: 71.77\n",
      "Loss: 1.3518, policy loss: -0.0490, critic_loss: 2.8157, entropy: 0.7027\n",
      "Returns: 5.6049, advantages: 0.3603, actions: 0.0986, actions std: 0.5246\n",
      "Step: 1556479, last rewards: -17.55, max reward: 72.92\n",
      "Loss: 0.0575, policy loss: -0.1195, critic_loss: 0.3677, entropy: 0.6942\n",
      "Returns: 5.2490, advantages: -0.0324, actions: 0.0155, actions std: 0.5045\n",
      "Step: 1576959, last rewards: -28.55, max reward: -21.20\n",
      "Loss: 0.1081, policy loss: -0.1197, critic_loss: 0.4694, entropy: 0.6889\n",
      "Returns: 7.8271, advantages: 2.3767, actions: -0.0300, actions std: 0.5769\n",
      "Step: 1597439, last rewards: -27.37, max reward: -24.10\n",
      "Loss: 1.0478, policy loss: -0.0279, critic_loss: 2.1651, entropy: 0.6822\n",
      "Returns: 6.4402, advantages: 0.7937, actions: -0.0031, actions std: 0.5274\n",
      "Step: 1617919, last rewards: -26.62, max reward: -24.38\n",
      "Loss: -0.0194, policy loss: -0.0784, critic_loss: 0.1317, entropy: 0.6772\n",
      "Returns: 7.1919, advantages: 1.4258, actions: 0.0208, actions std: 0.5426\n",
      "Step: 1638399, last rewards: -27.65, max reward: -23.71\n",
      "Loss: 1.2707, policy loss: -0.0504, critic_loss: 2.6558, entropy: 0.6721\n",
      "Returns: 6.9498, advantages: 1.0397, actions: -0.0339, actions std: 0.5235\n",
      "Step: 1658879, last rewards: -26.57, max reward: -21.97\n",
      "Loss: 0.6084, policy loss: 0.0294, critic_loss: 1.1712, entropy: 0.6680\n",
      "Returns: 6.9376, advantages: 0.8943, actions: -0.0334, actions std: 0.5137\n",
      "Step: 1679359, last rewards: 14.88, max reward: 82.79\n",
      "Loss: 4.0888, policy loss: 0.0674, critic_loss: 8.0561, entropy: 0.6634\n",
      "Returns: 9.0593, advantages: 2.9407, actions: 0.0635, actions std: 0.5170\n",
      "Step: 1699839, last rewards: -25.67, max reward: -21.93\n",
      "Loss: 0.2212, policy loss: -0.0284, critic_loss: 0.5123, entropy: 0.6565\n",
      "Returns: 7.3052, advantages: 1.2219, actions: 0.0150, actions std: 0.5233\n",
      "Step: 1720319, last rewards: -27.00, max reward: -19.55\n",
      "Loss: 0.8590, policy loss: -0.0679, critic_loss: 1.8668, entropy: 0.6519\n",
      "Returns: 6.3581, advantages: 0.1193, actions: -0.0709, actions std: 0.4775\n",
      "Step: 1740799, last rewards: -26.80, max reward: -23.07\n",
      "Loss: -0.0586, policy loss: -0.1068, critic_loss: 0.1093, entropy: 0.6454\n",
      "Returns: 7.1952, advantages: 0.8886, actions: -0.0956, actions std: 0.5041\n",
      "Step: 1761279, last rewards: -14.48, max reward: 77.45\n",
      "Loss: 0.0076, policy loss: -0.1117, critic_loss: 0.2515, entropy: 0.6395\n",
      "Returns: 7.3088, advantages: 0.9664, actions: 0.0658, actions std: 0.4971\n",
      "Step: 1781759, last rewards: -14.77, max reward: 76.69\n",
      "Loss: 1.2091, policy loss: -0.0537, critic_loss: 2.5385, entropy: 0.6351\n",
      "Returns: 7.4698, advantages: 1.0113, actions: -0.0393, actions std: 0.4996\n",
      "Step: 1802239, last rewards: -23.93, max reward: -21.18\n",
      "Loss: -0.0018, policy loss: -0.0318, critic_loss: 0.0726, entropy: 0.6302\n",
      "Returns: 7.4130, advantages: 0.7786, actions: -0.0182, actions std: 0.4794\n",
      "Step: 1822719, last rewards: -23.95, max reward: -21.76\n",
      "Loss: 0.1849, policy loss: -0.0261, critic_loss: 0.4346, entropy: 0.6244\n",
      "Returns: 6.4623, advantages: -0.2739, actions: -0.0076, actions std: 0.4650\n",
      "Step: 1843199, last rewards: -23.73, max reward: -19.88\n",
      "Loss: 0.0245, policy loss: -0.0155, critic_loss: 0.0924, entropy: 0.6193\n",
      "Returns: 7.3288, advantages: 0.5264, actions: -0.1049, actions std: 0.4761\n",
      "Step: 1863679, last rewards: -22.46, max reward: -20.60\n",
      "Loss: 0.3177, policy loss: -0.0043, critic_loss: 0.6562, entropy: 0.6134\n",
      "Returns: 7.2467, advantages: 0.4447, actions: -0.0924, actions std: 0.4676\n",
      "Step: 1884159, last rewards: -2.83, max reward: 80.07\n",
      "Loss: 1.2618, policy loss: -0.0353, critic_loss: 2.6062, entropy: 0.6074\n",
      "Returns: 6.7842, advantages: -0.1297, actions: 0.1423, actions std: 0.4755\n",
      "Step: 1904639, last rewards: -25.78, max reward: -21.37\n",
      "Loss: 1.3918, policy loss: 0.0080, critic_loss: 2.7796, entropy: 0.6019\n",
      "Returns: 8.8137, advantages: 1.9199, actions: -0.1751, actions std: 0.5026\n",
      "Step: 1925119, last rewards: -24.13, max reward: -19.74\n",
      "Loss: 0.0782, policy loss: -0.0529, critic_loss: 0.2742, entropy: 0.5985\n",
      "Returns: 7.2689, advantages: 0.1970, actions: -0.0031, actions std: 0.4882\n",
      "Step: 1945599, last rewards: -14.88, max reward: 81.96\n",
      "Loss: 1.0833, policy loss: 0.0351, critic_loss: 2.1083, entropy: 0.5947\n",
      "Returns: 8.5020, advantages: 1.4120, actions: -0.1034, actions std: 0.5080\n",
      "Step: 1966079, last rewards: 17.46, max reward: 80.39\n",
      "Loss: 0.1803, policy loss: -0.0472, critic_loss: 0.4669, entropy: 0.5887\n",
      "Returns: 10.7622, advantages: 3.6667, actions: 0.0874, actions std: 0.4995\n",
      "Step: 1986559, last rewards: -13.32, max reward: 77.83\n",
      "Loss: 0.2457, policy loss: -0.0052, critic_loss: 0.5136, entropy: 0.5839\n",
      "Returns: 7.7887, advantages: 0.5968, actions: -0.1007, actions std: 0.4739\n",
      "Step: 2007039, last rewards: -21.86, max reward: -18.51\n",
      "Loss: 0.7083, policy loss: 0.0277, critic_loss: 1.3727, entropy: 0.5781\n",
      "Returns: 7.1534, advantages: -0.0582, actions: 0.1472, actions std: 0.4593\n",
      "Step: 2027519, last rewards: -21.55, max reward: -18.78\n",
      "Loss: 0.0359, policy loss: -0.0913, critic_loss: 0.2657, entropy: 0.5726\n",
      "Returns: 7.0015, advantages: -0.2310, actions: -0.0388, actions std: 0.4416\n",
      "Step: 2047999, last rewards: 19.94, max reward: 84.74\n",
      "Loss: 1.2661, policy loss: 0.0675, critic_loss: 2.4086, entropy: 0.5667\n",
      "Returns: 7.7299, advantages: 0.5640, actions: 0.1145, actions std: 0.4544\n",
      "Step: 2068479, last rewards: -20.44, max reward: -17.90\n",
      "Loss: 0.2696, policy loss: 0.0172, critic_loss: 0.5159, entropy: 0.5616\n",
      "Returns: 8.0315, advantages: 0.9156, actions: 0.0096, actions std: 0.4698\n",
      "Step: 2088959, last rewards: -21.29, max reward: -18.46\n",
      "Loss: 0.5539, policy loss: 0.0352, critic_loss: 1.0486, entropy: 0.5552\n",
      "Returns: 8.4506, advantages: 1.3918, actions: 0.0131, actions std: 0.4776\n",
      "Step: 2109439, last rewards: 49.81, max reward: 85.77\n",
      "Loss: -0.0092, policy loss: -0.0604, critic_loss: 0.1133, entropy: 0.5494\n",
      "Returns: 9.2464, advantages: 2.1128, actions: 0.1183, actions std: 0.4472\n",
      "Step: 2129919, last rewards: -21.68, max reward: -18.47\n",
      "Loss: 1.0822, policy loss: -0.0895, critic_loss: 2.3541, entropy: 0.5421\n",
      "Returns: 8.3061, advantages: 1.0781, actions: -0.0758, actions std: 0.4620\n",
      "Step: 2150399, last rewards: -20.10, max reward: -16.88\n",
      "Loss: 0.1541, policy loss: -0.1033, critic_loss: 0.5255, entropy: 0.5363\n",
      "Returns: 8.1262, advantages: 0.8883, actions: -0.0100, actions std: 0.4500\n",
      "Step: 2170879, last rewards: 52.24, max reward: 87.24\n",
      "Loss: 0.9460, policy loss: 0.0143, critic_loss: 1.8740, entropy: 0.5304\n",
      "Returns: 7.3007, advantages: -0.0110, actions: 0.0716, actions std: 0.4310\n",
      "Step: 2191359, last rewards: 51.18, max reward: 87.48\n",
      "Loss: 0.0752, policy loss: -0.0305, critic_loss: 0.2218, entropy: 0.5242\n",
      "Returns: 10.4812, advantages: 3.2517, actions: 0.1994, actions std: 0.4377\n",
      "Step: 2211839, last rewards: 0.77, max reward: 85.52\n",
      "Loss: -0.0471, policy loss: -0.0764, critic_loss: 0.0690, entropy: 0.5203\n",
      "Returns: 7.7508, advantages: 0.4242, actions: -0.0042, actions std: 0.4325\n",
      "Step: 2232319, last rewards: 48.44, max reward: 83.86\n",
      "Loss: 0.1739, policy loss: 0.0748, critic_loss: 0.2084, entropy: 0.5144\n",
      "Returns: 8.0588, advantages: 0.8188, actions: 0.1808, actions std: 0.4242\n",
      "Step: 2252799, last rewards: -19.62, max reward: -17.09\n",
      "Loss: 0.3179, policy loss: 0.0694, critic_loss: 0.5073, entropy: 0.5106\n",
      "Returns: 7.6789, advantages: 0.4346, actions: -0.0407, actions std: 0.4454\n",
      "Step: 2273279, last rewards: -20.55, max reward: -17.89\n",
      "Loss: 0.0368, policy loss: -0.0317, critic_loss: 0.1471, entropy: 0.5088\n",
      "Returns: 7.6643, advantages: 0.2874, actions: -0.0529, actions std: 0.4265\n",
      "Step: 2293759, last rewards: -9.61, max reward: 81.24\n",
      "Loss: 0.2302, policy loss: -0.0032, critic_loss: 0.4768, entropy: 0.5018\n",
      "Returns: 7.9344, advantages: 0.5527, actions: -0.0048, actions std: 0.4412\n",
      "Step: 2314239, last rewards: -0.41, max reward: 87.81\n",
      "Loss: 0.7005, policy loss: 0.0504, critic_loss: 1.3102, entropy: 0.4984\n",
      "Returns: 9.2018, advantages: 1.8182, actions: -0.0438, actions std: 0.4779\n",
      "Step: 2334719, last rewards: -21.90, max reward: -17.64\n",
      "Loss: 0.9094, policy loss: -0.0258, critic_loss: 1.8801, entropy: 0.4947\n",
      "Returns: 8.8515, advantages: 1.3271, actions: -0.1010, actions std: 0.4649\n",
      "Step: 2355199, last rewards: -11.79, max reward: 79.47\n",
      "Loss: 1.0104, policy loss: 0.0097, critic_loss: 2.0111, entropy: 0.4908\n",
      "Returns: 10.3674, advantages: 2.6543, actions: -0.0241, actions std: 0.4633\n",
      "Step: 2375679, last rewards: 12.31, max reward: 86.57\n",
      "Loss: 2.9243, policy loss: -0.0305, critic_loss: 5.9193, entropy: 0.4833\n",
      "Returns: 10.8043, advantages: 3.0818, actions: 0.0916, actions std: 0.4422\n",
      "Step: 2396159, last rewards: -9.11, max reward: 79.58\n",
      "Loss: 1.1791, policy loss: -0.0191, critic_loss: 2.4059, entropy: 0.4785\n",
      "Returns: 9.2136, advantages: 1.4443, actions: -0.0382, actions std: 0.4446\n",
      "Step: 2416639, last rewards: -8.82, max reward: 82.60\n",
      "Loss: 0.1290, policy loss: -0.0199, critic_loss: 0.3075, entropy: 0.4766\n",
      "Returns: 9.3453, advantages: 1.3858, actions: 0.0085, actions std: 0.4342\n",
      "Step: 2437119, last rewards: 23.35, max reward: 85.89\n",
      "Loss: 0.2380, policy loss: 0.0576, critic_loss: 0.3702, entropy: 0.4711\n",
      "Returns: 8.2498, advantages: 0.2462, actions: -0.0874, actions std: 0.4140\n",
      "Step: 2457599, last rewards: -7.01, max reward: 87.15\n",
      "Loss: 0.1007, policy loss: -0.0010, critic_loss: 0.2127, entropy: 0.4657\n",
      "Returns: 9.3767, advantages: 1.3126, actions: 0.0032, actions std: 0.4248\n",
      "Step: 2478079, last rewards: -17.80, max reward: -14.08\n",
      "Loss: 0.0797, policy loss: 0.0043, critic_loss: 0.1599, entropy: 0.4627\n",
      "Returns: 9.0828, advantages: 1.0352, actions: -0.0525, actions std: 0.4415\n",
      "Step: 2498559, last rewards: 52.40, max reward: 84.77\n",
      "Loss: 4.1888, policy loss: 0.0167, critic_loss: 8.3535, entropy: 0.4595\n",
      "Returns: 10.9471, advantages: 2.8764, actions: -0.0092, actions std: 0.4364\n",
      "Step: 2519039, last rewards: 33.00, max reward: 89.96\n",
      "Loss: 0.3681, policy loss: -0.0403, critic_loss: 0.8258, entropy: 0.4539\n",
      "Returns: 10.6097, advantages: 2.4649, actions: -0.0790, actions std: 0.4486\n",
      "Step: 2539519, last rewards: 80.60, max reward: 89.27\n",
      "Loss: 0.2502, policy loss: -0.0306, critic_loss: 0.5706, entropy: 0.4509\n",
      "Returns: 11.0662, advantages: 2.7952, actions: 0.2660, actions std: 0.4276\n",
      "Step: 2559999, last rewards: 74.52, max reward: 89.67\n",
      "Loss: 1.5037, policy loss: 0.0523, critic_loss: 2.9116, entropy: 0.4471\n",
      "Returns: 10.9177, advantages: 2.6132, actions: 0.0208, actions std: 0.4387\n",
      "Step: 2580479, last rewards: 75.81, max reward: 89.00\n",
      "Loss: 3.7110, policy loss: -0.0132, critic_loss: 7.4573, entropy: 0.4428\n",
      "Returns: 11.0419, advantages: 2.7342, actions: 0.1556, actions std: 0.4075\n",
      "Step: 2600959, last rewards: -9.56, max reward: 84.72\n",
      "Loss: 0.7039, policy loss: -0.0232, critic_loss: 1.4630, entropy: 0.4393\n",
      "Returns: 9.6179, advantages: 1.1441, actions: -0.1513, actions std: 0.4189\n",
      "Step: 2621439, last rewards: -18.88, max reward: -15.91\n",
      "Loss: 0.0361, policy loss: -0.0591, critic_loss: 0.1991, entropy: 0.4361\n",
      "Returns: 9.2039, advantages: 0.6287, actions: -0.1050, actions std: 0.4171\n",
      "Step: 2641919, last rewards: 1.79, max reward: 87.04\n",
      "Loss: 0.2416, policy loss: 0.0616, critic_loss: 0.3686, entropy: 0.4321\n",
      "Returns: 9.8503, advantages: 1.2229, actions: -0.1046, actions std: 0.4372\n",
      "Step: 2662399, last rewards: -20.75, max reward: -15.28\n",
      "Loss: 0.4373, policy loss: -0.0275, critic_loss: 0.9382, entropy: 0.4287\n",
      "Returns: 10.0576, advantages: 1.3735, actions: -0.0957, actions std: 0.4375\n",
      "Step: 2682879, last rewards: -24.43, max reward: -20.48\n",
      "Loss: 0.2139, policy loss: 0.0445, critic_loss: 0.3474, entropy: 0.4262\n",
      "Returns: 10.7086, advantages: 1.9182, actions: -0.2188, actions std: 0.4678\n",
      "Step: 2703359, last rewards: 76.70, max reward: 92.00\n",
      "Loss: 0.3028, policy loss: -0.0400, critic_loss: 0.6940, entropy: 0.4230\n",
      "Returns: 10.7903, advantages: 1.8972, actions: 0.0812, actions std: 0.4060\n",
      "Step: 2723839, last rewards: 13.56, max reward: 90.06\n",
      "Loss: 1.3633, policy loss: -0.0711, critic_loss: 2.8772, entropy: 0.4215\n",
      "Returns: 10.9888, advantages: 2.1172, actions: -0.1252, actions std: 0.4449\n",
      "Step: 2744319, last rewards: -27.77, max reward: -25.06\n",
      "Loss: 0.7500, policy loss: -0.0383, critic_loss: 1.5851, entropy: 0.4215\n",
      "Returns: 10.9514, advantages: 1.9192, actions: -0.2862, actions std: 0.4472\n",
      "Step: 2764799, last rewards: 55.63, max reward: 89.92\n",
      "Loss: 0.0719, policy loss: -0.0881, critic_loss: 0.3285, entropy: 0.4210\n",
      "Returns: 11.4891, advantages: 2.2994, actions: 0.0944, actions std: 0.3996\n",
      "Step: 2785279, last rewards: 75.67, max reward: 89.26\n",
      "Loss: 0.1970, policy loss: -0.1072, critic_loss: 0.6169, entropy: 0.4174\n",
      "Returns: 10.5912, advantages: 1.3993, actions: 0.1100, actions std: 0.3907\n",
      "Step: 2805759, last rewards: 77.53, max reward: 90.57\n",
      "Loss: 0.5005, policy loss: 0.0125, critic_loss: 0.9842, entropy: 0.4144\n",
      "Returns: 9.9910, advantages: 0.8586, actions: 0.1908, actions std: 0.3804\n",
      "Step: 2826239, last rewards: 78.29, max reward: 92.70\n",
      "Loss: 3.5647, policy loss: -0.0330, critic_loss: 7.2038, entropy: 0.4121\n",
      "Returns: 11.0135, advantages: 1.9928, actions: 0.0735, actions std: 0.4077\n",
      "Step: 2846719, last rewards: 43.93, max reward: 91.08\n",
      "Loss: 0.1976, policy loss: 0.0338, critic_loss: 0.3358, entropy: 0.4115\n",
      "Returns: 10.7122, advantages: 1.6731, actions: -0.0648, actions std: 0.4474\n",
      "Step: 2867199, last rewards: 89.18, max reward: 92.31\n",
      "Loss: 0.0401, policy loss: -0.0894, critic_loss: 0.2673, entropy: 0.4099\n",
      "Returns: 11.6733, advantages: 2.5198, actions: -0.0297, actions std: 0.4011\n",
      "Step: 2887679, last rewards: 76.35, max reward: 92.33\n",
      "Loss: 1.6271, policy loss: 0.0685, critic_loss: 3.1254, entropy: 0.4096\n",
      "Returns: 11.5311, advantages: 2.4009, actions: 0.0080, actions std: 0.4156\n",
      "Step: 2908159, last rewards: 67.05, max reward: 90.75\n",
      "Loss: 3.7232, policy loss: -0.0571, critic_loss: 7.5687, entropy: 0.4070\n",
      "Returns: 11.8101, advantages: 2.6059, actions: 0.0415, actions std: 0.4053\n",
      "Step: 2928639, last rewards: 56.94, max reward: 90.35\n",
      "Loss: 3.8492, policy loss: 0.0868, critic_loss: 7.5329, entropy: 0.4049\n",
      "Returns: 11.6443, advantages: 2.4970, actions: 0.0297, actions std: 0.3962\n",
      "Step: 2949119, last rewards: 88.51, max reward: 91.17\n",
      "Loss: 3.7659, policy loss: 0.0546, critic_loss: 7.4307, entropy: 0.4027\n",
      "Returns: 12.8792, advantages: 3.7394, actions: 0.0930, actions std: 0.4111\n",
      "Step: 2969599, last rewards: 89.36, max reward: 92.54\n",
      "Loss: 4.0023, policy loss: -0.0012, critic_loss: 8.0150, entropy: 0.4010\n",
      "Returns: 12.5387, advantages: 3.4638, actions: 0.0207, actions std: 0.4142\n",
      "Step: 2990079, last rewards: 21.36, max reward: 86.31\n",
      "Loss: 0.0998, policy loss: -0.0333, critic_loss: 0.2741, entropy: 0.3983\n",
      "Returns: 11.4925, advantages: 2.2852, actions: -0.0969, actions std: 0.4445\n",
      "Step: 3010559, last rewards: 67.00, max reward: 92.55\n",
      "Loss: 0.3013, policy loss: -0.0411, critic_loss: 0.6928, entropy: 0.3984\n",
      "Returns: 11.5380, advantages: 2.3125, actions: 0.2025, actions std: 0.3916\n",
      "Step: 3031039, last rewards: 89.47, max reward: 92.42\n",
      "Loss: 3.8874, policy loss: -0.0432, critic_loss: 7.8691, entropy: 0.3923\n",
      "Returns: 12.8868, advantages: 3.7440, actions: 0.0460, actions std: 0.4114\n",
      "Step: 3051519, last rewards: 90.52, max reward: 94.66\n",
      "Loss: 0.0451, policy loss: -0.0625, critic_loss: 0.2230, entropy: 0.3875\n",
      "Returns: 12.6332, advantages: 3.4267, actions: 0.1356, actions std: 0.4033\n",
      "Step: 3071999, last rewards: 89.43, max reward: 92.59\n",
      "Loss: 0.4828, policy loss: -0.0316, critic_loss: 1.0367, entropy: 0.3842\n",
      "Returns: 11.1977, advantages: 2.0428, actions: 0.2083, actions std: 0.3824\n",
      "Step: 3092479, last rewards: 89.85, max reward: 93.53\n",
      "Loss: 0.3836, policy loss: -0.0077, critic_loss: 0.7902, entropy: 0.3820\n",
      "Returns: 11.1594, advantages: 2.0746, actions: 0.1352, actions std: 0.3714\n",
      "Step: 3112959, last rewards: 91.75, max reward: 94.08\n",
      "Loss: 1.3462, policy loss: -0.0148, critic_loss: 2.7296, entropy: 0.3778\n",
      "Returns: 12.2786, advantages: 3.2397, actions: -0.0090, actions std: 0.3884\n",
      "Step: 3133439, last rewards: 90.11, max reward: 92.58\n",
      "Loss: 3.8079, policy loss: 0.0004, critic_loss: 7.6225, entropy: 0.3761\n",
      "Returns: 12.2198, advantages: 3.1612, actions: 0.2347, actions std: 0.3819\n",
      "Step: 3153919, last rewards: 92.27, max reward: 94.79\n",
      "Loss: 3.6631, policy loss: -0.0104, critic_loss: 7.3544, entropy: 0.3724\n",
      "Returns: 13.1238, advantages: 4.0822, actions: 0.0252, actions std: 0.3893\n",
      "Step: 3174399, last rewards: 91.19, max reward: 93.15\n",
      "Loss: 3.8123, policy loss: 0.0531, critic_loss: 7.5259, entropy: 0.3724\n",
      "Returns: 12.4982, advantages: 3.5583, actions: -0.0173, actions std: 0.3908\n",
      "Step: 3194879, last rewards: 91.83, max reward: 93.24\n",
      "Loss: 0.4384, policy loss: -0.0337, critic_loss: 0.9516, entropy: 0.3719\n",
      "Returns: 11.4264, advantages: 2.4211, actions: 0.1571, actions std: 0.3828\n",
      "Step: 3215359, last rewards: 68.31, max reward: 92.22\n",
      "Loss: 0.6671, policy loss: 0.0105, critic_loss: 1.3205, entropy: 0.3679\n",
      "Returns: 10.0128, advantages: 1.0435, actions: -0.1227, actions std: 0.3955\n",
      "Step: 3235839, last rewards: 90.66, max reward: 93.71\n",
      "Loss: 0.3402, policy loss: 0.0517, critic_loss: 0.5843, entropy: 0.3649\n",
      "Returns: 11.1709, advantages: 2.1315, actions: 0.2320, actions std: 0.3713\n",
      "Step: 3256319, last rewards: 91.30, max reward: 93.61\n",
      "Loss: 0.6369, policy loss: -0.0048, critic_loss: 1.2906, entropy: 0.3636\n",
      "Returns: 11.4809, advantages: 2.5052, actions: 0.1114, actions std: 0.3735\n",
      "Step: 3276799, last rewards: 92.11, max reward: 93.91\n",
      "Loss: 3.5218, policy loss: 0.0516, critic_loss: 6.9476, entropy: 0.3608\n",
      "Returns: 11.8321, advantages: 2.8935, actions: -0.0260, actions std: 0.3667\n",
      "Step: 3297279, last rewards: 81.57, max reward: 93.90\n",
      "Loss: 0.2705, policy loss: -0.0886, critic_loss: 0.7255, entropy: 0.3613\n",
      "Returns: 11.9125, advantages: 2.9549, actions: -0.0030, actions std: 0.3703\n",
      "Step: 3317759, last rewards: 89.40, max reward: 91.95\n",
      "Loss: 3.4621, policy loss: -0.0425, critic_loss: 7.0165, entropy: 0.3609\n",
      "Returns: 13.2083, advantages: 4.0962, actions: 0.3365, actions std: 0.3860\n",
      "Step: 3338239, last rewards: 88.38, max reward: 91.35\n",
      "Loss: 0.2399, policy loss: 0.0219, critic_loss: 0.4432, entropy: 0.3593\n",
      "Returns: 12.1873, advantages: 3.1563, actions: 0.3614, actions std: 0.3880\n",
      "Step: 3358719, last rewards: 90.51, max reward: 93.10\n",
      "Loss: 0.3123, policy loss: 0.0175, critic_loss: 0.5966, entropy: 0.3597\n",
      "Returns: 12.3099, advantages: 3.3554, actions: 0.2383, actions std: 0.3824\n",
      "Step: 3379199, last rewards: 92.16, max reward: 95.02\n",
      "Loss: 0.1247, policy loss: -0.0602, critic_loss: 0.3771, entropy: 0.3589\n",
      "Returns: 12.1737, advantages: 3.2991, actions: 0.1800, actions std: 0.3790\n",
      "Step: 3399679, last rewards: 89.36, max reward: 93.13\n",
      "Loss: 0.3998, policy loss: 0.0628, critic_loss: 0.6811, entropy: 0.3564\n",
      "Returns: 11.3837, advantages: 2.5458, actions: 0.2113, actions std: 0.3809\n",
      "Step: 3420159, last rewards: 91.63, max reward: 93.55\n",
      "Loss: 0.2615, policy loss: -0.0342, critic_loss: 0.5985, entropy: 0.3578\n",
      "Returns: 12.2554, advantages: 3.4445, actions: 0.1227, actions std: 0.3830\n",
      "Step: 3440639, last rewards: -11.11, max reward: 87.24\n",
      "Loss: 0.9499, policy loss: 0.0283, critic_loss: 1.8503, entropy: 0.3547\n",
      "Returns: 10.9463, advantages: 2.0306, actions: -0.2330, actions std: 0.4264\n",
      "Step: 3461119, last rewards: 34.40, max reward: 93.90\n",
      "Loss: 1.4110, policy loss: 0.0083, critic_loss: 2.8126, entropy: 0.3533\n",
      "Returns: 11.2612, advantages: 2.0868, actions: -0.2071, actions std: 0.4290\n",
      "Step: 3481599, last rewards: -29.25, max reward: -26.39\n",
      "Loss: 0.3101, policy loss: -0.0215, critic_loss: 0.6702, entropy: 0.3549\n",
      "Returns: 10.5853, advantages: 1.0687, actions: -0.2963, actions std: 0.4174\n",
      "Step: 3502079, last rewards: 66.70, max reward: 94.74\n",
      "Loss: 0.2023, policy loss: 0.0335, critic_loss: 0.3446, entropy: 0.3534\n",
      "Returns: 13.7016, advantages: 3.8962, actions: -0.0704, actions std: 0.4025\n",
      "Step: 3522559, last rewards: 92.57, max reward: 94.32\n",
      "Loss: 0.2382, policy loss: -0.0912, critic_loss: 0.6658, entropy: 0.3514\n",
      "Returns: 12.7967, advantages: 3.0320, actions: 0.0656, actions std: 0.3814\n",
      "Step: 3543039, last rewards: 92.69, max reward: 94.29\n",
      "Loss: 0.2693, policy loss: 0.0743, critic_loss: 0.3970, entropy: 0.3479\n",
      "Returns: 12.9442, advantages: 3.1849, actions: 0.0520, actions std: 0.3941\n",
      "Step: 3563519, last rewards: 19.74, max reward: 93.53\n",
      "Loss: 1.2039, policy loss: 0.0710, critic_loss: 2.2727, entropy: 0.3498\n",
      "Returns: 13.1074, advantages: 3.1927, actions: -0.1335, actions std: 0.4153\n",
      "Step: 3583999, last rewards: 91.33, max reward: 92.77\n",
      "Loss: 0.2642, policy loss: 0.0338, critic_loss: 0.4676, entropy: 0.3480\n",
      "Returns: 13.9359, advantages: 4.0211, actions: 0.2568, actions std: 0.4162\n",
      "Step: 3604479, last rewards: 91.62, max reward: 93.36\n",
      "Loss: 0.1336, policy loss: -0.0072, critic_loss: 0.2887, entropy: 0.3492\n",
      "Returns: 14.0056, advantages: 4.2367, actions: 0.1661, actions std: 0.4103\n",
      "Step: 3624959, last rewards: 89.32, max reward: 92.68\n",
      "Loss: 3.3387, policy loss: -0.0634, critic_loss: 6.8112, entropy: 0.3500\n",
      "Returns: 14.0580, advantages: 4.3394, actions: 0.2770, actions std: 0.3883\n",
      "Step: 3645439, last rewards: 92.48, max reward: 94.01\n",
      "Loss: 3.4193, policy loss: -0.0458, critic_loss: 6.9373, entropy: 0.3480\n",
      "Returns: 13.8505, advantages: 4.1146, actions: 0.0208, actions std: 0.4094\n",
      "Step: 3665919, last rewards: 92.16, max reward: 94.17\n",
      "Loss: 3.8121, policy loss: 0.0461, critic_loss: 7.5388, entropy: 0.3488\n",
      "Returns: 14.0456, advantages: 4.2843, actions: 0.1020, actions std: 0.4128\n",
      "Step: 3686399, last rewards: 93.68, max reward: 94.85\n",
      "Loss: 3.6442, policy loss: -0.0039, critic_loss: 7.3034, entropy: 0.3490\n",
      "Returns: 14.6444, advantages: 4.9379, actions: 0.0234, actions std: 0.4049\n",
      "Step: 3706879, last rewards: 92.67, max reward: 94.93\n",
      "Loss: 3.6458, policy loss: 0.0196, critic_loss: 7.2593, entropy: 0.3470\n",
      "Returns: 13.7410, advantages: 4.0994, actions: 0.1512, actions std: 0.3971\n",
      "Step: 3727359, last rewards: 91.54, max reward: 93.57\n",
      "Loss: 2.8767, policy loss: -0.0053, critic_loss: 5.7709, entropy: 0.3483\n",
      "Returns: 13.1927, advantages: 3.5581, actions: 0.2432, actions std: 0.3747\n",
      "Step: 3747839, last rewards: 92.35, max reward: 94.82\n",
      "Loss: 3.9116, policy loss: 0.0492, critic_loss: 7.7317, entropy: 0.3472\n",
      "Returns: 13.7439, advantages: 4.1674, actions: 0.0883, actions std: 0.4082\n",
      "Step: 3768319, last rewards: 87.53, max reward: 91.58\n",
      "Loss: 0.2689, policy loss: -0.0275, critic_loss: 0.5998, entropy: 0.3447\n",
      "Returns: 12.9176, advantages: 3.3744, actions: 0.3607, actions std: 0.3690\n",
      "Step: 3788799, last rewards: 92.45, max reward: 94.48\n",
      "Loss: 1.9586, policy loss: -0.0910, critic_loss: 4.1060, entropy: 0.3437\n",
      "Returns: 13.8783, advantages: 4.3312, actions: -0.0842, actions std: 0.4065\n",
      "Step: 3809279, last rewards: 89.69, max reward: 93.83\n",
      "Loss: 1.4889, policy loss: 0.0179, critic_loss: 2.9488, entropy: 0.3442\n",
      "Returns: 12.9839, advantages: 3.3784, actions: -0.1881, actions std: 0.4559\n",
      "Step: 3829759, last rewards: 93.30, max reward: 95.08\n",
      "Loss: 2.4213, policy loss: 0.0490, critic_loss: 4.7514, entropy: 0.3411\n",
      "Returns: 13.8300, advantages: 4.1396, actions: 0.0930, actions std: 0.3976\n",
      "Step: 3850239, last rewards: 90.09, max reward: 92.99\n",
      "Loss: 0.2366, policy loss: 0.0387, critic_loss: 0.4028, entropy: 0.3417\n",
      "Returns: 12.4437, advantages: 2.7913, actions: 0.3006, actions std: 0.3725\n",
      "Step: 3870719, last rewards: 91.06, max reward: 93.05\n",
      "Loss: 0.3712, policy loss: -0.0495, critic_loss: 0.8482, entropy: 0.3415\n",
      "Returns: 12.5426, advantages: 2.9745, actions: 0.1788, actions std: 0.3858\n",
      "Step: 3891199, last rewards: 90.48, max reward: 93.75\n",
      "Loss: 0.1365, policy loss: -0.0761, critic_loss: 0.4321, entropy: 0.3431\n",
      "Returns: 13.2979, advantages: 3.7778, actions: 0.3190, actions std: 0.3873\n",
      "Step: 3911679, last rewards: 87.84, max reward: 91.16\n",
      "Loss: 3.3467, policy loss: 0.0257, critic_loss: 6.6489, entropy: 0.3410\n",
      "Returns: 14.1385, advantages: 4.7134, actions: 0.4316, actions std: 0.3926\n",
      "Step: 3932159, last rewards: 93.46, max reward: 95.71\n",
      "Loss: 3.2872, policy loss: -0.0448, critic_loss: 6.6707, entropy: 0.3428\n",
      "Returns: 13.4803, advantages: 4.1175, actions: 0.0658, actions std: 0.3880\n",
      "Step: 3952639, last rewards: 92.73, max reward: 94.52\n",
      "Loss: 3.4593, policy loss: 0.0270, critic_loss: 6.8713, entropy: 0.3437\n",
      "Returns: 13.5309, advantages: 4.1652, actions: 0.1154, actions std: 0.3953\n",
      "Step: 3973119, last rewards: 92.49, max reward: 95.43\n",
      "Loss: 2.7518, policy loss: -0.0017, critic_loss: 5.5141, entropy: 0.3445\n",
      "Returns: 12.8120, advantages: 3.4469, actions: -0.0701, actions std: 0.3947\n",
      "Step: 3993599, last rewards: 93.67, max reward: 95.69\n",
      "Loss: 0.5777, policy loss: -0.0313, critic_loss: 1.2248, entropy: 0.3439\n",
      "Returns: 12.4950, advantages: 3.1118, actions: -0.0562, actions std: 0.3813\n",
      "Step: 4014079, last rewards: 93.94, max reward: 95.33\n",
      "Loss: 3.2471, policy loss: 0.0262, critic_loss: 6.4486, entropy: 0.3419\n",
      "Returns: 13.6634, advantages: 4.1693, actions: -0.0029, actions std: 0.3953\n",
      "Step: 4034559, last rewards: 81.03, max reward: 89.42\n",
      "Loss: 3.0957, policy loss: -0.0470, critic_loss: 6.2922, entropy: 0.3399\n",
      "Returns: 14.2666, advantages: 4.8023, actions: 0.5975, actions std: 0.3752\n",
      "Step: 4055039, last rewards: 81.79, max reward: 88.92\n",
      "Loss: 0.3850, policy loss: 0.0278, critic_loss: 0.7213, entropy: 0.3410\n",
      "Returns: 12.4572, advantages: 3.2752, actions: 0.5760, actions std: 0.3777\n",
      "Step: 4075519, last rewards: 82.25, max reward: 86.13\n",
      "Loss: 3.5632, policy loss: -0.0491, critic_loss: 7.2312, entropy: 0.3380\n",
      "Returns: 12.4963, advantages: 3.4958, actions: 0.5249, actions std: 0.3861\n",
      "Step: 4095999, last rewards: 92.38, max reward: 95.86\n",
      "Loss: 3.4382, policy loss: -0.0360, critic_loss: 6.9551, entropy: 0.3392\n",
      "Returns: 12.9749, advantages: 4.0949, actions: 0.1601, actions std: 0.4004\n",
      "Step: 4116479, last rewards: 94.00, max reward: 95.87\n",
      "Loss: 2.6611, policy loss: -0.0270, critic_loss: 5.3831, entropy: 0.3395\n",
      "Returns: 12.9732, advantages: 4.1645, actions: 0.0612, actions std: 0.3949\n",
      "Step: 4136959, last rewards: 93.26, max reward: 95.06\n",
      "Loss: 0.0691, policy loss: -0.1178, critic_loss: 0.3805, entropy: 0.3399\n",
      "Returns: 12.9691, advantages: 4.2171, actions: 0.1454, actions std: 0.4012\n",
      "Step: 4157439, last rewards: 93.80, max reward: 95.12\n",
      "Loss: 0.4048, policy loss: 0.0014, critic_loss: 0.8136, entropy: 0.3377\n",
      "Returns: 14.2131, advantages: 5.4148, actions: -0.0400, actions std: 0.4224\n",
      "Step: 4177919, last rewards: 93.19, max reward: 95.47\n",
      "Loss: 3.5926, policy loss: 0.0026, critic_loss: 7.1867, entropy: 0.3373\n",
      "Returns: 13.9923, advantages: 5.1614, actions: -0.0273, actions std: 0.4017\n",
      "Step: 4198399, last rewards: 93.31, max reward: 95.77\n",
      "Loss: 0.3037, policy loss: 0.0121, critic_loss: 0.5901, entropy: 0.3406\n",
      "Returns: 12.8611, advantages: 3.9738, actions: 0.2017, actions std: 0.3923\n",
      "Step: 4218879, last rewards: 90.71, max reward: 94.78\n",
      "Loss: 3.4736, policy loss: -0.0491, critic_loss: 7.0523, entropy: 0.3420\n",
      "Returns: 11.5804, advantages: 2.8104, actions: 0.2925, actions std: 0.3914\n",
      "Step: 4239359, last rewards: 92.55, max reward: 94.32\n",
      "Loss: 0.2043, policy loss: -0.0272, critic_loss: 0.4698, entropy: 0.3433\n",
      "Returns: 12.1923, advantages: 3.5881, actions: 0.0821, actions std: 0.4058\n",
      "Step: 4259839, last rewards: 93.14, max reward: 95.43\n",
      "Loss: 3.2065, policy loss: -0.0406, critic_loss: 6.5010, entropy: 0.3434\n",
      "Returns: 12.5328, advantages: 3.9252, actions: 0.0948, actions std: 0.3998\n",
      "Step: 4280319, last rewards: 88.94, max reward: 92.78\n",
      "Loss: 3.8458, policy loss: 0.1066, critic_loss: 7.4853, entropy: 0.3417\n",
      "Returns: 12.4141, advantages: 3.8808, actions: 0.3720, actions std: 0.3911\n",
      "Step: 4300799, last rewards: 91.45, max reward: 94.40\n",
      "Loss: 3.8185, policy loss: 0.0179, critic_loss: 7.6082, entropy: 0.3420\n",
      "Returns: 12.5112, advantages: 4.0448, actions: 0.2228, actions std: 0.3869\n",
      "Step: 4321279, last rewards: 93.34, max reward: 96.13\n",
      "Loss: 0.1673, policy loss: 0.0034, critic_loss: 0.3345, entropy: 0.3418\n",
      "Returns: 12.7807, advantages: 4.2916, actions: 0.0489, actions std: 0.4055\n",
      "Step: 4341759, last rewards: 92.54, max reward: 94.65\n",
      "Loss: 3.2750, policy loss: 0.0391, critic_loss: 6.4786, entropy: 0.3412\n",
      "Returns: 12.9113, advantages: 4.3631, actions: 0.1162, actions std: 0.4073\n",
      "Step: 4362239, last rewards: 91.68, max reward: 94.88\n",
      "Loss: 3.3073, policy loss: -0.0353, critic_loss: 6.6920, entropy: 0.3365\n",
      "Returns: 13.5875, advantages: 4.9944, actions: -0.0773, actions std: 0.4225\n",
      "Step: 4382719, last rewards: 90.77, max reward: 93.74\n",
      "Loss: 3.3107, policy loss: 0.0403, critic_loss: 6.5477, entropy: 0.3366\n",
      "Returns: 13.0972, advantages: 4.5148, actions: 0.3645, actions std: 0.4000\n",
      "Step: 4403199, last rewards: 91.85, max reward: 93.99\n",
      "Loss: 3.5461, policy loss: -0.0573, critic_loss: 7.2135, entropy: 0.3384\n",
      "Returns: 13.3392, advantages: 4.9464, actions: 0.2444, actions std: 0.4109\n",
      "Step: 4423679, last rewards: 91.67, max reward: 94.34\n",
      "Loss: 3.5382, policy loss: -0.0683, critic_loss: 7.2200, entropy: 0.3417\n",
      "Returns: 13.5497, advantages: 5.1147, actions: -0.1046, actions std: 0.4413\n",
      "Step: 4444159, last rewards: 86.07, max reward: 90.55\n",
      "Loss: 0.5255, policy loss: -0.1075, critic_loss: 1.2729, entropy: 0.3418\n",
      "Returns: 11.8302, advantages: 3.3808, actions: 0.5666, actions std: 0.3691\n",
      "Step: 4464639, last rewards: 92.62, max reward: 95.16\n",
      "Loss: 0.1708, policy loss: -0.0144, critic_loss: 0.3775, entropy: 0.3448\n",
      "Returns: 12.1460, advantages: 3.8498, actions: 0.1516, actions std: 0.4021\n",
      "Step: 4485119, last rewards: 92.41, max reward: 95.32\n",
      "Loss: 3.1664, policy loss: -0.0345, critic_loss: 6.4087, entropy: 0.3461\n",
      "Returns: 12.3537, advantages: 4.1239, actions: 0.0179, actions std: 0.4123\n",
      "Step: 4505599, last rewards: 93.28, max reward: 95.49\n",
      "Loss: 3.5589, policy loss: 0.0144, critic_loss: 7.0959, entropy: 0.3445\n",
      "Returns: 12.3904, advantages: 4.1167, actions: -0.0319, actions std: 0.4071\n",
      "Step: 4526079, last rewards: 92.33, max reward: 95.03\n",
      "Loss: 3.5792, policy loss: 0.1085, critic_loss: 6.9482, entropy: 0.3419\n",
      "Returns: 12.5607, advantages: 4.1969, actions: -0.0665, actions std: 0.4155\n",
      "Step: 4546559, last rewards: 92.91, max reward: 94.18\n",
      "Loss: 3.6007, policy loss: 0.0144, critic_loss: 7.1795, entropy: 0.3390\n",
      "Returns: 13.5387, advantages: 5.1312, actions: 0.1091, actions std: 0.4257\n",
      "Step: 4567039, last rewards: 92.54, max reward: 94.72\n",
      "Loss: 3.5055, policy loss: 0.0393, critic_loss: 6.9393, entropy: 0.3362\n",
      "Returns: 13.5113, advantages: 5.1124, actions: 0.1486, actions std: 0.4180\n",
      "Step: 4587519, last rewards: 87.63, max reward: 94.38\n",
      "Loss: 3.5672, policy loss: -0.0540, critic_loss: 7.2491, entropy: 0.3349\n",
      "Returns: 12.0741, advantages: 3.6957, actions: 0.4317, actions std: 0.3920\n",
      "Step: 4607999, last rewards: 90.81, max reward: 92.99\n",
      "Loss: 3.5122, policy loss: -0.0391, critic_loss: 7.1095, entropy: 0.3357\n",
      "Returns: 13.0437, advantages: 4.7604, actions: 0.3168, actions std: 0.4083\n",
      "Step: 4628479, last rewards: 92.54, max reward: 93.78\n",
      "Loss: 3.3941, policy loss: 0.0424, critic_loss: 6.7101, entropy: 0.3366\n",
      "Returns: 12.0292, advantages: 3.7849, actions: 0.1914, actions std: 0.3980\n",
      "Step: 4648959, last rewards: 93.00, max reward: 95.54\n",
      "Loss: 3.5967, policy loss: 0.0694, critic_loss: 7.0612, entropy: 0.3358\n",
      "Returns: 12.2687, advantages: 4.0371, actions: -0.0139, actions std: 0.4070\n",
      "Step: 4669439, last rewards: 92.49, max reward: 94.24\n",
      "Loss: 2.4683, policy loss: 0.0516, critic_loss: 4.8401, entropy: 0.3364\n",
      "Returns: 12.6748, advantages: 4.3864, actions: 0.2273, actions std: 0.4051\n",
      "Step: 4689919, last rewards: 93.57, max reward: 95.30\n",
      "Loss: 3.5433, policy loss: 0.0196, critic_loss: 7.0542, entropy: 0.3398\n",
      "Returns: 12.4655, advantages: 4.1106, actions: -0.0677, actions std: 0.4162\n",
      "Step: 4710399, last rewards: 93.27, max reward: 95.31\n",
      "Loss: 3.2262, policy loss: 0.0011, critic_loss: 6.4570, entropy: 0.3365\n",
      "Returns: 12.6562, advantages: 4.1813, actions: -0.0903, actions std: 0.4197\n",
      "Step: 4730879, last rewards: 91.03, max reward: 93.04\n",
      "Loss: 3.6559, policy loss: -0.0382, critic_loss: 7.3949, entropy: 0.3347\n",
      "Returns: 12.4129, advantages: 3.9053, actions: 0.2256, actions std: 0.4014\n",
      "Step: 4751359, last rewards: 93.35, max reward: 95.81\n",
      "Loss: 3.3446, policy loss: 0.0137, critic_loss: 6.6686, entropy: 0.3333\n",
      "Returns: 12.4296, advantages: 3.9497, actions: 0.0903, actions std: 0.3927\n",
      "Step: 4771839, last rewards: 93.65, max reward: 95.68\n",
      "Loss: 3.5502, policy loss: -0.0210, critic_loss: 7.1492, entropy: 0.3349\n",
      "Returns: 13.2164, advantages: 4.7401, actions: 0.0716, actions std: 0.4013\n",
      "Step: 4792319, last rewards: 93.88, max reward: 95.20\n",
      "Loss: 0.3872, policy loss: -0.0412, critic_loss: 0.8636, entropy: 0.3353\n",
      "Returns: 11.4396, advantages: 2.9359, actions: 0.0192, actions std: 0.3912\n",
      "Step: 4812799, last rewards: 93.29, max reward: 95.26\n",
      "Loss: 0.3638, policy loss: -0.0538, critic_loss: 0.8419, entropy: 0.3352\n",
      "Returns: 12.8040, advantages: 4.1895, actions: 0.0264, actions std: 0.4046\n",
      "Step: 4833279, last rewards: 92.71, max reward: 94.56\n",
      "Loss: 3.0558, policy loss: -0.0423, critic_loss: 6.2029, entropy: 0.3369\n",
      "Returns: 13.4943, advantages: 4.8452, actions: 0.2448, actions std: 0.4003\n",
      "Step: 4853759, last rewards: 93.94, max reward: 95.91\n",
      "Loss: 3.7090, policy loss: 0.0215, critic_loss: 7.3817, entropy: 0.3363\n",
      "Returns: 13.7617, advantages: 5.1243, actions: 0.0281, actions std: 0.4088\n",
      "Step: 4874239, last rewards: 93.38, max reward: 94.89\n",
      "Loss: 3.6572, policy loss: -0.0615, critic_loss: 7.4440, entropy: 0.3352\n",
      "Returns: 14.1502, advantages: 5.3055, actions: -0.1034, actions std: 0.4185\n",
      "Step: 4894719, last rewards: 92.59, max reward: 94.41\n",
      "Loss: 3.2077, policy loss: -0.0027, critic_loss: 6.4276, entropy: 0.3367\n",
      "Returns: 14.8036, advantages: 5.8304, actions: 0.2564, actions std: 0.4023\n",
      "Step: 4915199, last rewards: 85.42, max reward: 91.28\n",
      "Loss: 5.8242, policy loss: -0.0246, critic_loss: 11.7043, entropy: 0.3379\n",
      "Returns: 13.6531, advantages: 4.7342, actions: 0.5618, actions std: 0.3902\n",
      "Step: 4935679, last rewards: 91.25, max reward: 93.29\n",
      "Loss: 3.3648, policy loss: 0.0255, critic_loss: 6.6855, entropy: 0.3405\n",
      "Returns: 12.3602, advantages: 3.6460, actions: 0.2709, actions std: 0.3946\n",
      "Step: 4956159, last rewards: 93.43, max reward: 95.02\n",
      "Loss: 3.3661, policy loss: 0.0600, critic_loss: 6.6191, entropy: 0.3404\n",
      "Returns: 12.9992, advantages: 4.3459, actions: -0.0060, actions std: 0.4180\n",
      "Step: 4976639, last rewards: 91.28, max reward: 93.14\n",
      "Loss: 0.1246, policy loss: -0.0606, critic_loss: 0.3773, entropy: 0.3407\n",
      "Returns: 13.0539, advantages: 4.3417, actions: -0.1862, actions std: 0.4397\n",
      "Step: 4997119, last rewards: 60.53, max reward: 91.68\n",
      "Loss: 3.5641, policy loss: -0.0417, critic_loss: 7.2183, entropy: 0.3404\n",
      "Returns: 11.6168, advantages: 2.7297, actions: -0.2522, actions std: 0.4626\n",
      "Step: 5017599, last rewards: 89.61, max reward: 93.74\n",
      "Loss: 0.1768, policy loss: -0.0531, critic_loss: 0.4668, entropy: 0.3405\n",
      "Returns: 12.3072, advantages: 3.3507, actions: 0.4666, actions std: 0.3803\n",
      "Step: 5038079, last rewards: 88.71, max reward: 91.82\n",
      "Loss: 3.2388, policy loss: 0.0511, critic_loss: 6.3822, entropy: 0.3394\n",
      "Returns: 13.1624, advantages: 4.3678, actions: 0.4286, actions std: 0.3966\n",
      "Step: 5058559, last rewards: 90.47, max reward: 93.12\n",
      "Loss: 0.4518, policy loss: 0.0842, critic_loss: 0.7420, entropy: 0.3373\n",
      "Returns: 13.1738, advantages: 4.6297, actions: 0.3268, actions std: 0.3986\n",
      "Step: 5079039, last rewards: 92.64, max reward: 95.20\n",
      "Loss: 0.1097, policy loss: -0.0642, critic_loss: 0.3545, entropy: 0.3353\n",
      "Returns: 12.5433, advantages: 4.0696, actions: 0.2127, actions std: 0.4162\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m     network_output = network(tensor_state)\n\u001b[32m     11\u001b[39m actor_mean, critic_value, dist = network_output\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m action = \u001b[43mdist\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m next_state, reward, terminated, truncated, _ = env.step(action.numpy())\n\u001b[32m     14\u001b[39m total_reward += \u001b[38;5;28mfloat\u001b[39m(reward)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/distributions/normal.py:80\u001b[39m, in \u001b[36mNormal.sample\u001b[39m\u001b[34m(self, sample_shape)\u001b[39m\n\u001b[32m     78\u001b[39m shape = \u001b[38;5;28mself\u001b[39m._extended_shape(sample_shape)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "buffer = []\n",
    "state, _ = env.reset()\n",
    "network.train()\n",
    "total_reward = 0\n",
    "total_reward_list = []\n",
    "\n",
    "for i in range(int(1e7)):\n",
    "    tensor_state = T.tensor(state).float()\n",
    "    with T.no_grad():\n",
    "        network_output = network(tensor_state)\n",
    "    actor_mean, critic_value, dist = network_output\n",
    "    action = dist.sample()\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action.numpy())\n",
    "    total_reward += float(reward)\n",
    "    done = T.tensor(terminated or truncated).float()\n",
    "    \n",
    "    velocity = next_state[-1]\n",
    "    buffer.append(\n",
    "        (\n",
    "            tensor_state,\n",
    "            action,\n",
    "            T.tensor(reward + 100 * (velocity ** 2) + 5 * abs(velocity)),\n",
    "            T.tensor(next_state),\n",
    "            done,\n",
    "            critic_value.detach()\n",
    "        )\n",
    "    )\n",
    "    if terminated or truncated:\n",
    "        state, _ = env.reset()\n",
    "        total_reward_list.append(total_reward)\n",
    "        total_reward = 0\n",
    "    else:\n",
    "        state = next_state\n",
    "    if len(buffer) == train_step:\n",
    "        states, actions, rewards, next_states, dones, critic_values = (\n",
    "            [T.stack(column, dim=0) for column in zip(*buffer)]\n",
    "        )\n",
    "        buffer.clear()\n",
    "        \n",
    "        # calculate returns and advantages\n",
    "        returns, advantages = [], []\n",
    "        g = T.tensor(0)\n",
    "        for k in reversed(range(train_step-1)):\n",
    "            td_error = rewards[k] + gamma_ * critic_values[k + 1, 0] * (1 - dones[k]) - critic_values[k, 0]\n",
    "            g = td_error + gamma_ * lambda_ * g * (1 - dones[k])\n",
    "            \n",
    "            returns.insert(0, g + critic_values[k, 0])\n",
    "            advantages.insert(0, g)\n",
    "        \n",
    "        returns = T.stack(returns, dim=0)\n",
    "        advantages = T.stack(advantages, dim=0)\n",
    "        \n",
    "        for batch_idx in range(0, train_step, 256):\n",
    "            batch_range = slice(batch_idx,min(batch_idx + 256, train_step-1))\n",
    "\n",
    "            batch_advantages = advantages[batch_range]\n",
    "            batch_states = states[batch_range]\n",
    "            batch_actions = actions[batch_range]\n",
    "            batch_returns = returns[batch_range]\n",
    "            \n",
    "            batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-8)\n",
    "            \n",
    "            # calculate losses\n",
    "            optimizer.zero_grad()\n",
    "            _, batch_critic_value, batch_dist = network(batch_states)\n",
    "            \n",
    "            log_prob = batch_dist.log_prob(batch_actions)\n",
    "            policy_loss = -(log_prob.sum(-1) * batch_advantages.detach()).mean()\n",
    "                \n",
    "            critic_loss = loss_fn(batch_critic_value.squeeze(-1), batch_returns)\n",
    "            \n",
    "            entropy = batch_dist.entropy().mean()\n",
    "            \n",
    "            loss = policy_loss + 0.5 * critic_loss - 0.01 * entropy\n",
    "            loss.backward()\n",
    "            T.nn.utils.clip_grad_norm_(network.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "        \n",
    "        if (i + 1) % (2048 * 10) == 0:\n",
    "            print(f\"Step: {i}, last rewards: {sum(total_reward_list[-10:])/len(total_reward_list[-10:]):.2f}, max reward: {max(total_reward_list[-10:]):.2f}\")\n",
    "            print(f\"Loss: {loss.item():.4f}, policy loss: {policy_loss.item():.4f}, critic_loss: {critic_loss.item():.4f}, entropy: {entropy.item():.4f}\")\n",
    "            print(f\"Returns: {returns.mean().item():.4f}, advantages: {advantages.mean().item():.4f}, actions: {actions.mean().item():.4f}, actions std: {actions.std().item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
