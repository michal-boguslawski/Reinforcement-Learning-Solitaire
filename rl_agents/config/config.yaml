experiment_name: MountainCarContinuous-PPO

env_kwargs:
  id: MountainCarContinuous-v0
  vectorization_mode: async
  num_envs: 8
  training_wrappers:
    power_obs_reward:
      pow_factors: [0, 10]
      abs_factors: [0, 0.1]
      decay_factor: 0.5

policy:
  type: ppo
  kwargs:
    advantage_normalize: batch
    gamma: 0.995
    lambda_: 0.975
    value_loss_coef: 0.5
    entropy_coef: 0.001
    entropy_decay: 0.99
    clip_epsilon: 0.2
    num_epochs: 5
    optimizer_kwargs:
      actor_lr: 3e-4
      critic_lr: 1e-3
    exploration_method:
      name: distribution

worker_kwargs:
  device: auto
  record_step: 20000

train_kwargs:
  num_steps: 100000
  batch_size: 1024
  minibatch_size: 128

network:
  kwargs:
    backbone_kwargs: {}
    backbone_name: mlp
    distribution: normal
    head_kwargs: {}
    head_name: actor_critic
    num_features: 16
    initial_deviation: 3.
