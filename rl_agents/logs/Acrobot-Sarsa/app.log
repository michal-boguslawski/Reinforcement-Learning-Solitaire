2026-01-17 11:00:16,993 : worker.worker : INFO : ==================== Start training ====================
2026-01-17 11:00:17,448 : worker.worker : INFO : Step 0, Avg Reward 0.0000, Max Reward 0.0000, Loss [0.0]
2026-01-17 11:00:17,506 : network.model : INFO : Saved model to logs/Acrobot-Sarsa/model.pt
2026-01-17 11:01:06,680 : agent.on_policy : DEBUG : Mean Losses: [6.27946001291275]
2026-01-17 11:01:12,326 : agent.on_policy : DEBUG : Mean Losses: [6.146900713443756]
2026-01-17 11:01:15,626 : agent.on_policy : DEBUG : Mean Losses: [6.067728877067566]
2026-01-17 11:01:20,679 : agent.on_policy : DEBUG : Mean Losses: [5.943699479103088]
2026-01-17 11:01:25,556 : agent.on_policy : DEBUG : Mean Losses: [5.8435018658638]
2026-01-17 11:01:28,495 : agent.on_policy : DEBUG : Mean Losses: [5.702747941017151]
2026-01-17 11:01:32,748 : agent.on_policy : DEBUG : Mean Losses: [5.626258373260498]
2026-01-17 11:01:36,649 : agent.on_policy : DEBUG : Mean Losses: [5.585469603538513]
2026-01-17 11:01:40,174 : agent.on_policy : DEBUG : Mean Losses: [5.58265882730484]
2026-01-17 11:01:45,041 : agent.on_policy : DEBUG : Mean Losses: [5.530300199985504]
2026-01-17 11:01:48,262 : agent.on_policy : DEBUG : Mean Losses: [5.554047167301178]
2026-01-17 11:01:52,173 : agent.on_policy : DEBUG : Mean Losses: [5.5589523911476135]
2026-01-17 11:01:55,774 : agent.on_policy : DEBUG : Mean Losses: [5.600059509277344]
2026-01-17 11:01:56,388 : worker.worker : DEBUG : Step 434, finished rewards -269.47, envs finished 1
2026-01-17 11:01:58,542 : agent.on_policy : DEBUG : Mean Losses: [8.069106340408325]
2026-01-17 11:02:01,423 : agent.on_policy : DEBUG : Mean Losses: [5.667397201061249]
2026-01-17 11:02:01,936 : worker.worker : DEBUG : Step 499, finished rewards -475.76, envs finished 7
2026-01-17 11:02:04,559 : agent.on_policy : DEBUG : Mean Losses: [3.8873719573020935]
2026-01-17 11:02:07,334 : agent.on_policy : DEBUG : Mean Losses: [5.902258098125458]
2026-01-17 11:02:09,945 : agent.on_policy : DEBUG : Mean Losses: [5.760181486606598]
2026-01-17 11:02:12,833 : agent.on_policy : DEBUG : Mean Losses: [5.686067044734955]
2026-01-17 11:02:17,656 : agent.on_policy : DEBUG : Mean Losses: [5.573170721530914]
2026-01-17 11:02:21,466 : agent.on_policy : DEBUG : Mean Losses: [5.544239282608032]
2026-01-17 11:02:25,126 : agent.on_policy : DEBUG : Mean Losses: [5.475875496864319]
2026-01-17 11:02:28,885 : agent.on_policy : DEBUG : Mean Losses: [5.393164873123169]
2026-01-17 11:02:33,355 : agent.on_policy : DEBUG : Mean Losses: [5.336373686790466]
2026-01-17 11:02:37,374 : agent.on_policy : DEBUG : Mean Losses: [5.388712286949158]
2026-01-17 11:02:40,107 : agent.on_policy : DEBUG : Mean Losses: [5.27778285741806]
2026-01-17 11:02:44,153 : agent.on_policy : DEBUG : Mean Losses: [5.2787569761276245]
2026-01-17 11:02:47,518 : agent.on_policy : DEBUG : Mean Losses: [5.162882566452026]
2026-01-17 11:02:50,507 : agent.on_policy : DEBUG : Mean Losses: [5.08750319480896]
2026-01-17 11:02:50,768 : worker.worker : DEBUG : Step 935, finished rewards -456.98, envs finished 1
2026-01-17 11:02:53,839 : agent.on_policy : DEBUG : Mean Losses: [5.066850364208221]
2026-01-17 11:02:57,830 : agent.on_policy : DEBUG : Mean Losses: [5.182566583156586]
2026-01-17 11:02:57,994 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.95
2026-01-17 11:02:58,054 : worker.worker : DEBUG : Step 1000, finished rewards -468.74, envs finished 7
2026-01-17 11:03:00,397 : agent.on_policy : DEBUG : Mean Losses: [4.515769392251968]
2026-01-17 11:03:02,913 : agent.on_policy : DEBUG : Mean Losses: [5.678327977657318]
2026-01-17 11:03:06,704 : agent.on_policy : DEBUG : Mean Losses: [5.63238251209259]
2026-01-17 11:03:10,189 : agent.on_policy : DEBUG : Mean Losses: [5.491288006305695]
2026-01-17 11:03:12,773 : agent.on_policy : DEBUG : Mean Losses: [5.388561964035034]
2026-01-17 11:03:17,772 : agent.on_policy : DEBUG : Mean Losses: [5.290326356887817]
2026-01-17 11:03:20,310 : agent.on_policy : DEBUG : Mean Losses: [5.39330530166626]
2026-01-17 11:03:23,724 : agent.on_policy : DEBUG : Mean Losses: [5.269860923290253]
2026-01-17 11:03:26,748 : agent.on_policy : DEBUG : Mean Losses: [5.372011244297028]
2026-01-17 11:03:29,119 : agent.on_policy : DEBUG : Mean Losses: [5.387678682804108]
2026-01-17 11:03:32,143 : agent.on_policy : DEBUG : Mean Losses: [5.113518714904785]
2026-01-17 11:03:36,021 : agent.on_policy : DEBUG : Mean Losses: [5.174144387245178]
2026-01-17 11:03:39,623 : agent.on_policy : DEBUG : Mean Losses: [5.062434554100037]
2026-01-17 11:03:40,234 : worker.worker : DEBUG : Step 1436, finished rewards -479.25, envs finished 1
2026-01-17 11:03:42,923 : agent.on_policy : DEBUG : Mean Losses: [4.997718632221222]
2026-01-17 11:03:46,389 : agent.on_policy : DEBUG : Mean Losses: [5.221953451633453]
2026-01-17 11:03:48,389 : worker.worker : DEBUG : Step 1501, finished rewards -475.75, envs finished 7
2026-01-17 11:03:51,242 : agent.on_policy : DEBUG : Mean Losses: [4.122536838054657]
2026-01-17 11:03:54,302 : agent.on_policy : DEBUG : Mean Losses: [5.513436913490295]
2026-01-17 11:03:57,029 : agent.on_policy : DEBUG : Mean Losses: [5.47318309545517]
2026-01-17 11:03:59,436 : agent.on_policy : DEBUG : Mean Losses: [5.3781797885894775]
2026-01-17 11:04:03,141 : agent.on_policy : DEBUG : Mean Losses: [5.390920639038086]
2026-01-17 11:04:05,796 : agent.on_policy : DEBUG : Mean Losses: [5.195545256137848]
2026-01-17 11:04:08,397 : agent.on_policy : DEBUG : Mean Losses: [5.173674404621124]
2026-01-17 11:04:10,861 : agent.on_policy : DEBUG : Mean Losses: [5.033805429935455]
2026-01-17 11:04:13,954 : agent.on_policy : DEBUG : Mean Losses: [4.9719038009643555]
2026-01-17 11:04:16,454 : agent.on_policy : DEBUG : Mean Losses: [4.909976363182068]
2026-01-17 11:04:21,560 : agent.on_policy : DEBUG : Mean Losses: [4.957631230354309]
2026-01-17 11:04:24,961 : agent.on_policy : DEBUG : Mean Losses: [4.991835951805115]
2026-01-17 11:04:27,948 : agent.on_policy : DEBUG : Mean Losses: [4.985207259654999]
2026-01-17 11:04:30,883 : agent.on_policy : DEBUG : Mean Losses: [4.959863722324371]
2026-01-17 11:04:31,319 : worker.worker : DEBUG : Step 1937, finished rewards -480.77, envs finished 1
2026-01-17 11:04:34,128 : agent.on_policy : DEBUG : Mean Losses: [4.699884921312332]
2026-01-17 11:04:38,137 : agent.on_policy : DEBUG : Mean Losses: [4.998636782169342]
2026-01-17 11:04:38,636 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.9025
2026-01-17 11:04:38,795 : worker.worker : DEBUG : Step 2002, finished rewards -468.31, envs finished 7
2026-01-17 11:04:41,615 : agent.on_policy : DEBUG : Mean Losses: [3.681750863790512]
2026-01-17 11:04:45,154 : agent.on_policy : DEBUG : Mean Losses: [5.3127992153167725]
2026-01-17 11:04:47,932 : agent.on_policy : DEBUG : Mean Losses: [5.331692636013031]
2026-01-17 11:04:51,221 : agent.on_policy : DEBUG : Mean Losses: [5.215575575828552]
2026-01-17 11:04:54,660 : agent.on_policy : DEBUG : Mean Losses: [5.086906731128693]
2026-01-17 11:04:57,797 : agent.on_policy : DEBUG : Mean Losses: [4.908267319202423]
2026-01-17 11:05:01,490 : agent.on_policy : DEBUG : Mean Losses: [4.747554779052734]
2026-01-17 11:05:05,253 : agent.on_policy : DEBUG : Mean Losses: [4.740488767623901]
2026-01-17 11:05:08,458 : agent.on_policy : DEBUG : Mean Losses: [4.670612812042236]
2026-01-17 11:05:11,193 : agent.on_policy : DEBUG : Mean Losses: [4.621989339590073]
2026-01-17 11:05:11,730 : worker.worker : DEBUG : Step 2320, finished rewards -176.94, envs finished 1
2026-01-17 11:05:14,461 : agent.on_policy : DEBUG : Mean Losses: [7.638004004955292]
2026-01-17 11:05:17,281 : agent.on_policy : DEBUG : Mean Losses: [4.963327586650848]
2026-01-17 11:05:20,135 : agent.on_policy : DEBUG : Mean Losses: [4.934036195278168]
2026-01-17 11:05:24,453 : agent.on_policy : DEBUG : Mean Losses: [4.904668211936951]
2026-01-17 11:05:24,691 : worker.worker : DEBUG : Step 2438, finished rewards -494.15, envs finished 1
2026-01-17 11:05:28,849 : agent.on_policy : DEBUG : Mean Losses: [4.949810028076172]
2026-01-17 11:05:31,855 : agent.on_policy : DEBUG : Mean Losses: [4.613774210214615]
2026-01-17 11:05:32,171 : worker.worker : DEBUG : Step 2503, finished rewards -466.15, envs finished 6
2026-01-17 11:05:34,550 : agent.on_policy : DEBUG : Mean Losses: [5.230899691581726]
2026-01-17 11:05:37,834 : agent.on_policy : DEBUG : Mean Losses: [4.981426477432251]
2026-01-17 11:05:41,166 : agent.on_policy : DEBUG : Mean Losses: [4.983415722846985]
2026-01-17 11:05:44,423 : agent.on_policy : DEBUG : Mean Losses: [4.837475657463074]
2026-01-17 11:05:47,296 : agent.on_policy : DEBUG : Mean Losses: [4.862118601799011]
2026-01-17 11:05:49,752 : agent.on_policy : DEBUG : Mean Losses: [4.887941360473633]
2026-01-17 11:05:54,339 : agent.on_policy : DEBUG : Mean Losses: [4.727419495582581]
2026-01-17 11:05:57,623 : agent.on_policy : DEBUG : Mean Losses: [4.712941825389862]
2026-01-17 11:06:00,738 : agent.on_policy : DEBUG : Mean Losses: [4.5838775634765625]
2026-01-17 11:06:03,351 : agent.on_policy : DEBUG : Mean Losses: [4.616801500320435]
2026-01-17 11:06:03,492 : worker.worker : DEBUG : Step 2821, finished rewards -479.39, envs finished 1
2026-01-17 11:06:06,523 : agent.on_policy : DEBUG : Mean Losses: [4.7516977190971375]
2026-01-17 11:06:09,413 : agent.on_policy : DEBUG : Mean Losses: [4.55047744512558]
2026-01-17 11:06:13,254 : agent.on_policy : DEBUG : Mean Losses: [4.563622415065765]
2026-01-17 11:06:14,910 : worker.worker : DEBUG : Step 2939, finished rewards -433.39, envs finished 1
2026-01-17 11:06:17,296 : agent.on_policy : DEBUG : Mean Losses: [4.4042187333106995]
2026-01-17 11:06:21,060 : agent.on_policy : DEBUG : Mean Losses: [4.710744857788086]
2026-01-17 11:06:21,984 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.8573749999999999
2026-01-17 11:06:22,269 : worker.worker : DEBUG : Step 3004, finished rewards -467.76, envs finished 6
2026-01-17 11:06:25,844 : agent.on_policy : DEBUG : Mean Losses: [4.379381835460663]
2026-01-17 11:06:28,756 : agent.on_policy : DEBUG : Mean Losses: [5.0400367975234985]
2026-01-17 11:06:31,223 : agent.on_policy : DEBUG : Mean Losses: [5.009601831436157]
2026-01-17 11:06:34,769 : agent.on_policy : DEBUG : Mean Losses: [4.938263654708862]
2026-01-17 11:06:37,571 : agent.on_policy : DEBUG : Mean Losses: [4.914433538913727]
2026-01-17 11:06:41,515 : agent.on_policy : DEBUG : Mean Losses: [4.842118501663208]
2026-01-17 11:06:45,035 : agent.on_policy : DEBUG : Mean Losses: [4.734896421432495]
2026-01-17 11:06:48,612 : agent.on_policy : DEBUG : Mean Losses: [4.689749777317047]
2026-01-17 11:06:51,484 : agent.on_policy : DEBUG : Mean Losses: [4.6430699825286865]
2026-01-17 11:06:55,822 : agent.on_policy : DEBUG : Mean Losses: [4.588696777820587]
2026-01-17 11:06:56,606 : worker.worker : DEBUG : Step 3322, finished rewards -478.30, envs finished 1
2026-01-17 11:06:59,577 : agent.on_policy : DEBUG : Mean Losses: [4.51116144657135]
2026-01-17 11:07:03,866 : agent.on_policy : DEBUG : Mean Losses: [4.5955893993377686]
2026-01-17 11:07:07,209 : agent.on_policy : DEBUG : Mean Losses: [4.4537040293216705]
2026-01-17 11:07:10,599 : agent.on_policy : DEBUG : Mean Losses: [4.478423774242401]
2026-01-17 11:07:10,953 : worker.worker : DEBUG : Step 3440, finished rewards -476.94, envs finished 1
2026-01-17 11:07:13,573 : agent.on_policy : DEBUG : Mean Losses: [4.50590181350708]
2026-01-17 11:07:16,757 : agent.on_policy : DEBUG : Mean Losses: [4.576367527246475]
2026-01-17 11:07:17,554 : worker.worker : DEBUG : Step 3505, finished rewards -475.48, envs finished 6
2026-01-17 11:07:20,679 : agent.on_policy : DEBUG : Mean Losses: [4.555604785680771]
2026-01-17 11:07:24,277 : agent.on_policy : DEBUG : Mean Losses: [4.861088871955872]
2026-01-17 11:07:28,569 : agent.on_policy : DEBUG : Mean Losses: [4.776785373687744]
2026-01-17 11:07:31,370 : agent.on_policy : DEBUG : Mean Losses: [4.6984352469444275]
2026-01-17 11:07:34,115 : agent.on_policy : DEBUG : Mean Losses: [4.627125918865204]
2026-01-17 11:07:36,390 : agent.on_policy : DEBUG : Mean Losses: [4.51334473490715]
2026-01-17 11:07:39,501 : agent.on_policy : DEBUG : Mean Losses: [4.45182129740715]
2026-01-17 11:07:42,777 : agent.on_policy : DEBUG : Mean Losses: [4.352925926446915]
2026-01-17 11:07:46,805 : agent.on_policy : DEBUG : Mean Losses: [4.336989313364029]
2026-01-17 11:07:50,672 : agent.on_policy : DEBUG : Mean Losses: [4.2723149955272675]
2026-01-17 11:07:51,426 : worker.worker : DEBUG : Step 3823, finished rewards -486.12, envs finished 1
2026-01-17 11:07:54,881 : agent.on_policy : DEBUG : Mean Losses: [4.341110169887543]
2026-01-17 11:08:00,276 : agent.on_policy : DEBUG : Mean Losses: [4.2086513340473175]
2026-01-17 11:08:04,220 : agent.on_policy : DEBUG : Mean Losses: [3.9952710568904877]
2026-01-17 11:08:04,469 : worker.worker : DEBUG : Step 3912, finished rewards -271.35, envs finished 1
2026-01-17 11:08:08,932 : agent.on_policy : DEBUG : Mean Losses: [6.618588596582413]
2026-01-17 11:08:09,078 : worker.worker : DEBUG : Step 3941, finished rewards -463.31, envs finished 1
2026-01-17 11:08:13,524 : agent.on_policy : DEBUG : Mean Losses: [4.437600255012512]
2026-01-17 11:08:14,711 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.8145062499999999
2026-01-17 11:08:17,451 : agent.on_policy : DEBUG : Mean Losses: [4.251808941364288]
2026-01-17 11:08:17,733 : worker.worker : DEBUG : Step 4006, finished rewards -465.01, envs finished 5
2026-01-17 11:08:21,949 : agent.on_policy : DEBUG : Mean Losses: [5.54634016752243]
2026-01-17 11:08:26,405 : agent.on_policy : DEBUG : Mean Losses: [4.578724801540375]
2026-01-17 11:08:31,825 : agent.on_policy : DEBUG : Mean Losses: [4.486940681934357]
2026-01-17 11:08:36,144 : agent.on_policy : DEBUG : Mean Losses: [4.379724323749542]
2026-01-17 11:08:40,308 : agent.on_policy : DEBUG : Mean Losses: [4.3662265837192535]
2026-01-17 11:08:43,992 : agent.on_policy : DEBUG : Mean Losses: [4.362042486667633]
2026-01-17 11:08:47,415 : agent.on_policy : DEBUG : Mean Losses: [4.344243347644806]
2026-01-17 11:08:50,318 : agent.on_policy : DEBUG : Mean Losses: [4.239306777715683]
2026-01-17 11:08:54,107 : agent.on_policy : DEBUG : Mean Losses: [4.276360958814621]
2026-01-17 11:08:58,347 : agent.on_policy : DEBUG : Mean Losses: [4.177526831626892]
2026-01-17 11:08:58,612 : worker.worker : DEBUG : Step 4324, finished rewards -449.51, envs finished 1
2026-01-17 11:09:05,614 : agent.on_policy : DEBUG : Mean Losses: [4.416906297206879]
2026-01-17 11:09:09,155 : agent.on_policy : DEBUG : Mean Losses: [4.202105164527893]
2026-01-17 11:09:10,525 : worker.worker : DEBUG : Step 4413, finished rewards -467.55, envs finished 1
2026-01-17 11:09:13,253 : agent.on_policy : DEBUG : Mean Losses: [4.2404493391513824]
2026-01-17 11:09:14,103 : worker.worker : DEBUG : Step 4442, finished rewards -470.32, envs finished 1
2026-01-17 11:09:18,577 : agent.on_policy : DEBUG : Mean Losses: [4.218600779771805]
2026-01-17 11:09:23,007 : agent.on_policy : DEBUG : Mean Losses: [4.129525661468506]
2026-01-17 11:09:24,032 : worker.worker : DEBUG : Step 4500, finished rewards -344.54, envs finished 1
2026-01-17 11:09:24,345 : worker.worker : DEBUG : Step 4507, finished rewards -478.71, envs finished 4
2026-01-17 11:09:27,574 : agent.on_policy : DEBUG : Mean Losses: [8.219197988510132]
2026-01-17 11:09:32,891 : agent.on_policy : DEBUG : Mean Losses: [4.568966209888458]
2026-01-17 11:09:36,611 : agent.on_policy : DEBUG : Mean Losses: [4.512284934520721]
2026-01-17 11:09:41,602 : agent.on_policy : DEBUG : Mean Losses: [4.417732775211334]
2026-01-17 11:09:46,490 : agent.on_policy : DEBUG : Mean Losses: [4.375380635261536]
2026-01-17 11:09:49,623 : agent.on_policy : DEBUG : Mean Losses: [4.3580136597156525]
2026-01-17 11:09:54,468 : agent.on_policy : DEBUG : Mean Losses: [4.34111088514328]
2026-01-17 11:09:58,103 : agent.on_policy : DEBUG : Mean Losses: [4.275099694728851]
2026-01-17 11:10:04,351 : agent.on_policy : DEBUG : Mean Losses: [4.237641483545303]
2026-01-17 11:10:09,818 : agent.on_policy : DEBUG : Mean Losses: [4.123162537813187]
2026-01-17 11:10:11,053 : worker.worker : DEBUG : Step 4825, finished rewards -494.36, envs finished 1
2026-01-17 11:10:15,206 : agent.on_policy : DEBUG : Mean Losses: [4.337523192167282]
2026-01-17 11:10:19,835 : agent.on_policy : DEBUG : Mean Losses: [4.128020763397217]
2026-01-17 11:10:25,091 : agent.on_policy : DEBUG : Mean Losses: [4.03998988866806]
2026-01-17 11:10:25,896 : worker.worker : DEBUG : Step 4914, finished rewards -471.16, envs finished 1
2026-01-17 11:10:29,638 : agent.on_policy : DEBUG : Mean Losses: [4.173547685146332]
2026-01-17 11:10:30,443 : worker.worker : DEBUG : Step 4943, finished rewards -477.12, envs finished 1
2026-01-17 11:10:35,126 : agent.on_policy : DEBUG : Mean Losses: [4.294551283121109]
2026-01-17 11:10:39,091 : agent.on_policy : DEBUG : Mean Losses: [3.8432277739048004]
2026-01-17 11:10:39,413 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.7737809374999999
2026-01-17 11:10:39,598 : worker.worker : INFO : Step 5000, Avg Reward -443.5374, Max Reward -176.9440, Loss [4.94644377]
2026-01-17 11:10:39,654 : worker.worker : DEBUG : Step 5001, finished rewards -484.83, envs finished 1
2026-01-17 11:10:40,365 : worker.worker : DEBUG : Step 5008, finished rewards -469.72, envs finished 4
2026-01-17 11:10:44,088 : agent.on_policy : DEBUG : Mean Losses: [5.561787188053131]
2026-01-17 11:10:47,463 : agent.on_policy : DEBUG : Mean Losses: [4.3619468212127686]
2026-01-17 11:10:51,354 : agent.on_policy : DEBUG : Mean Losses: [4.355143189430237]
2026-01-17 11:10:55,699 : agent.on_policy : DEBUG : Mean Losses: [4.307409286499023]
2026-01-17 11:10:59,362 : agent.on_policy : DEBUG : Mean Losses: [4.315147906541824]
2026-01-17 11:11:02,697 : agent.on_policy : DEBUG : Mean Losses: [4.2826219499111176]
2026-01-17 11:11:08,061 : agent.on_policy : DEBUG : Mean Losses: [4.196905791759491]
2026-01-17 11:11:13,677 : agent.on_policy : DEBUG : Mean Losses: [4.157036930322647]
2026-01-17 11:11:18,239 : agent.on_policy : DEBUG : Mean Losses: [4.07498762011528]
2026-01-17 11:11:23,184 : agent.on_policy : DEBUG : Mean Losses: [4.055725812911987]
2026-01-17 11:11:23,697 : worker.worker : DEBUG : Step 5326, finished rewards -484.10, envs finished 1
2026-01-17 11:11:27,104 : agent.on_policy : DEBUG : Mean Losses: [4.362296253442764]
2026-01-17 11:11:31,226 : agent.on_policy : DEBUG : Mean Losses: [3.9701761603355408]
2026-01-17 11:11:38,347 : agent.on_policy : DEBUG : Mean Losses: [3.939515918493271]
2026-01-17 11:11:39,121 : worker.worker : DEBUG : Step 5415, finished rewards -485.31, envs finished 1
2026-01-17 11:11:43,414 : agent.on_policy : DEBUG : Mean Losses: [4.097862899303436]
2026-01-17 11:11:43,737 : worker.worker : DEBUG : Step 5444, finished rewards -464.31, envs finished 1
2026-01-17 11:11:48,772 : agent.on_policy : DEBUG : Mean Losses: [4.281883656978607]
2026-01-17 11:11:50,289 : worker.worker : DEBUG : Step 5502, finished rewards -489.53, envs finished 1
2026-01-17 11:11:53,052 : agent.on_policy : DEBUG : Mean Losses: [4.255574613809586]
2026-01-17 11:11:53,203 : worker.worker : DEBUG : Step 5509, finished rewards -480.12, envs finished 4
2026-01-17 11:11:57,670 : agent.on_policy : DEBUG : Mean Losses: [5.379694402217865]
2026-01-17 11:12:01,540 : agent.on_policy : DEBUG : Mean Losses: [4.183111131191254]
2026-01-17 11:12:05,800 : agent.on_policy : DEBUG : Mean Losses: [4.1172953844070435]
2026-01-17 11:12:12,233 : agent.on_policy : DEBUG : Mean Losses: [4.08735990524292]
2026-01-17 11:12:16,177 : agent.on_policy : DEBUG : Mean Losses: [3.9851247668266296]
2026-01-17 11:12:20,959 : agent.on_policy : DEBUG : Mean Losses: [3.68038347363472]
2026-01-17 11:12:26,155 : agent.on_policy : DEBUG : Mean Losses: [3.478292316198349]
2026-01-17 11:12:30,577 : agent.on_policy : DEBUG : Mean Losses: [3.44188529253006]
2026-01-17 11:12:36,331 : agent.on_policy : DEBUG : Mean Losses: [3.360592842102051]
2026-01-17 11:12:43,629 : agent.on_policy : DEBUG : Mean Losses: [3.2824891209602356]
2026-01-17 11:12:43,893 : worker.worker : DEBUG : Step 5827, finished rewards -480.49, envs finished 1
2026-01-17 11:12:49,592 : agent.on_policy : DEBUG : Mean Losses: [3.4062391221523285]
2026-01-17 11:12:50,128 : worker.worker : DEBUG : Step 5872, finished rewards -211.32, envs finished 1
2026-01-17 11:12:54,058 : agent.on_policy : DEBUG : Mean Losses: [7.021370857954025]
2026-01-17 11:12:56,638 : worker.worker : DEBUG : Step 5916, finished rewards -447.98, envs finished 1
2026-01-17 11:13:00,539 : agent.on_policy : DEBUG : Mean Losses: [4.0210758447647095]
2026-01-17 11:13:02,559 : worker.worker : DEBUG : Step 5945, finished rewards -476.32, envs finished 1
2026-01-17 11:13:06,797 : agent.on_policy : DEBUG : Mean Losses: [3.902408242225647]
2026-01-17 11:13:10,586 : worker.worker : DEBUG : Step 5983, finished rewards -331.22, envs finished 1
2026-01-17 11:13:13,777 : agent.on_policy : DEBUG : Mean Losses: [8.168009102344513]
2026-01-17 11:13:14,726 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.7350918906249998
2026-01-17 11:13:15,040 : worker.worker : DEBUG : Step 6003, finished rewards -441.66, envs finished 1
2026-01-17 11:13:15,264 : worker.worker : DEBUG : Step 6010, finished rewards -457.15, envs finished 2
2026-01-17 11:13:19,692 : agent.on_policy : DEBUG : Mean Losses: [4.902345061302185]
2026-01-17 11:13:25,774 : agent.on_policy : DEBUG : Mean Losses: [3.9763263165950775]
2026-01-17 11:13:32,534 : agent.on_policy : DEBUG : Mean Losses: [3.9006204903125763]
2026-01-17 11:13:41,729 : agent.on_policy : DEBUG : Mean Losses: [3.809468448162079]
2026-01-17 11:13:46,929 : agent.on_policy : DEBUG : Mean Losses: [3.7689271569252014]
2026-01-17 11:13:51,400 : agent.on_policy : DEBUG : Mean Losses: [3.761925995349884]
2026-01-17 11:13:57,299 : agent.on_policy : DEBUG : Mean Losses: [3.764996349811554]
2026-01-17 11:14:01,468 : agent.on_policy : DEBUG : Mean Losses: [3.762435704469681]
2026-01-17 11:14:06,705 : agent.on_policy : DEBUG : Mean Losses: [3.677725166082382]
2026-01-17 11:14:13,251 : agent.on_policy : DEBUG : Mean Losses: [3.731313467025757]
2026-01-17 11:14:15,153 : worker.worker : DEBUG : Step 6328, finished rewards -492.69, envs finished 1
2026-01-17 11:14:20,543 : agent.on_policy : DEBUG : Mean Losses: [3.9911787509918213]
2026-01-17 11:14:27,456 : agent.on_policy : DEBUG : Mean Losses: [3.752114474773407]
2026-01-17 11:14:27,901 : worker.worker : DEBUG : Step 6373, finished rewards -471.56, envs finished 1
2026-01-17 11:14:33,590 : agent.on_policy : DEBUG : Mean Losses: [4.1140463054180145]
2026-01-17 11:14:34,243 : worker.worker : DEBUG : Step 6417, finished rewards -474.40, envs finished 1
2026-01-17 11:14:40,181 : agent.on_policy : DEBUG : Mean Losses: [4.185650318861008]
2026-01-17 11:14:40,679 : worker.worker : DEBUG : Step 6446, finished rewards -449.22, envs finished 1
2026-01-17 11:14:46,232 : agent.on_policy : DEBUG : Mean Losses: [4.161144107580185]
2026-01-17 11:14:47,446 : worker.worker : DEBUG : Step 6484, finished rewards -452.91, envs finished 1
2026-01-17 11:14:51,677 : agent.on_policy : DEBUG : Mean Losses: [4.485559672117233]
2026-01-17 11:14:52,242 : worker.worker : DEBUG : Step 6504, finished rewards -483.28, envs finished 1
2026-01-17 11:14:52,707 : worker.worker : DEBUG : Step 6511, finished rewards -489.16, envs finished 2
2026-01-17 11:14:57,284 : agent.on_policy : DEBUG : Mean Losses: [5.303714543581009]
2026-01-17 11:15:04,537 : agent.on_policy : DEBUG : Mean Losses: [3.929076671600342]
2026-01-17 11:15:12,548 : agent.on_policy : DEBUG : Mean Losses: [3.8381773233413696]
2026-01-17 11:15:20,235 : agent.on_policy : DEBUG : Mean Losses: [3.812759280204773]
2026-01-17 11:15:25,808 : agent.on_policy : DEBUG : Mean Losses: [3.7472453713417053]
2026-01-17 11:15:33,839 : agent.on_policy : DEBUG : Mean Losses: [3.6984405517578125]
2026-01-17 11:15:38,324 : agent.on_policy : DEBUG : Mean Losses: [3.6485424637794495]
2026-01-17 11:15:45,666 : agent.on_policy : DEBUG : Mean Losses: [3.6049997210502625]
2026-01-17 11:15:49,996 : agent.on_policy : DEBUG : Mean Losses: [3.536701649427414]
2026-01-17 11:15:54,733 : agent.on_policy : DEBUG : Mean Losses: [3.535696804523468]
2026-01-17 11:15:55,357 : worker.worker : DEBUG : Step 6829, finished rewards -491.32, envs finished 1
2026-01-17 11:15:59,299 : agent.on_policy : DEBUG : Mean Losses: [4.049041002988815]
2026-01-17 11:16:01,601 : worker.worker : DEBUG : Step 6874, finished rewards -477.72, envs finished 1
2026-01-17 11:16:05,659 : agent.on_policy : DEBUG : Mean Losses: [4.023631364107132]
2026-01-17 11:16:11,498 : agent.on_policy : DEBUG : Mean Losses: [3.637544572353363]
2026-01-17 11:16:11,775 : worker.worker : DEBUG : Step 6918, finished rewards -471.43, envs finished 1
2026-01-17 11:16:17,298 : agent.on_policy : DEBUG : Mean Losses: [4.090464115142822]
2026-01-17 11:16:17,547 : worker.worker : DEBUG : Step 6947, finished rewards -473.33, envs finished 1
2026-01-17 11:16:23,175 : agent.on_policy : DEBUG : Mean Losses: [3.937031865119934]
2026-01-17 11:16:23,372 : worker.worker : DEBUG : Step 6985, finished rewards -467.41, envs finished 1
2026-01-17 11:16:24,754 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.6983372960937497
2026-01-17 11:16:25,492 : worker.worker : DEBUG : Step 7005, finished rewards -482.59, envs finished 1
2026-01-17 11:16:29,917 : agent.on_policy : DEBUG : Mean Losses: [4.808100253343582]
2026-01-17 11:16:30,231 : worker.worker : DEBUG : Step 7012, finished rewards -489.52, envs finished 2
2026-01-17 11:16:37,995 : agent.on_policy : DEBUG : Mean Losses: [4.473953872919083]
2026-01-17 11:16:44,621 : agent.on_policy : DEBUG : Mean Losses: [3.75749534368515]
2026-01-17 11:16:50,889 : agent.on_policy : DEBUG : Mean Losses: [3.69020476937294]
2026-01-17 11:16:56,406 : agent.on_policy : DEBUG : Mean Losses: [3.6509936451911926]
2026-01-17 11:17:02,593 : agent.on_policy : DEBUG : Mean Losses: [3.6148631274700165]
2026-01-17 11:17:10,605 : agent.on_policy : DEBUG : Mean Losses: [3.5583834052085876]
2026-01-17 11:17:16,990 : agent.on_policy : DEBUG : Mean Losses: [3.493511825799942]
2026-01-17 11:17:22,533 : agent.on_policy : DEBUG : Mean Losses: [3.3723511397838593]
2026-01-17 11:17:28,485 : agent.on_policy : DEBUG : Mean Losses: [3.3529209792613983]
2026-01-17 11:17:36,235 : agent.on_policy : DEBUG : Mean Losses: [3.3075606524944305]
2026-01-17 11:17:36,431 : worker.worker : DEBUG : Step 7330, finished rewards -487.60, envs finished 1
2026-01-17 11:17:42,250 : agent.on_policy : DEBUG : Mean Losses: [3.480934500694275]
2026-01-17 11:17:43,451 : worker.worker : DEBUG : Step 7375, finished rewards -464.56, envs finished 1
2026-01-17 11:17:47,985 : agent.on_policy : DEBUG : Mean Losses: [4.043413579463959]
2026-01-17 11:17:51,069 : worker.worker : DEBUG : Step 7419, finished rewards -466.56, envs finished 1
2026-01-17 11:17:54,916 : agent.on_policy : DEBUG : Mean Losses: [3.9100744426250458]
2026-01-17 11:17:56,341 : worker.worker : DEBUG : Step 7448, finished rewards -485.05, envs finished 1
2026-01-17 11:18:02,391 : agent.on_policy : DEBUG : Mean Losses: [3.8585013449192047]
2026-01-17 11:18:04,119 : worker.worker : DEBUG : Step 7486, finished rewards -490.09, envs finished 1
2026-01-17 11:18:07,495 : agent.on_policy : DEBUG : Mean Losses: [4.166596055030823]
2026-01-17 11:18:08,824 : worker.worker : DEBUG : Step 7506, finished rewards -489.40, envs finished 1
2026-01-17 11:18:09,239 : worker.worker : DEBUG : Step 7513, finished rewards -460.79, envs finished 2
2026-01-17 11:18:14,963 : agent.on_policy : DEBUG : Mean Losses: [5.200127393007278]
2026-01-17 11:18:22,150 : agent.on_policy : DEBUG : Mean Losses: [3.4975486397743225]
2026-01-17 11:18:27,077 : agent.on_policy : DEBUG : Mean Losses: [3.3603889644145966]
2026-01-17 11:18:33,490 : agent.on_policy : DEBUG : Mean Losses: [3.4046016335487366]
2026-01-17 11:18:38,403 : agent.on_policy : DEBUG : Mean Losses: [3.4002133309841156]
2026-01-17 11:18:46,202 : agent.on_policy : DEBUG : Mean Losses: [3.45704385638237]
2026-01-17 11:18:53,403 : agent.on_policy : DEBUG : Mean Losses: [3.4421686232089996]
2026-01-17 11:18:58,347 : agent.on_policy : DEBUG : Mean Losses: [3.4586951434612274]
2026-01-17 11:19:04,714 : agent.on_policy : DEBUG : Mean Losses: [3.409001797437668]
2026-01-17 11:19:12,024 : agent.on_policy : DEBUG : Mean Losses: [3.3922612369060516]
2026-01-17 11:19:13,493 : worker.worker : DEBUG : Step 7831, finished rewards -491.03, envs finished 1
2026-01-17 11:19:20,017 : agent.on_policy : DEBUG : Mean Losses: [3.918906033039093]
2026-01-17 11:19:21,329 : worker.worker : DEBUG : Step 7868, finished rewards -266.96, envs finished 1
2026-01-17 11:19:26,542 : agent.on_policy : DEBUG : Mean Losses: [7.594373345375061]
2026-01-17 11:19:26,814 : worker.worker : DEBUG : Step 7876, finished rewards -474.88, envs finished 1
2026-01-17 11:19:31,425 : agent.on_policy : DEBUG : Mean Losses: [3.8608154952526093]
2026-01-17 11:19:32,712 : worker.worker : DEBUG : Step 7920, finished rewards -477.86, envs finished 1
2026-01-17 11:19:38,167 : agent.on_policy : DEBUG : Mean Losses: [4.11474135518074]
2026-01-17 11:19:44,402 : agent.on_policy : DEBUG : Mean Losses: [3.4423638582229614]
2026-01-17 11:19:45,695 : worker.worker : DEBUG : Step 7987, finished rewards -487.85, envs finished 1
2026-01-17 11:19:46,314 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.6634204312890623
2026-01-17 11:19:51,923 : agent.on_policy : DEBUG : Mean Losses: [4.304267227649689]
2026-01-17 11:19:52,290 : worker.worker : DEBUG : Step 8007, finished rewards -487.58, envs finished 1
2026-01-17 11:19:53,156 : worker.worker : DEBUG : Step 8014, finished rewards -487.67, envs finished 2
2026-01-17 11:19:59,259 : agent.on_policy : DEBUG : Mean Losses: [5.391717851161957]
2026-01-17 11:20:04,899 : agent.on_policy : DEBUG : Mean Losses: [3.3739565014839172]
2026-01-17 11:20:10,578 : agent.on_policy : DEBUG : Mean Losses: [3.3241515457630157]
2026-01-17 11:20:14,880 : agent.on_policy : DEBUG : Mean Losses: [3.251872330904007]
2026-01-17 11:20:22,482 : agent.on_policy : DEBUG : Mean Losses: [3.184981495141983]
2026-01-17 11:20:27,992 : agent.on_policy : DEBUG : Mean Losses: [3.2100483179092407]
2026-01-17 11:20:33,345 : agent.on_policy : DEBUG : Mean Losses: [3.30806165933609]
2026-01-17 11:20:39,683 : agent.on_policy : DEBUG : Mean Losses: [3.298476129770279]
2026-01-17 11:20:47,407 : agent.on_policy : DEBUG : Mean Losses: [3.2884678840637207]
2026-01-17 11:20:52,863 : agent.on_policy : DEBUG : Mean Losses: [3.2286595702171326]
2026-01-17 11:20:53,226 : worker.worker : DEBUG : Step 8332, finished rewards -486.52, envs finished 1
2026-01-17 11:20:59,447 : agent.on_policy : DEBUG : Mean Losses: [3.9732852578163147]
2026-01-17 11:21:00,833 : worker.worker : DEBUG : Step 8369, finished rewards -460.94, envs finished 1
2026-01-17 11:21:01,281 : worker.worker : DEBUG : Step 8377, finished rewards -489.16, envs finished 1
2026-01-17 11:21:04,740 : agent.on_policy : DEBUG : Mean Losses: [4.641263842582703]
2026-01-17 11:21:09,519 : agent.on_policy : DEBUG : Mean Losses: [3.168581187725067]
2026-01-17 11:21:10,076 : worker.worker : DEBUG : Step 8421, finished rewards -482.73, envs finished 1
2026-01-17 11:21:16,127 : agent.on_policy : DEBUG : Mean Losses: [3.678012728691101]
2026-01-17 11:21:22,340 : agent.on_policy : DEBUG : Mean Losses: [3.1580072045326233]
2026-01-17 11:21:23,147 : worker.worker : DEBUG : Step 8488, finished rewards -482.31, envs finished 1
2026-01-17 11:21:24,862 : worker.worker : DEBUG : Step 8508, finished rewards -477.86, envs finished 1
2026-01-17 11:21:29,323 : agent.on_policy : DEBUG : Mean Losses: [4.6096897423267365]
2026-01-17 11:21:29,925 : worker.worker : DEBUG : Step 8515, finished rewards -477.39, envs finished 2
2026-01-17 11:21:38,497 : agent.on_policy : DEBUG : Mean Losses: [4.063195765018463]
2026-01-17 11:21:44,941 : agent.on_policy : DEBUG : Mean Losses: [3.3275753259658813]
2026-01-17 11:21:49,909 : agent.on_policy : DEBUG : Mean Losses: [3.24192675948143]
2026-01-17 11:21:57,467 : agent.on_policy : DEBUG : Mean Losses: [3.128690093755722]
2026-01-17 11:22:04,136 : agent.on_policy : DEBUG : Mean Losses: [3.0692615807056427]
2026-01-17 11:22:08,165 : agent.on_policy : DEBUG : Mean Losses: [3.0193492472171783]
2026-01-17 11:22:16,049 : agent.on_policy : DEBUG : Mean Losses: [2.960438698530197]
2026-01-17 11:22:22,055 : agent.on_policy : DEBUG : Mean Losses: [3.027303695678711]
2026-01-17 11:22:30,080 : agent.on_policy : DEBUG : Mean Losses: [2.937713086605072]
2026-01-17 11:22:38,247 : agent.on_policy : DEBUG : Mean Losses: [2.945017546415329]
2026-01-17 11:22:38,333 : worker.worker : DEBUG : Step 8833, finished rewards -489.54, envs finished 1
2026-01-17 11:22:42,588 : agent.on_policy : DEBUG : Mean Losses: [3.1338832080364227]
2026-01-17 11:22:42,736 : worker.worker : DEBUG : Step 8870, finished rewards -459.13, envs finished 1
2026-01-17 11:22:43,141 : worker.worker : DEBUG : Step 8878, finished rewards -485.71, envs finished 1
2026-01-17 11:22:46,504 : agent.on_policy : DEBUG : Mean Losses: [4.458517014980316]
2026-01-17 11:22:47,862 : worker.worker : DEBUG : Step 8922, finished rewards -469.95, envs finished 1
2026-01-17 11:22:50,680 : agent.on_policy : DEBUG : Mean Losses: [3.690558612346649]
2026-01-17 11:22:51,188 : worker.worker : DEBUG : Step 8946, finished rewards -326.44, envs finished 1
2026-01-17 11:22:55,357 : agent.on_policy : DEBUG : Mean Losses: [8.39746868610382]
2026-01-17 11:23:01,247 : agent.on_policy : DEBUG : Mean Losses: [2.986264318227768]
2026-01-17 11:23:03,208 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.6302494097246091
2026-01-17 11:23:04,260 : worker.worker : DEBUG : Step 9009, finished rewards -486.50, envs finished 1
2026-01-17 11:23:04,660 : worker.worker : DEBUG : Step 9016, finished rewards -476.66, envs finished 2
2026-01-17 11:23:10,845 : agent.on_policy : DEBUG : Mean Losses: [5.405593305826187]
2026-01-17 11:23:17,595 : agent.on_policy : DEBUG : Mean Losses: [3.040124833583832]
2026-01-17 11:23:24,449 : agent.on_policy : DEBUG : Mean Losses: [2.9898794293403625]
2026-01-17 11:23:28,566 : agent.on_policy : DEBUG : Mean Losses: [2.90502592921257]
2026-01-17 11:23:37,302 : agent.on_policy : DEBUG : Mean Losses: [2.946421831846237]
2026-01-17 11:23:42,157 : agent.on_policy : DEBUG : Mean Losses: [2.9151376485824585]
2026-01-17 11:23:50,326 : agent.on_policy : DEBUG : Mean Losses: [2.8641947507858276]
2026-01-17 11:24:00,970 : agent.on_policy : DEBUG : Mean Losses: [2.872839719057083]
2026-01-17 11:24:11,382 : agent.on_policy : DEBUG : Mean Losses: [2.7244799733161926]
2026-01-17 11:24:17,806 : agent.on_policy : DEBUG : Mean Losses: [2.632164090871811]
2026-01-17 11:24:19,896 : worker.worker : DEBUG : Step 9334, finished rewards -476.15, envs finished 1
2026-01-17 11:24:26,818 : agent.on_policy : DEBUG : Mean Losses: [3.5039208978414536]
2026-01-17 11:24:28,457 : worker.worker : DEBUG : Step 9371, finished rewards -480.37, envs finished 1
2026-01-17 11:24:33,036 : agent.on_policy : DEBUG : Mean Losses: [3.4280049204826355]
2026-01-17 11:24:33,362 : worker.worker : DEBUG : Step 9379, finished rewards -447.72, envs finished 1
2026-01-17 11:24:43,016 : agent.on_policy : DEBUG : Mean Losses: [3.1490066945552826]
2026-01-17 11:24:44,248 : worker.worker : DEBUG : Step 9423, finished rewards -462.61, envs finished 1
2026-01-17 11:24:48,978 : agent.on_policy : DEBUG : Mean Losses: [3.6779733300209045]
2026-01-17 11:24:49,424 : worker.worker : DEBUG : Step 9447, finished rewards -443.01, envs finished 1
2026-01-17 11:24:58,447 : agent.on_policy : DEBUG : Mean Losses: [3.6504805982112885]
2026-01-17 11:25:05,274 : agent.on_policy : DEBUG : Mean Losses: [2.8619354367256165]
2026-01-17 11:25:05,470 : worker.worker : DEBUG : Step 9510, finished rewards -481.20, envs finished 1
2026-01-17 11:25:06,198 : worker.worker : DEBUG : Step 9517, finished rewards -478.92, envs finished 2
2026-01-17 11:25:12,473 : agent.on_policy : DEBUG : Mean Losses: [5.561593443155289]
2026-01-17 11:25:19,918 : agent.on_policy : DEBUG : Mean Losses: [3.034514456987381]
2026-01-17 11:25:27,937 : agent.on_policy : DEBUG : Mean Losses: [3.048472672700882]
2026-01-17 11:25:34,030 : agent.on_policy : DEBUG : Mean Losses: [3.0391735434532166]
2026-01-17 11:25:45,620 : agent.on_policy : DEBUG : Mean Losses: [2.9758752584457397]
2026-01-17 11:25:55,250 : agent.on_policy : DEBUG : Mean Losses: [2.941080152988434]
2026-01-17 11:26:01,875 : agent.on_policy : DEBUG : Mean Losses: [2.8934198319911957]
2026-01-17 11:26:10,850 : agent.on_policy : DEBUG : Mean Losses: [2.8379474878311157]
2026-01-17 11:26:16,990 : agent.on_policy : DEBUG : Mean Losses: [2.734435796737671]
2026-01-17 11:26:25,894 : agent.on_policy : DEBUG : Mean Losses: [2.673394203186035]
2026-01-17 11:26:26,906 : worker.worker : DEBUG : Step 9835, finished rewards -483.78, envs finished 1
2026-01-17 11:26:34,089 : agent.on_policy : DEBUG : Mean Losses: [3.580400437116623]
2026-01-17 11:26:34,904 : worker.worker : DEBUG : Step 9872, finished rewards -484.09, envs finished 1
2026-01-17 11:26:35,444 : worker.worker : DEBUG : Step 9880, finished rewards -480.35, envs finished 1
2026-01-17 11:26:43,458 : agent.on_policy : DEBUG : Mean Losses: [4.711834579706192]
2026-01-17 11:26:49,558 : agent.on_policy : DEBUG : Mean Losses: [2.730850785970688]
2026-01-17 11:26:49,994 : worker.worker : DEBUG : Step 9924, finished rewards -481.34, envs finished 1
2026-01-17 11:26:52,393 : worker.worker : DEBUG : Step 9948, finished rewards -472.97, envs finished 1
2026-01-17 11:26:56,071 : agent.on_policy : DEBUG : Mean Losses: [4.7366301119327545]
2026-01-17 11:27:06,347 : agent.on_policy : DEBUG : Mean Losses: [2.8453666865825653]
2026-01-17 11:27:06,878 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.5987369392383786
2026-01-17 11:27:07,035 : worker.worker : INFO : Step 10000, Avg Reward -464.8798, Max Reward -211.3237, Loss [3.81035991]
2026-01-17 11:27:07,745 : worker.worker : DEBUG : Step 10011, finished rewards -476.43, envs finished 1
2026-01-17 11:27:15,197 : agent.on_policy : DEBUG : Mean Losses: [3.887259751558304]
2026-01-17 11:27:15,532 : worker.worker : DEBUG : Step 10018, finished rewards -487.07, envs finished 2
2026-01-17 11:27:20,960 : agent.on_policy : DEBUG : Mean Losses: [3.5602069199085236]
2026-01-17 11:27:25,659 : agent.on_policy : DEBUG : Mean Losses: [2.8610896468162537]
2026-01-17 11:27:36,456 : agent.on_policy : DEBUG : Mean Losses: [2.685449331998825]
2026-01-17 11:27:45,791 : agent.on_policy : DEBUG : Mean Losses: [2.6423500776290894]
2026-01-17 11:27:53,090 : agent.on_policy : DEBUG : Mean Losses: [2.6260649859905243]
2026-01-17 11:28:03,052 : agent.on_policy : DEBUG : Mean Losses: [2.61142697930336]
2026-01-17 11:28:08,291 : agent.on_policy : DEBUG : Mean Losses: [2.5896789133548737]
2026-01-17 11:28:18,939 : agent.on_policy : DEBUG : Mean Losses: [2.531807601451874]
2026-01-17 11:28:25,880 : agent.on_policy : DEBUG : Mean Losses: [2.37319216132164]
2026-01-17 11:28:35,012 : agent.on_policy : DEBUG : Mean Losses: [2.1585278809070587]
2026-01-17 11:28:35,043 : worker.worker : DEBUG : Step 10336, finished rewards -487.34, envs finished 1
2026-01-17 11:28:35,600 : worker.worker : DEBUG : Step 10338, finished rewards -328.65, envs finished 1
2026-01-17 11:28:37,995 : worker.worker : DEBUG : Step 10363, finished rewards -215.09, envs finished 1
2026-01-17 11:28:41,983 : agent.on_policy : DEBUG : Mean Losses: [8.235347479581833]
2026-01-17 11:28:43,030 : worker.worker : DEBUG : Step 10381, finished rewards -472.95, envs finished 1
2026-01-17 11:28:50,750 : agent.on_policy : DEBUG : Mean Losses: [3.6354736387729645]
2026-01-17 11:28:51,674 : worker.worker : DEBUG : Step 10425, finished rewards -486.19, envs finished 1
2026-01-17 11:28:58,205 : agent.on_policy : DEBUG : Mean Losses: [3.44630765914917]
2026-01-17 11:28:59,094 : worker.worker : DEBUG : Step 10448, finished rewards -288.82, envs finished 1
2026-01-17 11:28:59,106 : worker.worker : DEBUG : Step 10449, finished rewards -478.43, envs finished 1
2026-01-17 11:29:03,820 : agent.on_policy : DEBUG : Mean Losses: [8.377112656831741]
2026-01-17 11:29:14,597 : agent.on_policy : DEBUG : Mean Losses: [2.644022986292839]
2026-01-17 11:29:18,242 : worker.worker : DEBUG : Step 10519, finished rewards -468.64, envs finished 1
2026-01-17 11:29:21,930 : agent.on_policy : DEBUG : Mean Losses: [3.8659550547599792]
2026-01-17 11:29:27,264 : agent.on_policy : DEBUG : Mean Losses: [2.647759050130844]
2026-01-17 11:29:34,475 : agent.on_policy : DEBUG : Mean Losses: [2.6409163773059845]
2026-01-17 11:29:42,151 : agent.on_policy : DEBUG : Mean Losses: [2.606662839651108]
2026-01-17 11:29:50,571 : agent.on_policy : DEBUG : Mean Losses: [2.534265249967575]
2026-01-17 11:29:56,779 : agent.on_policy : DEBUG : Mean Losses: [2.464307814836502]
2026-01-17 11:30:04,635 : agent.on_policy : DEBUG : Mean Losses: [2.4870931208133698]
2026-01-17 11:30:12,056 : agent.on_policy : DEBUG : Mean Losses: [2.3764195442199707]
2026-01-17 11:30:21,639 : agent.on_policy : DEBUG : Mean Losses: [2.353340268135071]
2026-01-17 11:30:27,027 : agent.on_policy : DEBUG : Mean Losses: [2.457721382379532]
2026-01-17 11:30:29,438 : worker.worker : DEBUG : Step 10837, finished rewards -478.50, envs finished 1
2026-01-17 11:30:29,465 : worker.worker : DEBUG : Step 10839, finished rewards -461.10, envs finished 1
2026-01-17 11:30:34,565 : agent.on_policy : DEBUG : Mean Losses: [4.760946735739708]
2026-01-17 11:30:35,657 : worker.worker : DEBUG : Step 10864, finished rewards -483.38, envs finished 1
2026-01-17 11:30:41,430 : agent.on_policy : DEBUG : Mean Losses: [3.5884021520614624]
2026-01-17 11:30:41,485 : worker.worker : DEBUG : Step 10882, finished rewards -474.19, envs finished 1
2026-01-17 11:30:46,947 : agent.on_policy : DEBUG : Mean Losses: [2.867305874824524]
2026-01-17 11:30:47,467 : worker.worker : DEBUG : Step 10926, finished rewards -492.80, envs finished 1
2026-01-17 11:30:58,188 : agent.on_policy : DEBUG : Mean Losses: [3.569035828113556]
2026-01-17 11:30:58,485 : worker.worker : DEBUG : Step 10949, finished rewards -454.79, envs finished 1
2026-01-17 11:30:58,503 : worker.worker : DEBUG : Step 10950, finished rewards -479.19, envs finished 1
2026-01-17 11:31:06,227 : agent.on_policy : DEBUG : Mean Losses: [4.138681530952454]
2026-01-17 11:31:07,325 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.5688000922764596
2026-01-17 11:31:10,568 : agent.on_policy : DEBUG : Mean Losses: [2.611035466194153]
2026-01-17 11:31:11,146 : worker.worker : DEBUG : Step 11020, finished rewards -492.65, envs finished 1
2026-01-17 11:31:16,339 : agent.on_policy : DEBUG : Mean Losses: [3.663842350244522]
2026-01-17 11:31:28,853 : agent.on_policy : DEBUG : Mean Losses: [2.5453076362609863]
2026-01-17 11:31:35,088 : agent.on_policy : DEBUG : Mean Losses: [2.422191172838211]
2026-01-17 11:31:44,385 : agent.on_policy : DEBUG : Mean Losses: [2.4120949506759644]
2026-01-17 11:31:52,267 : agent.on_policy : DEBUG : Mean Losses: [2.4107026755809784]
2026-01-17 11:32:01,285 : agent.on_policy : DEBUG : Mean Losses: [2.4401552975177765]
2026-01-17 11:32:07,535 : agent.on_policy : DEBUG : Mean Losses: [2.303856313228607]
2026-01-17 11:32:13,605 : agent.on_policy : DEBUG : Mean Losses: [2.319848746061325]
2026-01-17 11:32:20,444 : agent.on_policy : DEBUG : Mean Losses: [2.210409700870514]
2026-01-17 11:32:31,224 : agent.on_policy : DEBUG : Mean Losses: [2.206888645887375]
2026-01-17 11:32:32,771 : worker.worker : DEBUG : Step 11338, finished rewards -457.85, envs finished 1
2026-01-17 11:32:33,120 : worker.worker : DEBUG : Step 11340, finished rewards -471.95, envs finished 1
2026-01-17 11:32:40,339 : agent.on_policy : DEBUG : Mean Losses: [4.5459286868572235]
2026-01-17 11:32:41,193 : worker.worker : DEBUG : Step 11365, finished rewards -484.82, envs finished 1
2026-01-17 11:32:42,121 : worker.worker : DEBUG : Step 11383, finished rewards -482.33, envs finished 1
2026-01-17 11:32:47,798 : agent.on_policy : DEBUG : Mean Losses: [4.202884167432785]
2026-01-17 11:32:53,711 : agent.on_policy : DEBUG : Mean Losses: [2.2534196972846985]
2026-01-17 11:32:54,334 : worker.worker : DEBUG : Step 11427, finished rewards -480.76, envs finished 1
2026-01-17 11:32:57,696 : worker.worker : DEBUG : Step 11450, finished rewards -484.65, envs finished 1
2026-01-17 11:32:57,873 : worker.worker : DEBUG : Step 11451, finished rewards -475.09, envs finished 1
2026-01-17 11:33:02,321 : agent.on_policy : DEBUG : Mean Losses: [5.781918406486511]
2026-01-17 11:33:08,660 : agent.on_policy : DEBUG : Mean Losses: [2.40077805519104]
2026-01-17 11:33:15,177 : agent.on_policy : DEBUG : Mean Losses: [2.3844012916088104]
2026-01-17 11:33:15,223 : worker.worker : DEBUG : Step 11521, finished rewards -467.46, envs finished 1
2026-01-17 11:33:22,411 : agent.on_policy : DEBUG : Mean Losses: [2.5713257491588593]
2026-01-17 11:33:30,514 : agent.on_policy : DEBUG : Mean Losses: [2.3291551768779755]
2026-01-17 11:33:39,173 : agent.on_policy : DEBUG : Mean Losses: [2.2881051898002625]
2026-01-17 11:33:49,342 : agent.on_policy : DEBUG : Mean Losses: [2.2994501292705536]
2026-01-17 11:33:58,495 : agent.on_policy : DEBUG : Mean Losses: [2.279152736067772]
2026-01-17 11:34:06,301 : agent.on_policy : DEBUG : Mean Losses: [2.2625209987163544]
2026-01-17 11:34:16,724 : agent.on_policy : DEBUG : Mean Losses: [2.1635236740112305]
2026-01-17 11:34:26,791 : agent.on_policy : DEBUG : Mean Losses: [2.050410643219948]
2026-01-17 11:34:35,624 : agent.on_policy : DEBUG : Mean Losses: [1.9969156831502914]
2026-01-17 11:34:35,699 : worker.worker : DEBUG : Step 11808, finished rewards -327.60, envs finished 1
2026-01-17 11:34:42,187 : agent.on_policy : DEBUG : Mean Losses: [2.065004236996174]
2026-01-17 11:34:42,208 : worker.worker : DEBUG : Step 11841, finished rewards -486.13, envs finished 1
2026-01-17 11:34:43,447 : worker.worker : DEBUG : Step 11866, finished rewards -458.80, envs finished 1
2026-01-17 11:34:51,118 : agent.on_policy : DEBUG : Mean Losses: [3.7509298473596573]
2026-01-17 11:34:52,195 : worker.worker : DEBUG : Step 11884, finished rewards -467.34, envs finished 1
2026-01-17 11:34:57,621 : agent.on_policy : DEBUG : Mean Losses: [3.5675558000802994]
2026-01-17 11:34:59,536 : worker.worker : DEBUG : Step 11928, finished rewards -457.90, envs finished 1
2026-01-17 11:35:05,677 : agent.on_policy : DEBUG : Mean Losses: [3.711688220500946]
2026-01-17 11:35:07,030 : worker.worker : DEBUG : Step 11951, finished rewards -488.71, envs finished 1
2026-01-17 11:35:07,070 : worker.worker : DEBUG : Step 11952, finished rewards -485.13, envs finished 1
2026-01-17 11:35:15,134 : agent.on_policy : DEBUG : Mean Losses: [5.341259568929672]
2026-01-17 11:35:16,981 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.5403600876626365
2026-01-17 11:35:21,512 : agent.on_policy : DEBUG : Mean Losses: [2.3321540653705597]
2026-01-17 11:35:23,284 : worker.worker : DEBUG : Step 12022, finished rewards -491.56, envs finished 1
2026-01-17 11:35:29,384 : agent.on_policy : DEBUG : Mean Losses: [3.5673253536224365]
2026-01-17 11:35:39,025 : agent.on_policy : DEBUG : Mean Losses: [2.2381506860256195]
2026-01-17 11:35:45,004 : agent.on_policy : DEBUG : Mean Losses: [2.2136594355106354]
2026-01-17 11:35:53,818 : agent.on_policy : DEBUG : Mean Losses: [2.202998086810112]
2026-01-17 11:36:04,467 : agent.on_policy : DEBUG : Mean Losses: [2.1439958065748215]
2026-01-17 11:36:10,950 : agent.on_policy : DEBUG : Mean Losses: [2.1518722623586655]
2026-01-17 11:36:17,496 : agent.on_policy : DEBUG : Mean Losses: [2.1784012019634247]
2026-01-17 11:36:25,945 : agent.on_policy : DEBUG : Mean Losses: [1.9852597042918205]
2026-01-17 11:36:26,469 : worker.worker : DEBUG : Step 12267, finished rewards -314.11, envs finished 1
2026-01-17 11:36:31,933 : agent.on_policy : DEBUG : Mean Losses: [5.949250683188438]
2026-01-17 11:36:42,925 : agent.on_policy : DEBUG : Mean Losses: [2.007437899708748]
2026-01-17 11:36:46,575 : worker.worker : DEBUG : Step 12342, finished rewards -486.65, envs finished 1
2026-01-17 11:36:53,463 : agent.on_policy : DEBUG : Mean Losses: [3.364465892314911]
2026-01-17 11:36:54,829 : worker.worker : DEBUG : Step 12367, finished rewards -476.50, envs finished 1
2026-01-17 11:37:04,477 : agent.on_policy : DEBUG : Mean Losses: [3.161942020058632]
2026-01-17 11:37:04,566 : worker.worker : DEBUG : Step 12385, finished rewards -455.67, envs finished 1
2026-01-17 11:37:18,064 : agent.on_policy : DEBUG : Mean Losses: [2.339589238166809]
2026-01-17 11:37:19,012 : worker.worker : DEBUG : Step 12429, finished rewards -484.29, envs finished 1
2026-01-17 11:37:26,867 : agent.on_policy : DEBUG : Mean Losses: [3.4030079394578934]
2026-01-17 11:37:27,241 : worker.worker : DEBUG : Step 12452, finished rewards -478.24, envs finished 1
2026-01-17 11:37:27,405 : worker.worker : DEBUG : Step 12453, finished rewards -482.23, envs finished 1
2026-01-17 11:37:31,457 : agent.on_policy : DEBUG : Mean Losses: [3.7181111723184586]
2026-01-17 11:37:40,068 : agent.on_policy : DEBUG : Mean Losses: [2.191241905093193]
2026-01-17 11:37:40,354 : worker.worker : DEBUG : Step 12523, finished rewards -475.95, envs finished 1
2026-01-17 11:37:45,529 : agent.on_policy : DEBUG : Mean Losses: [3.491329461336136]
2026-01-17 11:37:55,742 : agent.on_policy : DEBUG : Mean Losses: [2.068498522043228]
2026-01-17 11:38:04,786 : agent.on_policy : DEBUG : Mean Losses: [1.943662405014038]
2026-01-17 11:38:15,355 : agent.on_policy : DEBUG : Mean Losses: [1.9241221100091934]
2026-01-17 11:38:26,984 : agent.on_policy : DEBUG : Mean Losses: [1.8281912803649902]
2026-01-17 11:38:36,299 : agent.on_policy : DEBUG : Mean Losses: [1.8206325620412827]
2026-01-17 11:38:42,035 : agent.on_policy : DEBUG : Mean Losses: [1.8097075670957565]
2026-01-17 11:38:53,537 : agent.on_policy : DEBUG : Mean Losses: [1.71678826212883]
2026-01-17 11:38:53,540 : worker.worker : DEBUG : Step 12768, finished rewards -474.75, envs finished 1
2026-01-17 11:39:03,778 : agent.on_policy : DEBUG : Mean Losses: [1.8326631933450699]
2026-01-17 11:39:15,229 : agent.on_policy : DEBUG : Mean Losses: [1.7474809885025024]
2026-01-17 11:39:16,712 : worker.worker : DEBUG : Step 12843, finished rewards -488.14, envs finished 1
2026-01-17 11:39:27,324 : agent.on_policy : DEBUG : Mean Losses: [3.115824520587921]
2026-01-17 11:39:27,458 : worker.worker : DEBUG : Step 12868, finished rewards -489.40, envs finished 1
2026-01-17 11:39:29,664 : worker.worker : DEBUG : Step 12886, finished rewards -481.03, envs finished 1
2026-01-17 11:39:37,015 : agent.on_policy : DEBUG : Mean Losses: [4.020961098372936]
2026-01-17 11:39:37,852 : worker.worker : DEBUG : Step 12905, finished rewards -323.17, envs finished 1
2026-01-17 11:39:47,719 : agent.on_policy : DEBUG : Mean Losses: [5.499841153621674]
2026-01-17 11:39:48,968 : worker.worker : DEBUG : Step 12953, finished rewards -477.49, envs finished 1
2026-01-17 11:39:49,032 : worker.worker : DEBUG : Step 12954, finished rewards -445.81, envs finished 1
2026-01-17 11:39:53,910 : agent.on_policy : DEBUG : Mean Losses: [5.7115912437438965]
2026-01-17 11:40:03,571 : agent.on_policy : DEBUG : Mean Losses: [2.1495425701141357]
2026-01-17 11:40:03,682 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.5133420832795047
2026-01-17 11:40:13,310 : agent.on_policy : DEBUG : Mean Losses: [2.099247857928276]
2026-01-17 11:40:14,031 : worker.worker : DEBUG : Step 13024, finished rewards -478.72, envs finished 1
2026-01-17 11:40:23,121 : agent.on_policy : DEBUG : Mean Losses: [2.014800250530243]
2026-01-17 11:40:29,763 : agent.on_policy : DEBUG : Mean Losses: [1.9539738297462463]
2026-01-17 11:40:38,355 : agent.on_policy : DEBUG : Mean Losses: [1.7946350425481796]
2026-01-17 11:40:48,296 : agent.on_policy : DEBUG : Mean Losses: [1.784166544675827]
2026-01-17 11:40:57,928 : agent.on_policy : DEBUG : Mean Losses: [1.7191306352615356]
2026-01-17 11:41:03,765 : agent.on_policy : DEBUG : Mean Losses: [1.7282367646694183]
2026-01-17 11:41:14,475 : agent.on_policy : DEBUG : Mean Losses: [1.7541400641202927]
2026-01-17 11:41:17,306 : worker.worker : DEBUG : Step 13269, finished rewards -477.74, envs finished 1
2026-01-17 11:41:23,050 : agent.on_policy : DEBUG : Mean Losses: [3.3658034801483154]
2026-01-17 11:41:29,774 : agent.on_policy : DEBUG : Mean Losses: [1.808954432606697]
2026-01-17 11:41:36,143 : agent.on_policy : DEBUG : Mean Losses: [1.6591444462537766]
2026-01-17 11:41:36,147 : worker.worker : DEBUG : Step 13344, finished rewards -482.94, envs finished 1
2026-01-17 11:41:37,495 : worker.worker : DEBUG : Step 13369, finished rewards -471.68, envs finished 1
2026-01-17 11:41:44,719 : agent.on_policy : DEBUG : Mean Losses: [3.271662801504135]
2026-01-17 11:41:45,166 : worker.worker : DEBUG : Step 13387, finished rewards -473.30, envs finished 1
2026-01-17 11:41:45,913 : worker.worker : DEBUG : Step 13406, finished rewards -480.49, envs finished 1
2026-01-17 11:41:51,172 : agent.on_policy : DEBUG : Mean Losses: [4.6329283863306046]
2026-01-17 11:41:59,439 : agent.on_policy : DEBUG : Mean Losses: [1.8398513197898865]
2026-01-17 11:42:01,209 : worker.worker : DEBUG : Step 13454, finished rewards -477.52, envs finished 1
2026-01-17 11:42:01,256 : worker.worker : DEBUG : Step 13455, finished rewards -458.35, envs finished 1
2026-01-17 11:42:07,375 : agent.on_policy : DEBUG : Mean Losses: [5.401363387703896]
2026-01-17 11:42:17,593 : agent.on_policy : DEBUG : Mean Losses: [1.9071237593889236]
2026-01-17 11:42:19,783 : worker.worker : DEBUG : Step 13525, finished rewards -474.34, envs finished 1
2026-01-17 11:42:25,576 : agent.on_policy : DEBUG : Mean Losses: [3.4048187285661697]
2026-01-17 11:42:31,824 : agent.on_policy : DEBUG : Mean Losses: [1.7943013906478882]
2026-01-17 11:42:38,286 : agent.on_policy : DEBUG : Mean Losses: [1.8334669470787048]
2026-01-17 11:42:42,819 : agent.on_policy : DEBUG : Mean Losses: [1.7219161242246628]
2026-01-17 11:42:48,703 : agent.on_policy : DEBUG : Mean Losses: [1.6518993377685547]
2026-01-17 11:42:51,328 : worker.worker : DEBUG : Step 13681, finished rewards -263.23, envs finished 1
2026-01-17 11:42:56,090 : agent.on_policy : DEBUG : Mean Losses: [6.463595062494278]
2026-01-17 11:43:05,808 : agent.on_policy : DEBUG : Mean Losses: [1.6785081773996353]
2026-01-17 11:43:15,311 : agent.on_policy : DEBUG : Mean Losses: [1.6503992974758148]
2026-01-17 11:43:21,813 : agent.on_policy : DEBUG : Mean Losses: [1.6067174524068832]
2026-01-17 11:43:27,341 : agent.on_policy : DEBUG : Mean Losses: [1.5545323863625526]
2026-01-17 11:43:28,755 : worker.worker : DEBUG : Step 13845, finished rewards -482.88, envs finished 1
2026-01-17 11:43:37,090 : agent.on_policy : DEBUG : Mean Losses: [3.295807346701622]
2026-01-17 11:43:37,400 : worker.worker : DEBUG : Step 13870, finished rewards -495.71, envs finished 1
2026-01-17 11:43:46,025 : agent.on_policy : DEBUG : Mean Losses: [3.150588810443878]
2026-01-17 11:43:46,032 : worker.worker : DEBUG : Step 13888, finished rewards -466.91, envs finished 1
2026-01-17 11:43:48,944 : worker.worker : DEBUG : Step 13907, finished rewards -485.87, envs finished 1
2026-01-17 11:43:57,252 : agent.on_policy : DEBUG : Mean Losses: [3.365622490644455]
2026-01-17 11:44:09,183 : agent.on_policy : DEBUG : Mean Losses: [1.7575382888317108]
2026-01-17 11:44:09,233 : worker.worker : DEBUG : Step 13955, finished rewards -470.65, envs finished 1
2026-01-17 11:44:09,294 : worker.worker : DEBUG : Step 13956, finished rewards -463.22, envs finished 1
2026-01-17 11:44:15,803 : agent.on_policy : DEBUG : Mean Losses: [3.2416299283504486]
2026-01-17 11:44:16,243 : agent.exploration.egreedy : DEBUG : Current epsilon for egreedy 0.48767497911552943
2026-01-17 11:44:26,091 : agent.on_policy : DEBUG : Mean Losses: [1.8904150277376175]
2026-01-17 11:44:26,756 : worker.worker : DEBUG : Step 14026, finished rewards -482.45, envs finished 1
2026-01-17 11:44:35,222 : agent.on_policy : DEBUG : Mean Losses: [3.178482711315155]
2026-01-17 11:44:40,350 : agent.on_policy : DEBUG : Mean Losses: [1.7585464715957642]
2026-01-17 11:44:47,145 : agent.on_policy : DEBUG : Mean Losses: [1.6581473499536514]
2026-01-17 11:44:58,289 : agent.on_policy : DEBUG : Mean Losses: [1.5711626559495926]
2026-01-17 11:45:07,952 : agent.on_policy : DEBUG : Mean Losses: [1.5129810869693756]
2026-01-17 11:45:08,890 : worker.worker : DEBUG : Step 14182, finished rewards -480.21, envs finished 1
2026-01-17 11:45:15,185 : agent.on_policy : DEBUG : Mean Losses: [2.529712736606598]
2026-01-17 11:45:23,612 : agent.on_policy : DEBUG : Mean Losses: [1.4555705785751343]
2026-01-17 11:45:33,874 : agent.on_policy : DEBUG : Mean Losses: [1.3469198420643806]
2026-01-17 11:45:34,118 : worker.worker : DEBUG : Step 14285, finished rewards -312.35, envs finished 1
2026-01-17 11:45:35,265 : worker.worker : DEBUG : Step 14299, finished rewards -146.49, envs finished 1
2026-01-17 11:45:41,794 : agent.on_policy : DEBUG : Mean Losses: [11.288195833563805]
2026-01-17 11:45:48,249 : agent.on_policy : DEBUG : Mean Losses: [1.5775609761476517]
2026-01-17 11:45:52,792 : agent.on_policy : DEBUG : Mean Losses: [1.445907361805439]
2026-01-17 11:45:53,026 : worker.worker : DEBUG : Step 14371, finished rewards -476.82, envs finished 1
2026-01-17 11:45:54,867 : worker.worker : DEBUG : Step 14389, finished rewards -463.21, envs finished 1
2026-01-17 11:46:02,006 : agent.on_policy : DEBUG : Mean Losses: [4.0123565047979355]
2026-01-17 11:46:02,627 : worker.worker : DEBUG : Step 14408, finished rewards -466.25, envs finished 1
2026-01-17 11:46:15,998 : agent.on_policy : DEBUG : Mean Losses: [2.991979479789734]
2026-01-17 11:46:18,594 : worker.worker : DEBUG : Step 14456, finished rewards -477.47, envs finished 1
2026-01-17 11:46:18,835 : worker.worker : DEBUG : Step 14457, finished rewards -483.09, envs finished 1
2026-01-17 11:46:26,153 : agent.on_policy : DEBUG : Mean Losses: [5.852250590920448]
2026-01-17 11:46:36,044 : agent.on_policy : DEBUG : Mean Losses: [1.6581060588359833]
2026-01-17 11:46:45,382 : agent.on_policy : DEBUG : Mean Losses: [1.590298444032669]
2026-01-17 11:46:53,941 : agent.on_policy : DEBUG : Mean Losses: [1.5871448069810867]
2026-01-17 11:47:07,820 : agent.on_policy : DEBUG : Mean Losses: [1.586438775062561]
2026-01-17 11:47:17,465 : agent.on_policy : DEBUG : Mean Losses: [1.5362719148397446]
2026-01-17 11:47:22,880 : agent.on_policy : DEBUG : Mean Losses: [1.5374731123447418]
2026-01-17 11:47:23,858 : worker.worker : DEBUG : Step 14683, finished rewards -475.16, envs finished 1
2026-01-17 11:47:28,267 : agent.on_policy : DEBUG : Mean Losses: [3.446984216570854]
2026-01-17 11:47:37,273 : agent.on_policy : DEBUG : Mean Losses: [1.6189852803945541]
2026-01-17 11:47:47,610 : agent.on_policy : DEBUG : Mean Losses: [1.563597597181797]
2026-01-17 11:47:56,025 : agent.on_policy : DEBUG : Mean Losses: [1.517921969294548]
2026-01-17 11:47:56,314 : worker.worker : DEBUG : Step 14786, finished rewards -480.09, envs finished 1
2026-01-17 11:47:57,044 : worker.worker : DEBUG : Step 14800, finished rewards -486.99, envs finished 1
