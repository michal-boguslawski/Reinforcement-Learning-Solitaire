experiment_name: BipedalWalker-PPO

env_kwargs:
  id: BipedalWalker-v3
  num_envs: 16
  training_wrappers:
    # terminal_bonus:
    #   truncated_bonus: -10
    power_obs_reward:
      abs_factors: [-0.05, 0, 0.1, -0.02, -0.01, 0.01, 0, 0, 0, -0.01, 0.01, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    no_movement_truncate:
      index: 2
    observations_interaction:
      factors:
        "(5, 10)": -10.
    scale_reward:
      scale_factor: 0.05
      loc_factor: -0.01
  vectorization_mode: async

policy:
  type: ppo
  kwargs:
    advantage_normalize: global
    gamma: 0.99
    lambda_: 0.95
    entropy_coef: 0.001
    value_loss_coef: 0.5
    clip_epsilon: 0.2
    num_epochs: 10
    optimizer_kwargs:
      actor_lr: 3e-5
      critic_lr: 1e-4
    exploration_method:
      name: distribution

worker_kwargs:
  device: auto
  record_step: 100000

train_kwargs:
  batch_size: 2048
  minibatch_size: 128
  num_steps: 1000000

network:
  kwargs:
    backbone_kwargs: {}
    backbone_name: mlp
    distribution: normal
    head_kwargs: {}
    head_name: actor_critic
    num_features: 32
    initial_deviation: 0.6
