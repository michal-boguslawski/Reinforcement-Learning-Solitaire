env_kwargs:
  general_wrappers: {}
  id: CarRacing-v3
  normalize_rewards: false
  num_envs: 12
  permute_observations: true
  training_wrappers:
    terminal_bonus:
      truncated_bonus: -5
    out_of_track:
      out_of_track_penalty: 0.5
      terminate_after: 20
    scale_reward:
      loc_factor: -0.01
      scale_factor: 0.2
  general_wrappers:
    rescale_observation:
      min_obs: 0.
      max_obs: 1.
  vectorization_mode: async
env_name: CarRacing
experiment_name: PPO/v12
network:
  kwargs:
    backbone_kwargs:
      out_features: 256
      dims: [32, 64, 128, 128]
      kernel_sizes: [5, 5, 3, 3]
      strides: [2, 2, 2, 1]
      activation_fn: relu
    backbone_name: cnn
    core_kwargs:
      out_features: 256
    core_name: gru
    distribution: normal
    head_kwargs:
      hidden_dim: 256
      activation_fn: relu
    head_name: actor_critic
    initial_deviation: 0.6
    weights_kwargs:
      file_path: /app/rl_agents/logs/CarRacing/PPO/v11/model.pt
policy:
  kwargs:
    advantage_normalize: batch
    clip_epsilon: 0.15
    entropy_kwargs:
      max_entropy: 0.001
      min_entropy: 0.
      scheduler_type: linear_entropy
      total_steps: 400
    exploration_method:
      kwargs: {}
      name: distribution
    gamma: 0.99
    lambda_: 0.95
    num_epochs: 10
    optimizer_kwargs:
      actor_lr: 6e-6
      critic_lr: 9e-6
    scheduler_kwargs:
      scheduler_type: cosine_annealing_lr
      T_max: 490
    use_value_clipping: absolute
    value_loss_coef: 0.5
  type: ppo
train_kwargs:
  batch_size: 2048
  minibatch_size: 512
  num_steps: 1000000
worker_kwargs:
  device: auto
  record_step: 100000
  temperature_config: {}
  verbose: 1
