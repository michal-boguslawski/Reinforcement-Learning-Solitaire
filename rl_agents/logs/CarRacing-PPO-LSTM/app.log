2026-01-09 17:36:56,724 : worker.worker : INFO : ==================== Start training ====================
2026-01-09 17:36:56,996 : worker.worker : INFO : Step 0, Avg Reward 0.0000, Max Reward 0.0000, Loss [0.0]
2026-01-09 17:37:43,507 : worker.worker : INFO : Step 0: 1000 steps, reward = -72.41, truncated = True, terminated = False
2026-01-09 17:37:43,509 : worker.worker : INFO : Covariance matrix:
[[1.7246825 0.        0.       ]
 [0.        1.7246825 0.       ]
 [0.        0.        1.7246825]]
2026-01-09 17:38:15,942 : agent.on_policy : DEBUG : Mean Losses: [-0.045896006166003644, -0.011956765141803771, 0.03356901434017345, 5.07237498164177]
2026-01-09 17:38:55,854 : agent.on_policy : DEBUG : Mean Losses: [-0.04169178903102875, -0.005467418314947281, 0.028724301420152188, 5.0586522161960605]
2026-01-09 17:39:37,092 : agent.on_policy : DEBUG : Mean Losses: [-0.05391439306549728, -0.015338140053790994, 0.023699016962200403, 5.042576289176941]
2026-01-09 17:40:15,837 : agent.on_policy : DEBUG : Mean Losses: [-0.05748188677243889, -0.015079585422063247, 0.015906937525141985, 5.035577112436295]
2026-01-09 17:40:48,391 : agent.on_policy : DEBUG : Mean Losses: [-0.06863133744336665, -0.02394838098116452, 0.011292749806307257, 5.032933235168457]
2026-01-09 17:41:27,330 : agent.on_policy : DEBUG : Mean Losses: [-0.0675973906647414, -0.021907336136791855, 0.009081123591749929, 5.023061782121658]
2026-01-09 17:41:59,230 : agent.on_policy : DEBUG : Mean Losses: [-0.07299621077254415, -0.026560741139110177, 0.007220385211985559, 5.004566341638565]
2026-01-09 17:42:08,544 : worker.worker : INFO : Step 999, temp mean rewards -32.64
2026-01-09 17:42:40,416 : agent.on_policy : DEBUG : Mean Losses: [-0.04375275890342891, -0.01986482237043674, 0.052073896210640666, 4.992488604784012]
2026-01-09 17:43:08,539 : agent.on_policy : DEBUG : Mean Losses: [-0.060123184230178595, -0.013287120655877515, 0.006032113009132445, 4.985212117433548]
2026-01-09 17:43:46,115 : agent.on_policy : DEBUG : Mean Losses: [-0.06665028855204583, -0.019433297944488004, 0.004959888967277948, 4.969693636894226]
2026-01-09 17:44:24,254 : agent.on_policy : DEBUG : Mean Losses: [-0.065258913859725, -0.019035905635973904, 0.0066346697072731334, 4.954034471511841]
2026-01-09 17:45:02,586 : agent.on_policy : DEBUG : Mean Losses: [-0.06501693865284323, -0.018261289622751066, 0.005321101679874119, 4.94162010550499]
2026-01-09 17:45:44,712 : agent.on_policy : DEBUG : Mean Losses: [-0.06620495524257422, -0.019224846832639742, 0.004595952585805208, 4.927808547019959]
2026-01-09 17:46:26,544 : agent.on_policy : DEBUG : Mean Losses: [-0.0585258498089388, -0.014135934610385448, 0.009537263463425916, 4.915854805707932]
2026-01-09 17:46:59,444 : agent.on_policy : DEBUG : Mean Losses: [-0.06916917848866433, -0.02577417414286174, 0.011434757139068098, 4.91123840212822]
2026-01-09 17:47:06,686 : worker.worker : INFO : Step 2000, temp mean rewards -25.32
2026-01-09 17:47:43,135 : agent.on_policy : DEBUG : Mean Losses: [-0.059843450249172744, -0.02955177623662166, 0.03759443867020309, 4.9088894665241245]
2026-01-09 17:48:14,543 : agent.on_policy : DEBUG : Mean Losses: [-0.06210349481552839, -0.015144554190919734, 0.004192457185126841, 4.905517071485519]
2026-01-09 17:49:00,262 : agent.on_policy : DEBUG : Mean Losses: [-0.050640899944119154, -0.00932596871862188, 0.015461911750026047, 4.904588824510574]
2026-01-09 17:49:32,504 : agent.on_policy : DEBUG : Mean Losses: [-0.040036666323430836, 0.002438143701874651, 0.01308320836542407, 4.901641547679901]
2026-01-09 17:50:16,316 : agent.on_policy : DEBUG : Mean Losses: [-0.057393925404176115, -0.011353650288947392, 0.005840478358732071, 4.89605153799057]
2026-01-09 17:50:53,548 : agent.on_policy : DEBUG : Mean Losses: [-0.06888237451203168, -0.022580626402431792, 0.005212531733559444, 4.8908014595508575]
2026-01-09 17:51:41,291 : agent.on_policy : DEBUG : Mean Losses: [-0.06727310405112803, -0.0219601526245242, 0.007114560953777982, 4.887023282051087]
2026-01-09 17:52:13,469 : agent.on_policy : DEBUG : Mean Losses: [-0.07155844189692288, -0.02479944355436601, 0.0041345460413140245, 4.882627230882645]
2026-01-09 17:52:18,114 : worker.worker : INFO : Step 3001, temp mean rewards -21.90
2026-01-09 17:52:56,719 : agent.on_policy : DEBUG : Mean Losses: [-0.06190658896230161, -0.028626638880814424, 0.031011532642878592, 4.878571730852127]
2026-01-09 17:54:03,060 : agent.on_policy : DEBUG : Mean Losses: [-0.05489518607500941, -0.00797246076108422, 0.0036046473775058984, 4.872505015134811]
2026-01-09 17:54:55,819 : agent.on_policy : DEBUG : Mean Losses: [-0.05666517780628055, -0.009526186549919657, 0.0030086816266702955, 4.864333325624466]
2026-01-09 17:55:35,830 : agent.on_policy : DEBUG : Mean Losses: [-0.05986251388676465, -0.012347310433688108, 0.0021195013649048635, 4.857495492696762]
2026-01-09 17:56:22,298 : agent.on_policy : DEBUG : Mean Losses: [-0.06574004329741001, -0.018508864025352522, 0.0024722089034185045, 4.846728456020355]
2026-01-09 17:57:06,930 : agent.on_policy : DEBUG : Mean Losses: [-0.05649153275880962, -0.009434368784422987, 0.0027141082653542982, 4.841421914100647]
2026-01-09 17:57:50,781 : agent.on_policy : DEBUG : Mean Losses: [-0.062063037604093554, -0.014841326387249864, 0.0022067399844672765, 4.832508236169815]
2026-01-09 17:58:24,397 : agent.on_policy : DEBUG : Mean Losses: [-0.06067503711674362, -0.013448677337146364, 0.0019751711475237245, 4.821394681930542]
2026-01-09 17:58:27,061 : worker.worker : INFO : Step 4002, temp mean rewards -19.40
2026-01-09 17:59:08,138 : agent.on_policy : DEBUG : Mean Losses: [-0.06581556231249124, -0.02628837144220597, 0.017238953377818687, 4.8146668612957]
2026-01-09 17:59:43,978 : agent.on_policy : DEBUG : Mean Losses: [-0.053769108513370155, -0.006889314844738692, 0.002427207904474926, 4.809339904785157]
2026-01-09 18:00:29,086 : agent.on_policy : DEBUG : Mean Losses: [-0.05453079405706376, -0.00799264592351392, 0.0030047340083910966, 4.804051619768143]
2026-01-09 18:01:00,531 : agent.on_policy : DEBUG : Mean Losses: [-0.055099685001187025, -0.009477748492847082, 0.004678344947751612, 4.79611104130745]
2026-01-09 18:01:39,336 : agent.on_policy : DEBUG : Mean Losses: [-0.05922690371517092, -0.012279176296218174, 0.0018944065211144335, 4.789493179321289]
2026-01-09 18:02:26,280 : agent.on_policy : DEBUG : Mean Losses: [-0.056420877762138846, -0.009867102545103989, 0.002716198388588964, 4.791187530755996]
2026-01-09 18:03:00,467 : agent.on_policy : DEBUG : Mean Losses: [-0.059527497133240105, -0.012095335015328602, 0.0008237924467266566, 4.784405934810638]
2026-01-09 18:03:39,783 : agent.on_policy : DEBUG : Mean Losses: [-0.056235926854424176, -0.009452854294795542, 0.0017484406050016332, 4.765729403495788]
2026-01-09 18:03:40,485 : worker.worker : INFO : Step 5000, Avg Reward -19.4016, Max Reward -11.8935, Loss [-0.05947613, -0.01565647, 0.01041954, 4.90294301]
2026-01-09 18:03:40,765 : worker.worker : INFO : Step 5003, temp mean rewards -10.06
